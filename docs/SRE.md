# ğŸ§  SRE (Site Reliability Engineering)

SRE is the practice of using software engineering to make systems reliable, scalable, and efficient.
It bridges the gap between development and operations by automating manual work and measuring reliability.

## ğŸ“ SLI (Service Level Indicator)

â€œWhat we measureâ€

Definition: A measurement of how well a service is performing.
It tells you what the user is actually experiencing.

Example:

Availability (e.g., % of successful requests)

Latency (e.g., 95% of requests respond in <200 ms)

Error rate (e.g., % of failed logins)

Throughput (e.g., requests per second)

## ğŸ¯ SLO (Service Level Objective) â€” â€œWhat we aim forâ€

Definition: A target value or range for an SLI that defines the desired level of service reliability.
Itâ€™s what you aim to achieve.

Example:

The system should have 99.9% availability per month.

The API should respond within 200 ms for 95% of requests.

## ğŸ¤ SLA (Service Level Agreement) â€” â€œWhat we promise (legally)â€

An SLA is usually a contract between you and your users/customers.
It defines the minimum service level you guarantee â€” and what happens if you fail.

ğŸ’¼ Example:

â€œWe guarantee 99.5% uptime per month.

If we go below that, weâ€™ll credit you 10% of your bill.

## Difference between Monitoring and Observibility

### ğŸ” Monitoring â€” â€œAre we OK right now?â€

Definition:
Monitoring means collecting and tracking predefined metrics to know if your system is healthy.  
Itâ€™s about detecting known problems.

Key idea: You already know what to look for.

Examples:

CPU usage above 90% â†’ alert

Website response time > 2s â†’ alert

Error rate > 1% â†’ alert

Goal:
Detect when something goes wrong â€” and alert the right people.

Tools:
Prometheus, Grafana, Nagios, CloudWatch, Datadog, Zabbix

### ğŸ§  Observability â€” â€œWhy arenâ€™t we OK?â€

Definition:
Observability means understanding whatâ€™s happening inside your system just by looking at the data it produces â€” even for problems you didnâ€™t predict.  
Itâ€™s about exploration and debugging unknown issues.

Key idea: You might not know what to look for yet.

Examples:

Tracing a single userâ€™s request across multiple microservices.

Investigating why latency suddenly spiked in one region.

Correlating logs, metrics, and traces to find the root cause.

Goal:
Help engineers ask new questions about the system and find unknown problems.

Tools:
OpenTelemetry, Grafana Tempo, Jaeger, Honeycomb, New Relic, Elastic Stack

âš™ï¸ Simple Analogy

Concept	Analogy	Purpose
Monitoring	Like a car dashboard â€” it shows known indicators (fuel, speed, temperature).	Tells you when somethingâ€™s wrong.  
Observability	Like a mechanic diagnosing the engine when the car makes a strange noise.	Helps you understand why itâ€™s wrong.

ğŸ§© Summary

| Aspect | Monitoring | Observability |
|--------|-------------|---------------|
| **Purpose** | Detect known issues | Explore and diagnose unknown issues |
| **Focus** | Metrics and alerts | Logs, metrics, and traces (the â€œthree pillarsâ€) |
| **Approach** | Reactive | Proactive |
| **Question answered** | â€œIs it working?â€ | â€œWhy isnâ€™t it working?â€ |

