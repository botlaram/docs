{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to RK's Knowledge Hub","text":""},{"location":"#hello-and-thank-you-for-visiting","title":"Hello, and thank you for visiting","text":"<p>My name is Ramakrishna Botla (RK), and I'm a dedicated Software Engineer with a passion for building, optimizing, and sharing knowledge on cutting-edge technologies.</p> <pre><code>from dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass BeingDeveloper:\n    name: str\n    skills: List[str]\n    majorCompetency: bool\n    quickLearner: bool\n    problemSolver: bool\n\n    def is_coder(self) -&gt; bool:\n        return self.quickLearner and self.problemSolver and len(self.skills) &gt;= 5\n\nbeingDeveloper = BeingDeveloper(\n    name='Ramakrishna_Botla',\n    skills=['Python', 'Kubernetes', 'Helm', 'ArgoCD', 'Openshift', 'AzCloud', 'Linux', 'Docker', 'Terraform', 'Ansible', 'Packer'],\n    majorCompetency=['Python', 'DevOps'],\n    quick_learner=True,\n    problemSolver=True\n)\n\nprint(beingDeveloper.is_coder())  # Output: True\n</code></pre>"},{"location":"#what-youll-find-here","title":"What You'll Find Here","text":"<p>This documentation is crafted to be a comprehensive resource for anyone interested in DevOps, Python programming, Kubernetes, Networking, Azure Cloud and Articles.</p> <ul> <li>Python Modules: Short note and syntax of python modules, and examples to enhance your Python development skills.</li> <li>DevOps Insights: Essential concepts, workflows, and tools for streamlining development, deployment, and automation.</li> <li>Kubernetes Knowledge Base: Brief on kubernetes components and their usage.</li> <li>Networking Fundamentals: Foundational and advanced networking concepts to help you navigate modern infrastructure.</li> <li>Azure Cloud Solutions: Definition of Azure resources this enabling you to leverage its cloud capabilities effectively.</li> </ul> <p>This is a curated space intended to provide valuable insights, efficient solutions, and reliable \"cheat-sheet\" commands to enhance your productivity and technical know-how.</p>"},{"location":"#join-the-conversation","title":"Join the Conversation","text":"<p>I believe in the power of community and collaboration. Please feel free to suggest improvements, contribute new insights, or provide feedback - repo . Your input helps make this hub a richer resource for everyone.</p> <p>Happy learning, coding, and exploring!</p>"},{"location":"DSA/","title":"All about Data Structure Algorithms with Real-time example","text":""},{"location":"DSA/#bigocheatsheet","title":"BigoCheatsheet","text":""},{"location":"DSA/#linear-search","title":"Linear Search","text":"<p>Linear search, also known as sequential search, is a simple search algorithm that checks each element in a list or array until the target element is found or the entire list is traversed. It starts searching from the beginning of the list and compares each element with the target element until a match is found or the end of the list is reached.</p> <p></p>"},{"location":"DSA/#realtime-example-linear-search","title":"Realtime Example Linear Search","text":"<p>A real-time example of linear search could be searching for a specific contact in your phone's contact list.</p> <p>Imagine you have a list of contacts stored in your phone, and you want to find the contact information for a particular person, let's say \"John Doe\". You could use linear search to look through each contact in your list one by one until you find the contact named \"John Doe\".</p>"},{"location":"DSA/#binary-search","title":"Binary Search","text":"<p>Binary search is a searching algorithm that efficiently finds the position of a target value within a sorted array. It works by repeatedly dividing the search interval in half. Here's how it works:</p> <ol> <li> <p>Initial Step: Binary search requires the array to be sorted initially. Let's say you have a sorted array arr.</p> </li> <li> <p>Divide: Start with the whole array. Calculate the midpoint of the array.</p> </li> <li> <p>Compare: Compare the target value with the element at the midpoint. If the target value matches the midpoint value, the search is successful.</p> </li> <li> <p>Adjust Search Range: If the target value is less than the midpoint value, then the target, if present, must be in the lower half of the array. If the target value is greater, then it must be in the upper half.</p> </li> <li> <p>Repeat: Repeat steps 2-4 until the target value is found or until the search interval is empty.</p> </li> </ol> <p></p>"},{"location":"DSA/#realtime-example-binary-search","title":"Realtime Example Binary Search","text":"<p>A real-life example of binary search can be found in a library catalogue system.</p> <p>Imagine you are searching for a particular book in a library with thousands of books arranged in alphabetical order by title. Instead of starting from the first book and checking each book sequentially, which could take a long time especially in a large library, you can use binary search.</p>"},{"location":"DSA/#linked-list","title":"Linked List","text":"<p>Linked list consists of a sequence of elements called nodes. Each node contains two parts: the data and a reference (or pointer) to the next node in the sequence. Linked lists offer dynamic memory allocation, efficient insertion and deletion operations, and are especially useful when the size of the data structure is unknown or frequently changing.</p> <p></p>"},{"location":"DSA/#why-linked-list","title":"Why Linked List","text":"<p>For eg: Let's assume we have a list of elements in array</p> <pre><code>list=[1,2,3,4,5,6,7,8,9,.......,1000000]\n</code></pre> <p>If we want to insert element in between, all numbers next to the insertion elements will be shifted further. Suppose, If we have 1 million of elements in the array (this may take a long time to shift). Also this increases the time complexity.</p> <p>LinkedList algorithm stores elements with the address of next node. So it doesn't need to shift any nodes while inserting or deleting an element from the middle of elements.</p>"},{"location":"DSA/#realtime-example-linked-list","title":"Realtime Example Linked List","text":"<p>Let's consider a real-life example of a linked list: a playlist in a music streaming application.</p> <p>In a music streaming application like Spotify or Apple Music, a playlist is essentially a collection of songs that users can organize and listen to in a particular order. We can represent a playlist using a linked list data structure.</p> <p>Node Representation: Each node in the linked list represents a song in the playlist. It contains two parts:</p> <p>Data: Information about the song, such as the song title, artist, album, duration, etc. Reference: A reference to the next song in the playlist. Playlist Structure:</p> <p>The playlist starts with the first song (the head of the linked list). Each song is linked to the next song in the playlist through the \"next\" reference. The last song in the playlist points to None, indicating the end of the playlist. Operations:</p> <p>Adding a Song: To add a new song to the playlist, we create a new node and update the reference of the last song to point to the new song. Deleting a Song: To remove a song from the playlist, we adjust the references of the neighboring nodes to bypass the deleted node. Playing Songs: We can traverse the linked list from the beginning (head) to the end, playing each song in the playlist sequentially. Reordering Songs: We can easily rearrange the playlist by modifying the references between nodes without moving the actual song data.</p>"},{"location":"DSA/#stack","title":"Stack","text":"<p>Stack is a linear data structure that follows the Last In, First Out (LIFO) principle. This means that the last element added to the stack is the first one to be removed. Think of it as a stack of plates where you can only remove the top plate or add a new plate on top.</p> <p></p>"},{"location":"DSA/#using-list-as-a-stack-in-python","title":"Using List as a Stack in Python","text":"<pre><code>s=[]\ns.append(\"https://google.com\")\ns.append(\"https://google.com/facebook\")\ns.append(\"https://google.com/feeds\")\ns.append(\"https://google.com/photos\")\nprint(s)\nprint(s.pop())\nprint(s.pop())\nprint(s)\n</code></pre> <p>In python we can use List as stack, but the problem is that List is dynamic array. Lets take an example, If we have a list with capacity of 10 for suppose if you have to insert 11th element in the List but the capacity was 10 Since, it is dynamic array it will create new array with capacity of 10*2 and copy all existing element to new memory and add 11th element. as shown in below figure</p> <p>Lets, assume if we have million elements and to add one more element. it will utilize more resources to copy into new array.</p> <p></p> <p>Therefore, using List as Stack in python is not recommended.</p> <p>Recommended approach is to use collections.deque instead.</p>"},{"location":"DSA/#real-time-example-of-stack","title":"Real time example of Stack","text":"<p>A real-life example of a stack in the context of web pages can be seen in the browser's history mechanism.</p> <p>Browser History Mechanism: When you visit web pages using a web browser, each page you visit is added to a history stack. The browser keeps track of the sequence of pages you have visited, and you can navigate through them using the browser's back and forward buttons.</p> <p>Navigation:</p> <p>When you visit a new page, it gets pushed onto the top of the history stack. If you click the browser's back button, the current page is popped off the stack, and you are taken to the previous page. If you click the forward button, the next page in the stack is popped, and you move forward in the browsing history. Stack Structure:</p> <p>Each page visited is analogous to an element in the stack. When you navigate back, you are essentially popping the top page off the stack and moving to the page beneath it. When you navigate forward, you are popping the page you previously went back from. Implementation:</p> <p>Browsers typically implement the history stack using a data structure similar to a stack, where pages are added to the top (pushed) when visited and removed from the top (popped) when navigating back or forward.</p>"},{"location":"DSA/#queue","title":"Queue","text":"<p>Queue is a linear data structure that follows the First In, First Out (FIFO) principle. It's similar to a queue of people waiting in line for something, where the person who joins the queue first is served first, and so on. In a queue, elements are added at one end called the rear (also known as enqueue operation), and elements are removed from the other end called the front (also known as dequeue operation).</p> <p>Properties of a Queue: FIFO (First In, First Out): The element that is added first will be the first one to be removed.</p> <ol> <li> <p>Insertion (enqueue) operation: Adds an element to the rear of the queue.</p> </li> <li> <p>Deletion (dequeue) operation: Removes an element from the front of the queue.</p> </li> <li> <p>Peek operation: Returns the element at the front of the queue without removing it.</p> </li> <li> <p>Empty and Full conditions: A queue may have restrictions on its size, and when it reaches its maximum capacity, it's considered full. An empty queue has no elements.</p> </li> </ol> <p></p>"},{"location":"DSA/#real-life-examples-of-queues","title":"Real-life Examples of Queues","text":"<ul> <li> <p>Waiting Lines: Queues model scenarios like waiting lines in banks, ATMs, restaurants, and amusement parks where the first person to join the line is the first one to be served.</p> </li> <li> <p>Print Queue: When multiple users send documents to a shared printer, their print jobs are queued up. The printer serves each print job in the order it was received.</p> </li> <li> <p>Breadth-First Search (BFS) Traversal: In graph algorithms like BFS traversal, a queue is used to keep track of vertices that need to be visited next.</p> </li> </ul>"},{"location":"DSA/#bubble-sort","title":"Bubble sort","text":"<p>Bubble Sort is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted. It is called Bubble Sort because with each pass through the list, the smaller elements gradually \"bubble\" up to their correct positions.</p> <p>Here's how Bubble Sort works:</p> <ol> <li> <p>Comparison: The algorithm compares each pair of adjacent items in the list, from the beginning to the end.</p> </li> <li> <p>Swap: If the elements are in the wrong order (i.e., the current element is greater than the next element), they are swapped.</p> </li> <li> <p>Iteration: This process is repeated for each pair of adjacent elements in the list until no more swaps are needed, indicating that the list is sorted.</p> </li> </ol> <p></p>"},{"location":"DSA/#real-time-example-of-bubble-sort","title":"Real time example of Bubble sort","text":"<p>let's consider a real-life example where Bubble Sort might be used: sorting a hand of playing cards.</p> <p>Imagine you have a hand of playing cards that are in a random order, and you want to arrange them in ascending order based on their values (e.g., from Ace to King for each suit).</p> <ol> <li> <p>Comparison: To sort the cards, you would compare adjacent cards in the hand.</p> </li> <li> <p>Swap: If the current card is greater than the next card, you would swap their positions.</p> </li> <li> <p>Iteration: You would repeat this process multiple times, traversing the hand of cards, until the entire hand is sorted.</p> </li> </ol>"},{"location":"DSA/#selection-sort","title":"Selection Sort","text":"<p>Selection Sort is a simple sorting algorithm that works by repeatedly selecting the smallest (or largest) element from an unsorted part of the list and swapping it with the element at the beginning of the unsorted list. This process continues until the entire list is sorted.</p> <p>How It Works (Algorithm Steps)</p> <ol> <li> <p>Start with the first element of the list and assume it's the minimum.</p> </li> <li> <p>Compare this element with the rest of the list to find the actual minimum element.</p> </li> <li> <p>Once the minimum element is found, swap it with the first element.</p> </li> <li> <p>Move to the second position and repeat the process for the remaining unsorted portion of the list.</p> </li> <li> <p>Continue this process until the entire list is sorted.</p> </li> </ol> <p></p> <p>Real-Time Example of Selection Sort Imagine you have a line of people, each holding a random number, and you want to arrange them in increasing order.</p>"},{"location":"SRE/","title":"\ud83e\udde0 SRE (Site Reliability Engineering)","text":"<p>SRE is the practice of using software engineering to make systems reliable, scalable, and efficient. It bridges the gap between development and operations by automating manual work and measuring reliability.</p>"},{"location":"SRE/#sli-service-level-indicator","title":"\ud83d\udccf SLI (Service Level Indicator)","text":"<p>\u201cWhat we measure\u201d</p> <p>Definition: A measurement of how well a service is performing. It tells you what the user is actually experiencing.</p> <p>Example:</p> <p>Availability (e.g., % of successful requests)</p> <p>Latency (e.g., 95% of requests respond in &lt;200 ms)</p> <p>Error rate (e.g., % of failed logins)</p> <p>Throughput (e.g., requests per second)</p>"},{"location":"SRE/#slo-service-level-objective-what-we-aim-for","title":"\ud83c\udfaf SLO (Service Level Objective) \u2014 \u201cWhat we aim for\u201d","text":"<p>Definition: A target value or range for an SLI that defines the desired level of service reliability. It\u2019s what you aim to achieve.</p> <p>Example:</p> <p>The system should have 99.9% availability per month.</p> <p>The API should respond within 200 ms for 95% of requests.</p>"},{"location":"SRE/#sla-service-level-agreement-what-we-promise-legally","title":"\ud83e\udd1d SLA (Service Level Agreement) \u2014 \u201cWhat we promise (legally)\u201d","text":"<p>An SLA is usually a contract between you and your users/customers. It defines the minimum service level you guarantee \u2014 and what happens if you fail.</p> <p>\ud83d\udcbc Example:</p> <p>\u201cWe guarantee 99.5% uptime per month.</p> <p>If we go below that, we\u2019ll credit you 10% of your bill.</p>"},{"location":"SRE/#difference-between-monitoring-and-observibility","title":"Difference between Monitoring and Observibility","text":""},{"location":"SRE/#monitoring-are-we-ok-right-now","title":"\ud83d\udd0d Monitoring \u2014 \u201cAre we OK right now?\u201d","text":"<p>Definition: Monitoring means collecting and tracking predefined metrics to know if your system is healthy. It\u2019s about detecting known problems.</p> <p>Key idea: You already know what to look for.</p> <p>Examples:</p> <p>CPU usage above 90% \u2192 alert</p> <p>Website response time &gt; 2s \u2192 alert</p> <p>Error rate &gt; 1% \u2192 alert</p> <p>Goal: Detect when something goes wrong \u2014 and alert the right people.</p> <p>Tools: Prometheus, Grafana, Nagios, CloudWatch, Datadog, Zabbix</p>"},{"location":"SRE/#observability-why-arent-we-ok","title":"\ud83e\udde0 Observability \u2014 \u201cWhy aren\u2019t we OK?\u201d","text":"<p>Definition: Observability means understanding what\u2019s happening inside your system just by looking at the data it produces \u2014 even for problems you didn\u2019t predict. It\u2019s about exploration and debugging unknown issues.</p> <p>Key idea: You might not know what to look for yet.</p> <p>Examples:</p> <p>Tracing a single user\u2019s request across multiple microservices.</p> <p>Investigating why latency suddenly spiked in one region.</p> <p>Correlating logs, metrics, and traces to find the root cause.</p> <p>Goal: Help engineers ask new questions about the system and find unknown problems.</p> <p>Tools: OpenTelemetry, Grafana Tempo, Jaeger, Honeycomb, New Relic, Elastic Stack</p> <p>\u2699\ufe0f Simple Analogy</p> <p>Concept Analogy Purpose Monitoring  Like a car dashboard \u2014 it shows known indicators (fuel, speed, temperature).    Tells you when something\u2019s wrong. Observability   Like a mechanic diagnosing the engine when the car makes a strange noise.   Helps you understand why it\u2019s wrong.</p> <p>\ud83e\udde9 Summary</p> Aspect Monitoring Observability Purpose Detect known issues Explore and diagnose unknown issues Focus Metrics and alerts Logs, metrics, and traces (the \u201cthree pillars\u201d) Approach Reactive Proactive Question answered \u201cIs it working?\u201d \u201cWhy isn\u2019t it working?\u201d"},{"location":"SRE/#what-is-toil-in-sre","title":"What is Toil in SRE","text":"<p>Toil is the manual, repetitive, and operational work that keeps a system running \u2014 but doesn\u2019t add long-term value to the system.</p> <p>It\u2019s the kind of work that: - You have to do over and over again - Scales linearly with the system (more servers = more toil) - Could be automated, but isn\u2019t (yet)</p> <p>In short: Toil = repetitive work that keeps the lights on, not work that improves the lights.</p>"},{"location":"SRE/#examples-of-toil","title":"Examples of Toil","text":"Type of Work Toil Example Why It\u2019s Toil Manual Operations Restarting crashed servers every morning Repetitive and should be automated Deployment Running manual deployment commands each release Adds no lasting value Monitoring Manually checking dashboards for CPU usage Can be replaced by alerts Incident Response Manually clearing disk space every week Same problem keeps recurring Support Responding to the same user issue again and again Could be automated with self-service tools"},{"location":"SRE/#non-toil-valuable-work","title":"Non-Toil (Valuable) Work","text":"Type of Work Example Why It\u2019s Valuable Automation Writing a script to auto-restart crashed servers Reduces future toil System Improvement Improving monitoring to detect issues earlier Increases reliability Optimization Making deployments fully automated Saves time long-term <p>One of the core goals of SRE is to reduce toil through automation and process improvement.</p> <p>Rule of Thumb: Toil should be less than 50% of an SRE\u2019s time. The rest should go into improving systems so future toil decreases. Toil is doing the same manual task again and again, instead of fixing the root cause or automating it.</p>"},{"location":"SRE/#how-to-measure-toil-in-sre","title":"How to Measure Toil in SRE","text":"<p>Measuring toil means figuring out how much of your team\u2019s time is spent doing manual, repetitive operational work \u2014 instead of engineering improvements.</p> <p>The goal is to quantify toil, so you can: - Justify automation work - Prioritize improvements - Ensure SREs focus on reliability, not busywork</p>"},{"location":"SRE/#1-define-what-counts-as-toil","title":"1. Define What Counts as Toil","text":"<p>Toil Characteristics: - Manual (requires human intervention) - Repetitive (done over and over) - Reactive (responding to issues) - Tactical (short-term fix, not long-term improvement) - No enduring value (system state is the same after you finish)</p> <p>\u2705 Example (Counts as Toil): Restarting a failed service manually. \u274c Not Toil: Writing a script to auto-restart that service.</p>"},{"location":"SRE/#2-track-time-spent-on-toil","title":"2. Track Time Spent on Toil","text":"<p>Start simple: measure how much time engineers spend on toil tasks.</p> <p>Ways to track: - Self-reporting (log hours weekly) - Ticket analysis (track operational vs. project work) - Incident reviews (count repetitive manual actions)</p> <p>Example Metric: \u201cIn the past month, 40% of our team\u2019s time was spent on toil.\u201d</p>"},{"location":"SRE/#3-categorize-and-quantify-toil","title":"3. Categorize and Quantify Toil","text":"Category Example Task Estimated % of Time Deployments Manual deploys to prod 20% Incidents Restarting services 15% Monitoring Checking dashboards 10% Maintenance Clearing disk space 5%"},{"location":"SRE/#4-set-a-target-or-threshold","title":"4. Set a Target or Threshold","text":"<p>SRE best practice:  </p> <p>Toil should be less than 50% of an SRE\u2019s time.</p> <p>If it\u2019s higher, the team is likely overloaded with operational work.</p>"},{"location":"SRE/#5-reduce-toil-and-re-measure","title":"5. Reduce Toil and Re-Measure","text":"<p>Once you know where toil comes from: - Automate manual steps (scripts, CI/CD, auto-remediation) - Improve tooling (better dashboards, alerts, runbooks) - Eliminate root causes (fix recurring incidents) - Measure again every 1\u20132 months</p>"},{"location":"SRE/#example-improvement","title":"Example Improvement","text":"Metric Before After Automation Manual deployments per week 10 1 Time spent on deployment toil 8 hrs 1 hr Total toil % of team time 45% 25% <p>\u2705 This shows measurable improvement \u2014 clear ROI on automation.</p>"},{"location":"SRE/#summary","title":"Summary","text":"<p>Measure toil = track time spent on manual, repetitive work \u2192 categorize it \u2192 reduce it \u2192 track again.</p>"},{"location":"SRE/#sre-toil-tracking-template-markdown","title":"SRE Toil Tracking Template (Markdown)","text":""},{"location":"SRE/#sre-toil-tracking-template","title":"SRE Toil Tracking Template","text":"Date Engineer Toil Category Task Description Time Spent (hrs) Frequency Automation Possible? (Y/N) Comments / Next Steps 2025-11-10 Alice Incident Response Restarted crashed web service 2 Daily Y Add auto-restart script 2025-11-10 Bob Monitoring Checked disk usage manually 1 Weekly Y Add disk alert rule 2025-11-11 Carol Deployment Ran manual release to prod 3 Biweekly Y Automate CI/CD pipeline 2025-11-11 Dan Maintenance Cleared old log files 1 Weekly Y Add log rotation config"},{"location":"SRE/#summary-for-week","title":"Summary for Week","text":"Category Total Hours % of Total Notes Incident Response 5 40% Need better alerting Deployment 3 25% Automate release pipeline Monitoring 2 15% Add proactive alerting Maintenance 2 20% Improve log cleanup <p>Total Toil Hours: 12 hrs Total Team Hours: 40 hrs Toil %: 30% Goal: Keep below 50%</p>"},{"location":"aws/","title":"AWS","text":""},{"location":"aws/#3-tier-application","title":"3 Tier Application","text":""},{"location":"aws/#aws_1","title":"AWS","text":""},{"location":"aws/#deploying-a-3-tier-application-architecture-in-aws","title":"Deploying a 3-Tier Application Architecture in AWS","text":"<p>This guide explains how to deploy a production-style 3-tier architecture in AWS with:</p> <ul> <li>Route 53</li> <li>ACM (SSL)</li> <li>VPC (Public &amp; Private Subnets)</li> <li>Bastion Host</li> <li>RDS MySQL (Multi-AZ)</li> <li>Presentation Tier (Frontend)</li> <li>Application Tier (Backend)</li> <li>Auto Scaling</li> <li>CloudWatch Monitoring</li> </ul>"},{"location":"aws/#3-tier-architecture","title":"3-Tier Architecture","text":"<ol> <li>Presentation Tier</li> <li>Public ALB</li> <li> <p>EC2 instances (Frontend)</p> </li> <li> <p>Application Tier</p> </li> <li>Internal ALB</li> <li> <p>EC2 instances (Backend - Node.js)</p> </li> <li> <p>Data Tier</p> </li> <li>RDS MySQL (Multi-AZ)</li> </ol>"},{"location":"aws/#1-create-route-53-hosted-zone","title":"1\ufe0f\u20e3 Create Route 53 Hosted Zone","text":"<p>Create Hosted Zone</p> <ul> <li>Domain Name: xyz.com</li> <li>Type:</li> <li>Public Hosted Zone (Public Access)</li> <li>Private Hosted Zone (VPC Only Access)</li> </ul> <p>After creation:</p> <ul> <li>Copy assigned Name Servers (NS records)</li> <li>Go to external domain provider (e.g., GoDaddy)</li> <li>Replace existing nameservers with AWS NS values</li> </ul>"},{"location":"aws/#2-request-public-ssl-certificate-acm","title":"2\ufe0f\u20e3 Request Public SSL Certificate (ACM)","text":"<p>Navigate to:</p> <p>AWS Certificate Manager \u2192 Request Certificate</p> <p>Add domain names:</p> <p>xyz.com xyz.in  </p> <p>Choose:</p> <p>DNS Validation</p> <p>Create validation records in Route 53.</p> <p>Wait until certificate status shows:</p> <p>Issued</p>"},{"location":"aws/#3-create-vpc-and-subnets","title":"3\ufe0f\u20e3 Create VPC and Subnets","text":"<p>VPC Configuration</p> <ul> <li>Availability Zones: 2</li> <li>Public Subnets: 2</li> <li>Private Subnets: 4 (2 per AZ)</li> <li>NAT Gateway: 1 (single AZ)</li> <li>Endpoints: Not used</li> </ul>"},{"location":"aws/#4-subnet-configuration","title":"4\ufe0f\u20e3 Subnet Configuration","text":"<p>Enable for Public Subnets:</p> <p>Auto-Assign Public IP \u2192 Enabled</p> <p>Private Subnets:</p> <p>Auto-Assign Public IP \u2192 Disabled</p>"},{"location":"aws/#5-security-groups-configuration","title":"5\ufe0f\u20e3 Security Groups Configuration","text":"<p>Bastion Host SG</p> <p>Inbound: - SSH (22) \u2192 Allow from specific IP (Production Best Practice)</p>"},{"location":"aws/#presentation-layer-alb-sg","title":"Presentation Layer ALB SG","text":"<p>Inbound: - HTTP (80) \u2192 0.0.0.0/0</p> <p>Note: HTTPS can be configured later using CloudFront + ACM.</p>"},{"location":"aws/#presentation-layer-ec2-sg","title":"Presentation Layer EC2 SG","text":"<p>Inbound: - SSH (22) \u2192 Bastion SG - HTTP (80) \u2192 Presentation ALB SG</p>"},{"location":"aws/#application-layer-alb-sg","title":"Application Layer ALB SG","text":"<p>Inbound: - HTTP (80) \u2192 Presentation EC2 SG</p>"},{"location":"aws/#application-layer-ec2-sg","title":"Application Layer EC2 SG","text":"<p>Inbound: - SSH (22) \u2192 Bastion SG - TCP (3200) \u2192 Application ALB SG</p>"},{"location":"aws/#data-tier-sg","title":"Data Tier SG","text":"<p>Inbound: - MySQL (3306) \u2192 Application EC2 SG - MySQL (3306) \u2192 Bastion SG</p>"},{"location":"aws/#6-launch-bastion-host","title":"6\ufe0f\u20e3 Launch Bastion Host","text":"<p>EC2 \u2192 Launch Instance</p> <ul> <li>AMI \u2192 Amazon Linux</li> <li>Key Pair \u2192 Create new</li> <li>VPC \u2192 Public Subnet</li> <li>Security Group \u2192 Bastion SG</li> </ul>"},{"location":"aws/#7-create-db-subnet-group","title":"7\ufe0f\u20e3 Create DB Subnet Group","text":"<p>RDS \u2192 Subnet Groups \u2192 Create</p> <ul> <li>Select VPC</li> <li>Select Private Subnets</li> </ul>"},{"location":"aws/#8-create-rds-mysql-multi-az","title":"8\ufe0f\u20e3 Create RDS MySQL (Multi-AZ)","text":"<p>RDS \u2192 Create Database</p> <ul> <li>Engine \u2192 MySQL</li> <li>Deployment \u2192 Multi-AZ (Primary + Standby)</li> <li>Master Username</li> <li>Master Password</li> <li>Storage</li> <li>VPC</li> <li>DB Subnet Group</li> <li>Security Group \u2192 Data Tier SG</li> </ul>"},{"location":"aws/#9-connect-to-rds","title":"9\ufe0f\u20e3 Connect to RDS","text":""},{"location":"aws/#ssh-to-bastion","title":"SSH to Bastion","text":"<pre><code>ssh -i key.pem ec2-user@bastion-public-ip\n</code></pre>"},{"location":"aws/#connect-to-mysql","title":"Connect to MySQL","text":"<pre><code>mysql -h &lt;rds-endpoint&gt; -u masteruser -p\n</code></pre>"},{"location":"aws/#create-app-database-user","title":"Create App Database &amp; User","text":"<pre><code>CREATE DATABASE appdb;\n\nCREATE USER 'appuser'@'%' IDENTIFIED BY 'password';\n\nGRANT ALL PRIVILEGES ON appdb.* TO 'appuser'@'%';\n\nFLUSH PRIVILEGES;\n</code></pre> <p>All application operations should use appuser.</p>"},{"location":"aws/#setup-presentation-tier","title":"\ud83d\udd1f Setup Presentation Tier","text":""},{"location":"aws/#create-launch-template","title":"Create Launch Template","text":"<ul> <li>AMI</li> <li>Key Pair</li> <li>Public Subnet</li> <li>Presentation EC2 SG</li> </ul>"},{"location":"aws/#user-data-script","title":"User Data Script","text":"<pre><code>#!/bin/bash\nyum update -y\nyum install nginx -y\nsystemctl start nginx\nsystemctl enable nginx\n\nINSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id)\nPUBLIC_IP=$(curl http://169.254.169.254/latest/meta-data/public-ipv4)\n\necho \"Instance ID: $INSTANCE_ID\" &gt; /usr/share/nginx/html/index.html\necho \"Public IP: $PUBLIC_IP\" &gt;&gt; /usr/share/nginx/html/index.html\n</code></pre>"},{"location":"aws/#11-create-presentation-target-group","title":"1\ufe0f\u20e31\ufe0f\u20e3 Create Presentation Target Group","text":"<ul> <li>Target Type \u2192 Instance</li> <li>Protocol \u2192 HTTP</li> <li>Port \u2192 80</li> <li>VPC \u2192 Select VPC</li> <li>Health Check \u2192 /</li> </ul>"},{"location":"aws/#12-create-presentation-load-balancer-internet-facing","title":"1\ufe0f\u20e32\ufe0f\u20e3 Create Presentation Load Balancer (Internet Facing)","text":"<p>ALB Configuration:</p> <ul> <li>Scheme \u2192 Internet Facing</li> <li>Subnets \u2192 Public Subnets</li> <li>Security Group \u2192 Presentation ALB SG</li> <li>Listener \u2192 HTTP \u2192 Forward to Presentation Target Group</li> </ul>"},{"location":"aws/#13-create-auto-scaling-group-presentation-tier","title":"1\ufe0f\u20e33\ufe0f\u20e3 Create Auto Scaling Group (Presentation Tier)","text":"<ul> <li>Launch Template \u2192 Presentation Template</li> <li>Subnets \u2192 Public</li> <li>Attach \u2192 Presentation Target Group</li> <li>Enable Health Check</li> <li>Enable CloudWatch</li> <li>Min Capacity \u2192 2</li> <li>Max Capacity \u2192 4</li> <li>Target CPU Utilization \u2192 50%</li> </ul>"},{"location":"aws/#14-test-auto-scaling","title":"1\ufe0f\u20e34\ufe0f\u20e3 Test Auto Scaling","text":"<p>SSH to Presentation Instance:</p> <pre><code>sudo yum install stress -y\nstress --cpu 2 --timeout 300\n</code></pre> <p>Monitor CloudWatch for scaling activity.</p>"},{"location":"aws/#15-setup-application-tier","title":"1\ufe0f\u20e35\ufe0f\u20e3 Setup Application Tier","text":""},{"location":"aws/#create-launch-template_1","title":"Create Launch Template","text":"<ul> <li>Instance Type \u2192 t2.micro</li> <li>Private Subnet</li> <li>Application EC2 SG</li> </ul>"},{"location":"aws/#user-data-script_1","title":"User Data Script","text":"<pre><code>#!/bin/bash\nyum update -y\nyum install git nodejs npm -y\n\ngit clone &lt;repository-url&gt;\ncd app\nnpm install\n\ncat &lt;&lt;EOF &gt; .env\nDB_HOST=&lt;rds-endpoint&gt;\nDB_USER=appuser\nDB_PASS=password\nDB_NAME=appdb\nEOF\n\nnpm start\n</code></pre>"},{"location":"aws/#16-create-application-target-group","title":"1\ufe0f\u20e36\ufe0f\u20e3 Create Application Target Group","text":"<ul> <li>Target Type \u2192 Instance</li> <li>Protocol \u2192 HTTP</li> <li>Port \u2192 3200</li> <li>VPC \u2192 Select VPC</li> <li>Health Check \u2192 /</li> </ul>"},{"location":"aws/#17-create-application-load-balancer-internal","title":"1\ufe0f\u20e37\ufe0f\u20e3 Create Application Load Balancer (Internal)","text":"<ul> <li>Scheme \u2192 Internal</li> <li>Subnets \u2192 Private Subnets</li> <li>Security Group \u2192 Application ALB SG</li> <li>Listener \u2192 HTTP \u2192 Forward to Application Target Group</li> </ul>"},{"location":"aws/#18-create-auto-scaling-group-application-tier","title":"1\ufe0f\u20e38\ufe0f\u20e3 Create Auto Scaling Group (Application Tier)","text":"<ul> <li>Launch Template \u2192 Application Template</li> <li>Subnets \u2192 Private</li> <li>Attach \u2192 Application Target Group</li> <li>Enable Health Check</li> <li>Enable CloudWatch</li> <li>Min \u2192 2</li> <li>Max \u2192 4</li> <li>Target CPU \u2192 50%</li> </ul>"},{"location":"aws/#19-verify-application-ec2","title":"1\ufe0f\u20e39\ufe0f\u20e3 Verify Application EC2","text":"<p>SSH:</p> <p>Bastion \u2192 Application EC2</p> <p>Check logs:</p> <pre><code>cd app\ncat logs/app.log\n</code></pre>"},{"location":"aws/#20-update-presentation-tier-frontend","title":"2\ufe0f\u20e30\ufe0f\u20e3 Update Presentation Tier (Frontend)","text":"<p>Create New Launch Template Version.</p>"},{"location":"aws/#updated-user-data","title":"Updated User Data","text":"<pre><code>#!/bin/bash\nyum install git nodejs npm -y\n\ngit clone &lt;frontend-repo&gt;\ncd frontend\nnpm install\n\ncat &lt;&lt;EOF &gt; .env\nAPI_URL=http://&lt;internal-application-alb&gt;\nDOMAIN=xyz.com\nSUBDOMAIN=xyz.in\nEOF\n\nnpm start\n</code></pre> <p>Update Launch Template Version in ASG.</p>"},{"location":"aws/#21-deploy-latest-version","title":"2\ufe0f\u20e31\ufe0f\u20e3 Deploy Latest Version","text":"<p>Auto Scaling \u2192 Presentation ASG</p> <ul> <li>Select Latest Launch Template Version</li> <li>Terminate Old Instances</li> </ul> <p>Access:</p> <p>http://xyz.com</p>"},{"location":"aws/#22-setup-cloudwatch","title":"2\ufe0f\u20e32\ufe0f\u20e3 Setup CloudWatch","text":""},{"location":"aws/#create-iam-role","title":"Create IAM Role","text":"<ul> <li>Attach Policy \u2192 CloudWatchLogsFullAccess</li> <li>Attach Role \u2192 EC2</li> </ul>"},{"location":"aws/#23-create-log-group","title":"2\ufe0f\u20e33\ufe0f\u20e3 Create Log Group","text":"<p>CloudWatch \u2192 Log Groups \u2192 Create</p>"},{"location":"aws/#24-install-cloudwatch-agent","title":"2\ufe0f\u20e34\ufe0f\u20e3 Install CloudWatch Agent","text":"<pre><code>sudo yum install amazon-cloudwatch-agent -y\n</code></pre> <p>Attach IAM role to EC2 instance.</p> <p>Configure CloudWatch agent.</p>"},{"location":"aws/#25-view-logs","title":"2\ufe0f\u20e35\ufe0f\u20e3 View Logs","text":"<p>CloudWatch \u2192 Log Groups</p> <p>Monitor:</p> <ul> <li>Application logs</li> <li>System logs</li> <li>Metrics</li> </ul>"},{"location":"aws/#final-architecture-flow","title":"\u2705 Final Architecture Flow","text":"<p>Route53    \u2193 Public ALB    \u2193 Presentation ASG (Public Subnets)    \u2193 Internal ALB    \u2193 Application ASG (Private Subnets)    \u2193 RDS MySQL (Multi-AZ)  </p>"},{"location":"aws/#production-recommendations","title":"\ud83c\udfaf Production Recommendations","text":"<ul> <li>Enable HTTPS (ACM + ALB)</li> <li>Use AWS Secrets Manager for DB credentials</li> <li>Enable Automated RDS Backups</li> <li>Use WAF for security</li> <li>Consider CloudFront for CDN</li> <li>Implement Infrastructure as Code (Terraform)</li> </ul> <p>\ud83d\ude80 You now have a scalable, secure, production-ready 3-tier architecture in AWS.</p>"},{"location":"aws/#scenario-based-questions","title":"Scenario Based Questions","text":""},{"location":"aws/#read-and-write-separate-database-how-to-scale-application-to-1m-users","title":"Read and Write Separate Database (How to scale application to 1M users)","text":"<p>Scaling AWS RDS with Read Replicas in EKS Architecture</p> <p>I want to understand how database architecture works in AWS. Suppose I have a web application hosted on AWS with an RDS database, and everything is functioning correctly.</p> <p>In what scenarios should I use two separate databases \u2014 one primary database for write operations and another read replica for read-only operations?</p> <p>How does this architecture work internally, and how can it be implemented in AWS?</p> <p>1\ufe0f\u20e3 Current Scenario: Single RDS Instance</p> <pre><code>Users \u2192 ALB \u2192 EC2 / EKS \u2192 RDS (Single DB)\n</code></pre> <p>All operations go to one database:</p> <ul> <li><code>SELECT</code> (Read)</li> <li><code>INSERT</code></li> <li><code>UPDATE</code></li> <li><code>DELETE</code></li> </ul> <p>This works perfectly when:</p> <ul> <li>Traffic is low to medium</li> <li>Read/write ratio is balanced</li> <li>Database CPU and connections are within limits</li> </ul>"},{"location":"aws/#2-when-do-you-need-a-separate-write-db-and-read-replica","title":"\ud83d\udea8 2\ufe0f\u20e3 When Do You Need a Separate Write DB and Read Replica?","text":"<p>You introduce Read Replicas when scaling read operations becomes necessary.</p> <p>\ud83d\udd39 Condition 1: Heavy Read Traffic</p> <p>Example:</p> <ul> <li>E-commerce application</li> <li>10,000 users browsing products (reads)</li> <li>500 users placing orders (writes)</li> <li>Reads are 20x more than writes</li> </ul> <p>Problem:</p> <ul> <li>Primary DB overloaded handling <code>SELECT</code> queries</li> <li>CPU spikes</li> <li>Slow queries</li> <li>Application performance degradation</li> </ul> <p>\ud83d\udc49 Solution: Offload read traffic to replicas</p> <p>\ud83d\udd39 Condition 2: Reporting &amp; Analytics Queries</p> <p>If:</p> <ul> <li>BI team runs large <code>SELECT</code> queries</li> <li>Long-running reporting jobs</li> <li>Heavy analytical workloads</li> </ul> <p>These queries:</p> <ul> <li>Lock tables</li> <li>Consume CPU</li> <li>Impact production users</li> </ul> <p>\ud83d\udc49 Run them on a Read Replica</p> <p>\ud83d\udd39 Condition 3: Scaling Without Sharding</p> <p>Instead of vertically scaling (larger DB instance), use horizontal scaling with read replicas \u2014 often more cost-efficient.</p>"},{"location":"aws/#3-architecture-with-read-replica","title":"\ud83c\udfd7 3\ufe0f\u20e3 Architecture with Read Replica","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Primary   \u2502\n                    \u2502   (Write)  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                    Replication\n                          \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                   \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 Read Replica\u2502      \u2502 Read Replica\u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"aws/#application-flow","title":"Application Flow","text":"<pre><code>App Server\n   \u2502\n   \u251c\u2500\u2500 Write \u2192 Primary DB\n   \u2502\n   \u2514\u2500\u2500 Read \u2192 Read Replica\n</code></pre> <p>\ud83d\udd01 4\ufe0f\u20e3 How Read Replica Works Internally</p> <p>For MySQL / PostgreSQL:</p> <ul> <li>Asynchronous replication</li> <li>Binary logs (binlog)</li> </ul>"},{"location":"aws/#replication-flow","title":"Replication Flow","text":"<ol> <li>Client writes to Primary DB</li> <li>Primary writes changes to binlog</li> <li>Replica reads binlog</li> <li>Replica applies changes</li> </ol> <p>\u26a0 Important:</p> <ul> <li>Replication is asynchronous</li> <li>Small delay (milliseconds to seconds)</li> <li>Called replication lag</li> <li>Provides eventual consistency, not strong consistency</li> </ul>"},{"location":"aws/#when-not-to-use-read-replicas","title":"\ud83c\udfaf When NOT to Use Read Replicas","text":"<p>\u274c Write-heavy workloads \u274c Applications requiring strict real-time consistency \u274c Systems that cannot tolerate replication lag  </p>"},{"location":"aws/#multi-az-vs-read-replica-critical-difference","title":"\ud83c\udd9a Multi-AZ vs Read Replica (Critical Difference)","text":"<p>\ud83d\udd39 Multi-AZ</p> <ul> <li>High Availability</li> <li>Standby DB</li> <li>Not used for reads</li> <li>Automatic failover</li> </ul> <p>\ud83d\udd39 Read Replica</p> <ul> <li>Used for scaling reads</li> <li>Application connects to it</li> <li>No automatic failover (unless promoted)</li> </ul> <p>\u2699 5\ufe0f\u20e3 How to Implement Read Replica in AWS</p> <p>Option 1: AWS Console</p> <ol> <li>Go to RDS</li> <li>Select your database</li> <li>Click Actions</li> <li>Choose Create Read Replica</li> <li>Select instance size</li> <li>Deploy</li> </ol> <p>Option 2: AWS CLI</p> <pre><code>aws rds create-db-instance-read-replica \\\n    --db-instance-identifier mydb-replica \\\n    --source-db-instance-identifier mydb-primary \\\n    --db-instance-class db.t3.medium\n</code></pre> <p>Option 3: Terraform (Recommended for DevOps)</p> <pre><code>resource \"aws_db_instance\" \"primary\" {\n  identifier         = \"mydb-primary\"\n  engine             = \"mysql\"\n  instance_class     = \"db.t3.medium\"\n  allocated_storage  = 20\n  username           = \"admin\"\n  password           = \"password\"\n}\n\nresource \"aws_db_instance\" \"replica\" {\n  identifier          = \"mydb-replica\"\n  replicate_source_db = aws_db_instance.primary.identifier\n  instance_class      = \"db.t3.medium\"\n}\n</code></pre> <p>\ud83e\udde0 6\ufe0f\u20e3 Application-Level Implementation (Most Important)</p> <p>AWS does NOT automatically split read/write traffic.</p> <p>Your application must handle it.</p>"},{"location":"aws/#example-python-concept","title":"Example (Python Concept)","text":"<pre><code>write_db = connect(primary_endpoint)\nread_db = connect(replica_endpoint)\n\n# Write operation\nwrite_db.execute(\"INSERT INTO users VALUES (...)\")\n\n# Read operation\nread_db.execute(\"SELECT * FROM users\")\n</code></pre>"},{"location":"aws/#real-world-example","title":"\ud83c\udfe2 Real-World Example","text":"<p>Netflix-like workload:</p> <ul> <li>Browse catalog \u2192 Read</li> <li>Watch list \u2192 Read</li> <li>Signup \u2192 Write</li> <li>Like button \u2192 Write</li> </ul> <p>If 90% traffic is reads:</p> <pre><code>1 Primary\n5 Read Replicas\n</code></pre>"},{"location":"aws/#scaling-strategy","title":"\ud83d\udcca Scaling Strategy","text":"<p>Start simple:</p> <pre><code>1 Primary\n0 Replica\n</code></pre> <p>If CPU consistently &gt; 70% due to reads:</p> <pre><code>1 Primary\n1 Replica\n</code></pre> <p>As traffic increases:</p> <pre><code>1 Primary\n3 Replicas\n</code></pre>"},{"location":"azure/","title":"All about Azure Cloud","text":""},{"location":"azure/#cloud-computing","title":"Cloud Computing","text":""},{"location":"azure/#what-is-cloud","title":"What is Cloud","text":"<p>In simpler terms, imagine the cloud as a vast, virtual space where you can store files, run software, and access various services over the internet.</p> <p>It's like having a powerful computer somewhere out there on the web that you can use for tasks without needing to own or physically manage the hardware. This allows users to access data and applications from anywhere with an internet connection.</p>"},{"location":"azure/#what-is-cloud-computing","title":"What is Cloud Computing","text":"<p>Cloud computing is a technology model that involves the delivery of computing services over the internet. Instead of owning and maintaining physical servers and infrastructure, users can access and use computing resources, applications, and storage provided by either third-party service providers (public cloud) or their own organization (private cloud) through the internet. These services are hosted in data centers located around the world.</p> <p>In essence, cloud computing can involve both third-party providers (public cloud) and an organization's internal resources (private cloud). The distinction lies in whether the computing resources are shared among multiple customers (public cloud) or dedicated to a single organization (private cloud). The flexibility of cloud computing allows organizations to choose the deployment model that best aligns with their needs and requirements.</p>"},{"location":"azure/#public-cloud","title":"Public Cloud","text":"<p>Who Uses It: Everyone, like individuals, businesses, and organizations.</p> <p>What It's Like: Imagine a giant, shared computer space on the internet. It's like using apps, storing files, or doing tasks on the internet that anyone can access.</p> <p>Example: Think of Google Drive or Amazon Web Services (AWS).</p>"},{"location":"azure/#private-cloud","title":"Private Cloud","text":"<p>Who Uses It: One specific organization or business.</p> <p>What It's Like: Picture having your own personal, private computer space. It's like a digital clubhouse where only you and your team have access. Others can't just drop in.</p> <p>Example: A company using its own server for all its digital needs.</p>"},{"location":"azure/#hybrid-cloud","title":"Hybrid Cloud","text":"<p>Who Uses It: A mix of everyone, depending on needs.</p> <p>What It's Like: It's like having your private computer space, but sometimes you use the shared internet space too.</p> <p>Example: A business storing sensitive data in its private space but using the public cloud for hosting web-application or using virtual machines from other cloud providers.</p>"},{"location":"azure/#cloud-service-models","title":"Cloud Service Models","text":""},{"location":"azure/#infrastructure-as-a-service","title":"Infrastructure as a Service","text":"<p>IaaS is a cloud computing model that provides virtualized computing resources over the internet. In Azure, IaaS offerings include virtual machines, storage, and networking components. Users have more control over the infrastructure but are responsible for managing and maintaining the operating system, middleware, and applications.</p>"},{"location":"azure/#key-characteristics-of-azure-iaas","title":"Key Characteristics of Azure IaaS","text":"<ul> <li> <p>Scalability: Easily scale resources up or down based on demand.</p> </li> <li> <p>Full Control: Users have control over the underlying infrastructure, including operating systems and applications.</p> </li> <li> <p>Flexibility: IaaS is suitable for a wide range of applications, offering flexibility in terms of technology stack.</p> </li> </ul>"},{"location":"azure/#platform-as-a-service","title":"Platform as a Service","text":"<p>PaaS is a cloud computing model that provides a platform allowing customers to develop, run, and manage applications without dealing with the complexity of underlying infrastructure. In Azure, PaaS offerings include Azure App Service, Azure SQL Database, and Azure Functions.</p>"},{"location":"azure/#key-characteristics-of-azure-paas","title":"Key Characteristics of Azure PaaS","text":"<ul> <li> <p>Simplified Development: Developers can focus on coding and application logic, while Azure manages the underlying infrastructure.</p> </li> <li> <p>Automatic Scaling: PaaS offerings often include built-in scaling capabilities, automatically adjusting resources based on demand.</p> </li> <li> <p>Reduced Maintenance: Azure handles tasks like patching, updates, and maintenance, freeing up resources for innovation.</p> </li> </ul>"},{"location":"azure/#software-as-a-service","title":"Software as a Service","text":"<p>SaaS is a cloud computing model that delivers software applications over the internet. Users can access the software through a web browser without the need for installation or maintenance. In Azure, SaaS offerings include Microsoft 365, Dynamics 365, and many third-party applications.</p>"},{"location":"azure/#key-characteristics-of-azure-saas","title":"Key Characteristics of Azure SaaS","text":"<ul> <li> <p>Accessibility: Access software applications from any device with an internet connection.</p> </li> <li> <p>Managed by Providers: SaaS providers handle maintenance, updates, and security, reducing the burden on end-users.</p> </li> <li> <p>Subscription-Based: SaaS applications are typically offered on a subscription basis, allowing users to pay for what they use.</p> </li> </ul>"},{"location":"azure/#choosing-the-right-model-in-azure","title":"Choosing the Right Model in Azure","text":"<p>When deciding between IaaS, PaaS, and SaaS in Azure, consider factors such as:</p> <ul> <li> <p>Development Needs: Choose PaaS for streamlined development, IaaS for more control, and SaaS for off-the-shelf solutions.</p> </li> <li> <p>Maintenance Preferences: If you want to minimize maintenance tasks, opt for PaaS or SaaS.</p> </li> <li> <p>Resource Control: Choose IaaS if you need more control over the underlying infrastructure.</p> </li> <li> <p>Cost Considerations: Evaluate pricing models for each service and choose based on your budget and usage patterns.</p> </li> </ul>"},{"location":"azure/#iaas-paas-saas","title":"IAAS-PAAS-SAAS","text":""},{"location":"azure/#azure-cli","title":"Azure CLI","text":"<ol> <li>azure cli to get list of secret</li> </ol> <pre><code>az keyvault secret list --vault-name &lt;key_vault_name&gt; --query \"[].id\" -o tsv\n</code></pre>"},{"location":"azure/#azure-resources","title":"Azure Resources","text":"<p>Azure resources are the building blocks of your cloud infrastructure in Microsoft Azure. These resources can be virtual machines, databases, storage accounts, or any other service offered by Azure. Each resource is a manageable item in Azure, and they are provisioned and managed individually.</p>"},{"location":"azure/#resource-groups-in-azure","title":"Resource Groups in Azure","text":"<p>A Resource Group in Azure is a logical container for resources that share the same lifecycle, permissions, and policies. It helps you organize and manage related Azure resources efficiently. Resources within a group can be deployed, updated, and deleted together as a single management unit.</p>"},{"location":"azure/#key-points-about-resource-groups","title":"Key Points about Resource Groups","text":"<ul> <li> <p>Lifecycle Management: Resources within a group can be managed collectively, making it easy to handle deployments, updates, and deletions.</p> </li> <li> <p>Resource Organization: Grouping resources based on projects, environments, or applications helps keep your Azure environment well-organized.</p> </li> <li> <p>Role-Based Access Control (RBAC): Permissions and access control are applied at the resource group level, allowing you to manage who can access and modify resources within a group.</p> </li> </ul>"},{"location":"azure/#azure-resource-manager-arm-overview","title":"Azure Resource Manager (ARM) Overview","text":"<p>Azure Resource Manager (ARM) is the deployment and management service for Azure. It provides a consistent management layer that enables you to deploy resources with declarative templates. ARM templates describe the resources you need and their configurations, allowing you to deploy and update resources in a predictable manner.</p>"},{"location":"azure/#key-features-of-azure-resource-manager","title":"Key Features of Azure Resource Manager","text":"<ul> <li> <p>Template-Based Deployment: ARM uses JSON templates to define the infrastructure and configuration of your Azure resources. This enables repeatable and consistent deployments.</p> </li> <li> <p>Dependency Management: ARM automatically handles dependencies between resources, ensuring they are deployed in the correct order.</p> </li> <li> <p>Rollback and Roll-forward: In case of deployment failures, ARM can automatically roll back changes to maintain the desired state, or roll forward to the last known good state.</p> </li> <li> <p>Tagging and Categorization: You can use tags to label and categorize resources, making it easier to manage and organize your Azure environment.</p> </li> </ul> <p>Note: Understanding Azure resources, resource groups, and Azure Resource Manager is fundamental to effectively utilize and manage your resources in the Azure cloud.</p>"},{"location":"azure/#azure-resource-hierarchy","title":"Azure Resource Hierarchy","text":""},{"location":"azure/#virtualization","title":"Virtualization","text":""},{"location":"azure/#background","title":"Background","text":"<p>In traditional computing, a single physical server runs a single operating system, and applications are installed directly on that OS. This approach has limitations, such as under utilization of hardware resources, difficulty in managing multiple servers, and lack of flexibility in scaling.</p> <p>Virtualization addresses these challenges by introducing a layer of abstraction between the hardware and the operating system. It enables the creation of multiple virtual instances, each running its own operating system and applications, on a single physical server. This technology has become fundamental in modern data centers and cloud computing environments.</p>"},{"location":"azure/#components-of-virtualization","title":"Components of Virtualization","text":"<ol> <li>Hypervisor (Virtual Machine Monitor):</li> <li>The hypervisor is a crucial component of virtualization. It sits between the hardware and the operating systems, managing and allocating resources to virtual machines (VMs).</li> <li> <p>There are two types of hypervisors: Type 1 (bare-metal) runs directly on the hardware, while Type 2 (hosted) runs on top of an existing operating system.</p> </li> <li> <p>Virtual Machines (VMs):</p> </li> <li>VMs are the instances created by the hypervisor. Each VM operates as an independent computer with its own virtualized hardware, including CPU, memory, storage, and network interfaces.</li> <li>Multiple VMs can run on a single physical server, allowing for efficient resource utilization.</li> </ol>"},{"location":"azure/#key-concepts-in-virtualization","title":"Key Concepts in Virtualization","text":"<ol> <li>Server Virtualization:</li> <li> <p>In server virtualization, a physical server is divided into multiple VMs, each running its own OS. This allows for better utilization of hardware resources and easier management of servers.</p> </li> <li> <p>Resource Pooling:</p> </li> <li> <p>Virtualization enables the pooling of physical resources, such as CPU, memory, and storage. These resources can be dynamically allocated to VMs based on demand.</p> </li> <li> <p>Isolation:</p> </li> <li> <p>VMs operate independently of each other. This isolation ensures that issues in one VM do not affect others, providing a more secure and stable environment.</p> </li> <li> <p>Snap shotting and Cloning:</p> </li> <li>Virtualization allows the creation of snapshots, which capture the state of a VM at a specific point in time. This facilitates easy backup and recovery. Cloning enables the rapid duplication of VMs for scalability.</li> </ol>"},{"location":"azure/#types-of-virtual-machines-on-azure","title":"Types of Virtual Machines on Azure","text":"<p>Azure provides a variety of virtual machine (VM) offerings to cater to different workload requirements. Each VM type is designed with specific hardware configurations to meet diverse performance and scalability needs.</p>"},{"location":"azure/#general-purpose-vms","title":"General Purpose VMs","text":"<p>Example: Standard_D2s_v3</p> <ul> <li> <p>Description: General-purpose VMs are well-balanced machines suitable for a variety of workloads. They offer a good balance of CPU-to-memory ratio and are suitable for development, testing, and small to medium-sized databases.</p> </li> <li> <p>Use Case: Hosting websites, lightweight applications, or development and testing environments.</p> </li> </ul>"},{"location":"azure/#compute-optimized-vms","title":"Compute Optimized VMs","text":"<p>Example: Standard_F2s_v2</p> <ul> <li> <p>Description: Compute optimized VMs are designed for compute-intensive workloads that require high CPU power. They provide a high CPU-to-memory ratio, making them suitable for data analytics and computational tasks.</p> </li> <li> <p>Use Case: Batch processing, gaming applications, and other CPU-intensive workloads.</p> </li> </ul>"},{"location":"azure/#memory-optimized-vms","title":"Memory Optimized VMs","text":"<p>Example: Standard_E16s_v3</p> <ul> <li> <p>Description: Memory optimized VMs are tailored for memory-intensive applications. They provide a high memory-to-CPU ratio, making them suitable for databases, in-memory caching, and analytics.</p> </li> <li> <p>Use Case: Running large databases, in-memory caching, and analytics applications.</p> </li> </ul>"},{"location":"azure/#storage-optimized-vms","title":"Storage Optimized VMs","text":"<p>Example: Standard_L8s_v2</p> <ul> <li> <p>Description: Storage optimized VMs are designed for workloads that require high storage throughput and I/O performance. They provide high local disk throughput, making them suitable for big data and large databases.</p> </li> <li> <p>Use Case: Big data applications, data warehousing, and large-scale databases.</p> </li> </ul>"},{"location":"azure/#gpu-vms","title":"GPU VMs","text":"<p>Example: Standard_NC6s_v3</p> <ul> <li> <p>Description: GPU (Graphics Processing Unit) VMs are equipped with powerful graphics processors, suitable for graphics-intensive applications and parallel processing tasks.</p> </li> <li> <p>Use Case: Machine learning, graphics rendering, and simulations that require GPU acceleration.</p> </li> </ul>"},{"location":"azure/#high-performance-compute-vms","title":"High-Performance Compute VMs","text":"<p>Example: Standard_H16r</p> <ul> <li> <p>Description: High-Performance Compute VMs are designed for demanding, parallel processing and high-performance computing (HPC) applications.</p> </li> <li> <p>Use Case: Simulations, modeling, and scenarios that require massive parallel processing.</p> </li> </ul>"},{"location":"azure/#burstable-vms","title":"Burstable VMs","text":"<p>Example: B1s</p> <ul> <li> <p>Description: Burstable VMs provide a baseline level of CPU performance with the ability to burst above the baseline for a certain period. They are cost-effective for workloads with varying CPU usage.</p> </li> <li> <p>Use Case: Development and testing environments, small websites, and applications with variable workloads. Benefits of Virtualization</p> </li> <li> <p>Server Consolidation:</p> </li> <li> <p>Multiple VMs can run on a single physical server, reducing the need for a large number of physical machines. This leads to cost savings and energy efficiency.</p> </li> <li> <p>Flexibility and Scalability:</p> </li> <li> <p>Virtualization allows for the easy creation, modification, and scaling of VMs. This flexibility is essential in dynamic computing environments.</p> </li> <li> <p>Disaster Recovery:</p> </li> <li> <p>Virtualization simplifies disaster recovery by enabling the quick restoration of VMs from snapshots or backups.</p> </li> <li> <p>Resource Optimization:</p> </li> <li> <p>Resources can be allocated and de-allocated dynamically based on workload, optimizing resource utilization.</p> </li> <li> <p>Testing and Development:</p> </li> <li>Virtualization provides a sandbox for testing and development. VMs can be easily created, modified, and discarded without affecting the production environment.</li> </ul>"},{"location":"azure/#azure-networking","title":"Azure Networking","text":""},{"location":"azure/#virtual-network","title":"Virtual Network","text":"<p>A Virtual Network (VNet) in Azure is a logically isolated network that securely connects Azure resources and extends on-premises networks. </p> <p>Key features include:</p> <ul> <li> <p>Isolation: VNets provide isolation at the network level for segmenting resources and controlling traffic.</p> </li> <li> <p>Subnet: Divide a VNet into subnets for resource organization and traffic control.</p> </li> <li> <p>Address Space: VNets have an address space defined using CIDR notation, determining the IP address range.</p> </li> </ul>"},{"location":"azure/#subnets-and-cidr","title":"Subnets and CIDR","text":"<p>Subnets : Subnets are subdivisions of a Virtual Network, allowing for better organization and traffic management.</p> <p>A subnet (short for subnetwork) is a smaller network inside a larger network. It divides a big IP network into smaller, manageable segments.</p> <p>Example:</p> <pre><code>- You have a network: `192.168.0.0/16`  \n- You can split it into smaller subnets like:\n  - `192.168.1.0/24`\n  - `192.168.2.0/24`\n  - `192.168.3.0/24`\n\nEach `/24` subnet can hold up to **254 hosts**.\n</code></pre>"},{"location":"azure/#main-uses-of-subnets","title":"Main Uses of Subnets","text":"Purpose Explanation 1. Better Network Management Splitting a large network makes it easier to manage \u2014 you can isolate departments, environments (dev/test/prod), or regions. 2. Improved Security You can apply firewall rules or access controls per subnet, limiting communication between groups (e.g., only DB subnet can talk to App subnet). 3. Efficient IP Address Usage Avoid wasting IP addresses \u2014 allocate only as many as you need for each subnet. 4. Reduced Network Congestion Broadcast traffic (like ARP requests) stays inside a subnet, preventing unnecessary traffic across the whole network. 5. Easier Troubleshooting Problems are contained within one subnet, making it easier to identify and fix issues. 6. Logical Separation Commonly used to separate layers \u2014 e.g. Web, App, and Database tiers."},{"location":"azure/#example-in-real-life","title":"Example in Real Life","text":"<p>In a cloud environment like Azure or AWS:</p> <ul> <li> <p>You might have a Virtual Network (VNet) or VPC: 10.0.0.0/16</p> </li> <li> <p>And you divide it into subnets:</p> </li> </ul> <p>10.0.1.0/24 \u2192 Web subnet 10.0.2.0/24 \u2192 App subnet 10.0.3.0/24 \u2192 Database subnet</p> <p>Then you apply: - NSG/Security Group rules - Route tables - Different availability zones or gateways  </p> <p>Each subnet is isolated but can communicate via configured routing.</p> <p>Analogy</p> <p>Imagine a large office building (the whole network) \ud83c\udfe2 - Each floor is a subnet \u2014 it has its own rooms (devices) and local rules. - People can move between floors (routing), but each floor controls its own space and traffic.</p> <p>In Short: A subnet divides a large network into smaller, isolated sections \u2014 improving security, **performan</p> <p>CIDR (Classless Inter-Domain Routing) : CIDR notation represents IP addresses and their routing prefix, specifying the range of IP addresses for a network.</p>"},{"location":"azure/#routes-and-route-tables","title":"Routes and Route Tables","text":"<p>Routes : Routes dictate how network traffic is directed, specifying the destination and next hop.</p> <p>Route Tables : Route Tables are collections of routes associated with subnets, enabling custom routing rules.</p>"},{"location":"azure/#network-security-groups-nsgs","title":"Network Security Groups (NSGs)","text":"<p>NSGs are fundamental for Azure's network security, allow filtering of inbound and outbound traffic.</p> <p>Key aspects include:</p> <ul> <li> <p>Rules: NSGs define allowed or denied traffic based on source, destination, port, and protocol.</p> </li> <li> <p>Default Rules: NSGs have default rules for controlling traffic within the Virtual Network and between subnets.</p> </li> <li> <p>Association: NSGs can be associated with subnets or individual network interfaces.</p> </li> </ul>"},{"location":"azure/#application-security-groups-asgs","title":"Application Security Groups (ASGs)","text":"<p>ASGs group Azure virtual machines based on application requirements, simplifying network security:</p> <ul> <li> <p>Simplification: ASGs allow defining rules based on application roles instead of individual IP addresses.</p> </li> <li> <p>Dynamic Membership: ASGs support dynamic membership based on tags or other attributes.</p> </li> <li> <p>Rule Association: Security rules can be associated with ASGs for intuitive and scalable network security management.</p> </li> </ul> <p></p>"},{"location":"azure/#azure-networking-advanced","title":"Azure Networking Advanced","text":""},{"location":"azure/#azure-app-gateway-waf","title":"Azure App Gateway &amp; WAF","text":"<p>Azure Application Gateway is a web traffic load balancer that enables you to manage and route traffic to your web applications. Web Application Firewall (WAF) provides protection against web vulnerabilities.</p> <p>Key features include:</p> <ul> <li> <p>Load Balancing: Distributes incoming traffic across multiple servers to ensure no single server is overwhelmed.</p> </li> <li> <p>SSL Termination: Offloads SSL processing, improving the efficiency of web servers.</p> </li> <li> <p>Web Application Firewall (WAF): Protects web applications from common web vulnerabilities and exploits.</p> </li> </ul>"},{"location":"azure/#azure-load-balancer","title":"Azure Load Balancer","text":"<p>Azure Load Balancer distributes incoming network traffic across multiple servers to ensure no single server is overwhelmed.</p> <p>Key features include:</p> <ul> <li> <p>Load Balancing Algorithms: Supports different algorithms for distributing traffic, such as round-robin and least connections.</p> </li> <li> <p>Availability Sets: Works seamlessly with availability sets to ensure high availability.</p> </li> <li> <p>Inbound and Outbound Traffic: Balances both inbound and outbound traffic.</p> </li> </ul>"},{"location":"azure/#azure-dns","title":"Azure DNS","text":"<p>Azure DNS is a scalable and secure domain hosting service. It provides name resolution using the Microsoft Azure infrastructure.</p> <p>Key features include:</p> <ul> <li> <p>Domain Hosting: Hosts domain names and provides name resolution within Azure.</p> </li> <li> <p>Integration with Azure Services: Easily integrates with other Azure services like App Service and Traffic Manager.</p> </li> <li> <p>Global Availability: Provides low-latency responses globally.</p> </li> </ul>"},{"location":"azure/#azure-firewall","title":"Azure Firewall","text":"<p>Azure Firewall is a managed, cloud-based network security service that protects your Azure Virtual Network resources.</p> <p>Key features include:</p> <ul> <li> <p>Stateful Firewall: Allows or denies traffic based on rules and supports stateful inspection.</p> </li> <li> <p>Application FQDN Filtering: Filters traffic based on fully qualified domain names.</p> </li> <li> <p>Threat Intelligence Integration: Integrates with threat intelligence feeds for enhanced security.</p> </li> </ul>"},{"location":"azure/#virtual-network-peering-and-vnet-gateway","title":"Virtual Network Peering and VNet Gateway","text":"<p>Virtual Network Peering : Virtual Network Peering allows connecting Azure Virtual Networks directly, enabling resources in one VNet to communicate with resources in another.</p> <p>Key features include:</p> <ul> <li> <p>Global VNet Peering: Peering can be established across regions.</p> </li> <li> <p>Transitive Routing: Traffic between peered VNets flows directly, improving performance.</p> </li> </ul> <p>VNet Gateway : VNet Gateway enables secure communication between on-premises networks and Azure Virtual Networks. </p> <p>Key features include:</p> <ul> <li> <p>Site-to-Site VPN: Connects on-premises networks to Azure over an encrypted VPN tunnel.</p> </li> <li> <p>Point-to-Site VPN: Enables secure remote access to Azure resources.</p> </li> </ul>"},{"location":"azure/#vpn-gateway","title":"VPN Gateway","text":"<p>Azure VPN Gateway provides secure, site-to-site connectivity between your on-premises network and Azure.</p> <p>Key features include:</p> <ul> <li> <p>IPsec/IKE VPN Protocols: Ensures secure communication over the Internet.</p> </li> <li> <p>High Availability: Supports active-active and active-passive configurations for high availability.</p> </li> <li> <p>BGP Support: Allows dynamic routing between your on-premises network and Azure.</p> </li> </ul> <p></p>"},{"location":"azure/#expressroute","title":"ExpressRoute","text":"<p>Azure ExpressRoute is a private internet connection between your on-premises data center (or office) and Microsoft Azure.</p> <p>Think of it like a dedicated fiber cable that connects your company\u2019s network directly to Azure \u2014 not through the public internet.</p> <p>ex: It\u2019s like having your own private highway to Azure, instead of driving on the public internet roads \ud83d\ude97\ud83d\udca8  </p> <p>Why Companies Use ExpressRoute</p> <ul> <li>Private and secure \u2192 Data never travels over the public internet.  </li> <li>Faster and more reliable \u2192 Dedicated bandwidth (e.g., 1 Gbps, 10 Gbps).  </li> <li>Consistent performance \u2192 No internet congestion or fluctuations.  </li> <li>Supports hybrid setups \u2192 Connect on-prem servers and Azure VMs like one big network.</li> </ul> Feature Description Private connection Data bypasses the public internet. High speed Choose bandwidth up to 100 Gbps. Low latency Faster and more consistent response times. Secure Enterprise-grade encryption and network isolation. Reliable 99.95% availability SLA from Microsoft. <p>Use Case</p> <p>A bank or hospital might use ExpressRoute to:</p> <ul> <li>Connect securely to Azure SQL Database from their private network.  </li> <li>Transfer large volumes of data (terabytes) to Azure Storage.  </li> <li>Run hybrid applications between on-premises systems and Azure.</li> </ul> <p>Comparison Table with other network resources</p> Connection Type Goes over Internet? Speed / Reliability Security Best For Public Internet (normal) \u2705 Yes Medium Medium Small apps, testing VPN (Site-to-Site) \u2705 Yes (encrypted) Medium High Small/medium hybrid networks ExpressRoute \u274c No Very High Very High Enterprise-grade, mission-critical workloads"},{"location":"azure/#ddos","title":"DDoS","text":"<p>Azure Distributed Denial of Service (DDoS) Protection safeguards your applications and resources in Azure against DDoS attacks. These attacks aim to overwhelm your application by flooding it with fake traffic, causing slowdowns or making it unavailable to legitimate users.</p> <p>Azure provides two tiers of DDoS protection:</p> <p>Basic (Default): Enabled automatically for all Azure services, offering protection at the network level. Standard (Enhanced): Offers advanced mitigation capabilities and is ideal for applications that require higher protection.</p> <p>Key Features of Azure DDoS Protection</p> <ol> <li> <p>Automatic Protection</p> <p>Basic Protection: Built into Azure and available for all customers without extra cost. Standard Protection: Automatically detects and mitigates threats when a resource (like a virtual network) is under attack.</p> </li> <li> <p>Advanced Threat Detection</p> <p>Analyzes traffic patterns to differentiate between legitimate traffic and malicious attacks. Protects against various DDoS attack types, such as: Volumetric Attacks: Flood the network with high traffic (e.g., UDP floods). Protocol Attacks: Exploit weaknesses in network protocols (e.g., SYN floods). Resource Layer Attacks: Target the application directly (e.g., HTTP/S floods).</p> </li> <li> <p>Integration with Azure Resources</p> <p>Works seamlessly with Azure services like Virtual Networks, Load Balancers, and Application Gateways. Protects applications hosted on Azure without requiring changes to the infrastructure.</p> </li> <li> <p>Cost Protection</p> <p>Offers cost protection for scaled resources during an attack. For example, if an attack causes your application to scale up, Azure provides credits for additional resource costs incurred during the attack.</p> </li> <li> <p>Attack Analytics</p> <p>Detailed reports and insights are available via Azure Monitor and Log Analytics. You can analyze attack metrics such as: Attack type Attack duration Traffic volume</p> </li> <li> <p>Alerting and Monitoring</p> <p>Azure Monitor can trigger alerts when an attack is detected or mitigated. Alerts allow you to stay informed and take additional actions if required.  </p> </li> <li> <p>Easy Configuration</p> <p>DDoS Standard protection is enabled by associating it with a Virtual Network (VNet) in your Azure subscription. No complex setup or deployment is required.</p> </li> </ol> <p>How Azure DDoS Protection Works</p> <ul> <li> <p>Traffic Monitoring:</p> <p>Azure continuously monitors traffic for anomalies. It uses machine learning and global threat intelligence to detect unusual patterns.</p> </li> <li> <p>Attack Detection:</p> <p>When abnormal traffic is detected, Azure automatically triggers mitigation.</p> </li> <li> <p>Traffic Scrubbing:</p> <p>Malicious traffic is filtered out while legitimate traffic continues to flow to your application without disruption.</p> </li> </ul> <p>Pricing</p> <p>DDoS Basic: Free and included with all Azure resources. DDoS Standard: Paid, with a flat monthly fee and additional data charges. It is more cost-effective for mission-critical applications requiring higher protection.</p> <p>When to Use DDoS Standard?</p> <ol> <li>Applications that handle sensitive data or critical workloads.  </li> <li>Public-facing web applications, APIs, or online services.  </li> <li>Resources that require compliance with strict security regulations.  </li> </ol> <p>Benefits of Azure DDoS Protection  </p> <ol> <li>Reduced Downtime: Ensures application availability during attacks.  </li> <li>Cost Savings: Provides cost protection during attack-related scaling.  </li> <li>Ease of Use: Seamlessly integrates with Azure resources.  </li> <li>Global Reach: Uses Microsoft's global infrastructure for high availability.</li> </ol>"},{"location":"azure/#storage","title":"Storage","text":""},{"location":"azure/#azure-blob-storage","title":"Azure Blob Storage","text":"<ol> <li> <p>What is it?</p> <p>Azure Blob Storage is a cloud-based object storage solution provided by Microsoft Azure. It is designed to store and manage large amounts of unstructured data, such as documents, images, videos, and other types of binary and text data. Blobs are organized into containers, and each blob is assigned a unique URL for access.</p> </li> <li> <p>When to use it?</p> <p>Use Azure Blob Storage when you need to store and retrieve large amounts of unstructured data. It is suitable for scenarios like serving images or videos to a website, storing backups, and handling data for analytics and big data processing.</p> </li> <li> <p>Example from DevOps Engineer point of view?</p> <p>A DevOps engineer may use Azure Blob Storage to store artifacts and binaries produced during the build process, ensuring a centralized and scalable storage solution. Azure Storage Explorer or Azure CLI can be used to automate the uploading and retrieval of artifacts during deployment pipelines.</p> </li> <li> <p>Equivalent service in AWS:</p> <p>The equivalent service in AWS is Amazon Simple Storage Service (S3). S3 is also an object storage service designed for scalable and secure storage of objects, such as files and data.</p> </li> </ol>"},{"location":"azure/#azure-file-storage","title":"Azure File Storage","text":"<ol> <li> <p>What is it?</p> <p>Azure File Storage is a fully managed file share service in the cloud. It provides the Server Message Block (SMB) protocol for sharing files across applications and VMs in the Azure cloud. Azure File Storage is useful for applications that require shared file access, such as configuration files or data files.</p> </li> <li> <p>When to use it?</p> <p>Use Azure File Storage when you need a shared file system that can be accessed from multiple VMs or applications. It is suitable for scenarios like storing configuration files, sharing data between applications, and serving as a common storage location for applications in a cloud environment.</p> </li> <li> <p>Example from DevOps Engineer point of view?</p> <p>A DevOps engineer may leverage Azure File Storage to store configuration files that are shared among multiple application instances. In a deployment pipeline, scripts or configuration files stored in Azure File Storage can be mounted to VMs or containers during the deployment process.</p> </li> <li> <p>Equivalent service in AWS:</p> <p>The equivalent service in AWS is Amazon Elastic File System (EFS). EFS provides scalable file storage for use with Amazon EC2 instances, supporting the Network File System (NFS) protocol.</p> </li> </ol>"},{"location":"azure/#azure-tables","title":"Azure Tables","text":"<ol> <li> <p>What is it?</p> <p>Azure Tables is a NoSQL data store service provided by Azure. It stores large amounts of semi-structured data and allows for fast and efficient querying using a key-based access model. Data is organized into tables, and each table can store billions of entities.</p> </li> <li> <p>When to use it?</p> <p>Use Azure Tables when you need a highly scalable NoSQL data store for semi-structured data with simple key-based access. It is suitable for scenarios like storing configuration data, user profiles, and other data where a key-value or key-attribute data model is appropriate.</p> </li> <li> <p>Example from DevOps Engineer point of view?</p> <p>A DevOps engineer may use Azure Tables to store configuration settings for applications or services. During the deployment process, scripts can retrieve configuration data from Azure Tables to customize the behavior of deployed applications.</p> </li> <li> <p>Equivalent service in AWS:</p> <p>While AWS does not have a direct equivalent service for Azure Tables, Amazon DynamoDB is a similar NoSQL database service that provides key-value and document storage. DynamoDB can be used for similar use cases.</p> </li> </ol>"},{"location":"azure/#azure-queue-storage","title":"Azure Queue Storage","text":"<ol> <li> <p>What is it?</p> <p>Azure Queue Storage is a message queue service that allows decoupling of components in a distributed application. It provides a reliable way to store and retrieve messages between application components, ensuring asynchronous communication.</p> </li> <li> <p>When to use it?</p> <p>Use Azure Queue Storage when you need to enable communication and coordination between different parts of a distributed application. It is suitable for scenarios like handling background jobs, managing tasks asynchronously, and facilitating communication between loosely coupled components.</p> </li> <li> <p>Example from DevOps Engineer point of view?</p> <p>A DevOps engineer may use Azure Queue Storage to implement a message queue for processing background tasks or managing communication between microservices. During deployment, scripts can enqueue messages to trigger specific actions or coordinate tasks between different components.</p> </li> <li> <p>Equivalent service in AWS:</p> <p>The equivalent service in AWS is Amazon Simple Queue Service (SQS). SQS provides a fully managed message queue service for decoupling components in a distributed system.</p> </li> </ol> <p></p>"},{"location":"azure/#app-service-usage","title":"App Service usage","text":""},{"location":"azure/#azure-managed-identity-vs-service-principal","title":"Azure Managed Identity vs. Service Principal","text":""},{"location":"azure/#overview","title":"Overview","text":"<p>When dealing with authentication and authorization in Azure, two key mechanisms are commonly used for securing access to resources: Managed Identity and Service Principal. Both serve the purpose of enabling secure access to Azure services, but they have different use cases and advantages.</p>"},{"location":"azure/#what-is-a-service-principal","title":"What is a Service Principal?","text":"<p>A Service Principal is an identity used by applications, services, or automation scripts to authenticate and access Azure resources. It is created within Microsoft Entra ID (formerly Azure AD) and functions similarly to a user account, but for applications.</p>"},{"location":"azure/#key-characteristics-of-sp","title":"Key Characteristics of SP","text":"<ul> <li>It has credentials (client ID &amp; secret or certificate).</li> <li>It must be manually managed (rotation of secrets, permissions, etc.).</li> <li>It can be assigned RBAC roles (Role-Based Access Control).</li> <li>It can be used in multi-tenant scenarios (i.e., across different Azure Active Directory tenants).</li> </ul>"},{"location":"azure/#use-cases","title":"Use Cases","text":"Use Case Description CI/CD Pipelines Used in Azure DevOps, GitHub Actions, or Jenkins for deploying applications to Azure resources (e.g., VMs, AKS, App Services). Third-party applications When an external system needs to authenticate with Azure, Service Principals provide authentication. Custom applications Applications running outside Azure (e.g., on-premises or other cloud providers) can use a Service Principal to authenticate. Automation scripts PowerShell or Python scripts that need to access Azure resources can use a Service Principal for authentication."},{"location":"azure/#how-it-works","title":"How It Works","text":"<ol> <li>Register an Application in Microsoft Entra ID.</li> <li>Create a Service Principal for this application.</li> <li>Assign it necessary permissions/RBAC roles.</li> <li>Use the Client ID and Client Secret/Certificate for authentication.</li> </ol>"},{"location":"azure/#challenges","title":"Challenges","text":"<ul> <li>Secret Management: The client secret expires and needs rotation.</li> <li>Security Risk: If credentials are leaked, it can lead to unauthorized access.</li> <li>Manual Setup: Requires additional configuration and maintenance.</li> </ul>"},{"location":"azure/#what-is-managed-identity","title":"What is Managed Identity","text":"<p>Azure Managed Identity is an automated identity service provided by Azure that allows Azure resources to authenticate to services securely without needing explicit credential management.</p>"},{"location":"azure/#key-characteristics-of-manage-identity","title":"Key Characteristics of Manage Identity","text":"<ul> <li>It is tied to an Azure resource (like VM, Function App, or Logic App).</li> <li>It does not require explicit credentials (Azure handles authentication).</li> <li>It can only be used within Azure (not for external systems).</li> <li>It supports RBAC-based access to Azure resources.</li> </ul>"},{"location":"azure/#types-of-managed-identities","title":"Types of Managed Identities","text":"Type Description System-assigned Managed Identity Created and tied to a specific Azure resource (e.g., VM, App Service). Automatically deleted when the resource is deleted. User-assigned Managed Identity Created independently and can be shared across multiple Azure resources. Managed separately from any single resource."},{"location":"azure/#use-cases-of-mi","title":"Use Cases Of MI","text":"Use Case Description Virtual Machines (VMs) Accessing Azure Services VMs need to access Azure Key Vault, Blob Storage, or SQL Database without storing credentials. Function Apps Authenticating to Other Azure Services Azure Functions need to access Event Hubs, Cosmos DB, or Storage Accounts securely. App Services (Web Apps) Interacting with Azure Resources An Azure Web App requires access to Azure Key Vault to fetch secrets or certificates. AKS Pods Needing Secure Access Kubernetes workloads running on Azure Kubernetes Service (AKS) need secure access to Azure services."},{"location":"azure/#how-does-it-works","title":"How Does It Works","text":"<ol> <li>Enable Managed Identity for an Azure resource.</li> <li>Assign necessary RBAC roles (e.g., Reader, Contributor).</li> <li>The resource can request an OAuth token from Azure AD.</li> <li>Use the token to authenticate to other Azure services.</li> </ol>"},{"location":"azure/#advantages","title":"Advantages","text":"<p>\u2705 No Credential Management: Azure automatically handles authentication. \u2705 More Secure: No client secrets or certificates that can be leaked. \u2705 Automatic Identity Lifecycle: System-assigned identity gets deleted with the resource. \u2705 Easy Integration: Works natively with Azure services like Key Vault, Storage, etc.  </p>"},{"location":"azure/#managed-identity-vs-service-principal-key-differences","title":"Managed Identity vs. Service Principal: Key Differences","text":"Feature Managed Identity Service Principal Credential Management No credentials required Requires client ID &amp; secret/certificate Security More secure (no secrets stored) Potential risk (secrets can be exposed) Lifespan Auto-created and managed by Azure Needs manual creation and rotation Scope Works only within Azure Works within and outside Azure RBAC Integration Directly integrates with Azure resources Needs explicit RBAC assignment Multi-tenant Support No Yes Use Case Best for Azure-native applications Best for external apps, automation scripts"},{"location":"azure/#when-to-use-which","title":"When to Use Which?","text":"Scenario Use Managed Identity? Use Service Principal? Azure VM accessing Key Vault \u2705 Yes \u274c No External app (outside Azure) accessing Azure APIs \u274c No \u2705 Yes CI/CD pipeline deploying to Azure \u274c No \u2705 Yes Azure Function calling Azure SQL Database \u2705 Yes \u274c No On-premises script running against Azure API \u274c No \u2705 Yes Multi-tenant applications needing authentication \u274c No \u2705 Yes Long-lived access for automation tools \u274c No \u2705 Yes"},{"location":"azure/#log-analytics-workspace","title":"Log Analytics Workspace","text":"<p>Azure Log Analytics Workspace is like a central storage and analysis hub for logs and data from different Azure services, applications, and resources. It helps you collect, store, and analyze logs in one place, making it easier to monitor and troubleshoot your systems.</p> <p></p>"},{"location":"azure/#key-features","title":"Key Features","text":"<p>\u2705 Collect Logs \u2013 Gathers logs from Azure services, on-premises servers, and applications. \u2705 Store Data \u2013 Keeps logs securely for a defined period. \u2705 Analyze with Queries \u2013 Uses Kusto Query Language (KQL) to search and analyze logs. \u2705 Alerts &amp; Monitoring \u2013 Helps detect issues and trigger alerts for unusual activity. \u2705 Integrates with Azure Services \u2013 Works with Azure Monitor, Security Center, Sentinel, and more.</p>"},{"location":"azure/#azure-monitor","title":"Azure Monitor","text":""},{"location":"azure/#azure-monitor-agent","title":"Azure monitor agent","text":"<p>The purpose of azure monitor agent is to collect logs from guest operating system and inject to log analytics workspace. The OS can be windows/linux, machines can be client/server. They can exist anywhere in azure or on-premise resources.</p> <p></p> <ol> <li>Azure monitoring agent use manage identity for monitoring agent</li> <li>Data collection rule need to create to define what type of data need to collect  </li> <li>Azure monitor used ETL (extract, transform and load) data ingestion pipeline</li> </ol>"},{"location":"azure/#data-collection-rule","title":"Data Collection Rule","text":"<p>Data collection is basically a configuration, which lets azure monitor agents know what type of data must be collected and forward to data ingestion pipeline, which then data ingested to workspace. Data collection rule must be applied to Azure VM or Azure Arc-enabled servers (Azure Arc-enabled servers lets you manage Windows and Linux physical servers and virtual machines hosted outside of Azure, on your corporate network, or other cloud provide).</p> <p>Workflow of DCR</p> <p></p> <p>Once you create DCR and select resources (VM, VMSS), it will install Azure Monitor Agent in that particular resources. Example in below figure.</p> <p></p> <p></p> <p>Whenever the AMA is installed on machine, it create a folder as WindowsAzure &gt; Resources in which it stores log and the machine sends heartbeat which is recorded in HeartBeat table of logs analytics workspace. When a DCR is created to capture performance and windows event logs, the data ingested in perf and Event table of workspace.</p>"},{"location":"azure/#cheat-sheet","title":"Cheat Sheet","text":""},{"location":"azure/#cloud-comparison","title":"Cloud Comparison","text":""},{"location":"azure/#azure-cloud-service-cheat-sheet","title":"Azure Cloud Service Cheat Sheet","text":""},{"location":"cheat_code/","title":"Commands","text":""},{"location":"cheat_code/#python-commands","title":"Python commands","text":"Execute Command Create virtual env <code>python -m venv venv</code> Activate virtual environment Windows   <code>./venv/bin/activate</code>  Linux   <code>source venv/bin/activate</code> Deactivate virtual environment <code>deactivate</code> Install requirements.txt <code>pip install -r requirements.txt</code> Create requirements.txt <code>pip freeze &gt; requirements.txt</code> Upgrade a Package <code>pip install --upgrade &lt;package-name&gt;</code> List Installed Packages <code>pip list</code> Show Package Details <code>pip show &lt;package-name&gt;</code> Install package from git repository <code>pip install git+https://github.com/psf/requests.git</code> Configure Pip Settings <code>pip config set global.index-url https://pypi.org/simple</code>"},{"location":"cheat_code/#docker-commands","title":"Docker commands","text":"Category Command Image Management Build an image from a Dockerfile <code>docker build -t &lt;image_name&gt; .</code> Build an image without cache <code>docker build -t &lt;image_name&gt; . --no-cache</code> List all images <code>docker images</code> Delete an image <code>docker rmi &lt;image_name&gt;</code> Remove all unused images <code>docker image prune</code> Delete all images <code>docker rmi $(docker images -q)</code> Tag an image <code>docker tag &lt;image_id&gt; &lt;new_image_name:tag&gt;</code> Pull an image from Docker Hub <code>docker pull &lt;image_name&gt;</code> Push an image to Docker Hub <code>docker push &lt;image_name&gt;</code> Container Management Run a container <code>docker run &lt;image_name&gt;</code> Run and remove container on stop <code>docker run --rm -p 80:5001 &lt;image_name&gt;</code> Create and name a container <code>docker run --name &lt;container_name&gt; &lt;image_name&gt;</code> Run in the background (detached) <code>docker run -d &lt;image_name&gt;</code> Run with port mapping <code>docker run -p &lt;host_port&gt;:&lt;container_port&gt; &lt;image_name&gt;</code> Run with volume mapping <code>docker run -v &lt;host_path&gt;:&lt;container_path&gt; &lt;image_name&gt;</code> Start a container <code>docker start &lt;container_name&gt;</code> Stop a container <code>docker stop &lt;container_name&gt;</code> Restart a container <code>docker restart &lt;container_name&gt;</code> Kill a running container <code>docker kill &lt;container_name&gt;</code> Remove a stopped container <code>docker rm &lt;container_name&gt;</code> Remove all stopped containers <code>docker container prune</code> Container Inspection List running containers <code>docker ps</code> List all containers <code>docker ps --all</code> Fetch logs <code>docker logs &lt;container_name&gt;</code> Follow logs in real-time <code>docker logs -f &lt;container_name&gt;</code> Inspect container details <code>docker inspect &lt;container_name&gt;</code> Check resource usage stats <code>docker container stats</code> Open a shell inside a container <code>docker exec -it &lt;container_name&gt; sh</code> Execute a command in container <code>docker exec -it &lt;container_name&gt; &lt;command&gt;</code> Network Management List networks <code>docker network ls</code> Create a network <code>docker network create &lt;network_name&gt;</code> Inspect a network <code>docker network inspect &lt;network_name&gt;</code> Connect a container to a network <code>docker network connect &lt;network_name&gt; &lt;container_name&gt;</code> Disconnect a container from a network <code>docker network disconnect &lt;network_name&gt; &lt;container_name&gt;</code> Remove a network <code>docker network rm &lt;network_name&gt;</code> Volumes Management List volumes <code>docker volume ls</code> Create a volume <code>docker volume create &lt;volume_name&gt;</code> Inspect a volume <code>docker volume inspect &lt;volume_name&gt;</code> Remove a volume <code>docker volume rm &lt;volume_name&gt;</code> Remove unused volumes <code>docker volume prune</code> Use a volume in a container <code>docker run -v &lt;volume_name&gt;:&lt;path&gt; &lt;image_name&gt;</code> System Cleanup Remove unused objects <code>docker system prune</code> Clean unused volumes <code>docker volume prune</code> Clean unused images <code>docker image prune</code> Clean unused networks <code>docker network prune</code> Miscellaneous Display Docker version <code>docker --version</code> Display system-wide information <code>docker info</code> Login to Docker Hub <code>docker login</code> Logout from Docker Hub <code>docker logout</code> Save an image to a tar file <code>docker save -o &lt;filename.tar&gt; &lt;image_name&gt;</code> Load an image from a tar file <code>docker load -i &lt;filename.tar&gt;</code> Export a container to a tar file <code>docker export &lt;container_name&gt; &gt; &lt;file.tar&gt;</code> Import a tar file as an image <code>docker import &lt;file.tar&gt; &lt;image_name&gt;</code> Pause a container <code>docker pause &lt;container_name&gt;</code> Unpause a container <code>docker unpause &lt;container_name&gt;</code> Rename a container <code>docker rename &lt;old_name&gt; &lt;new_name&gt;</code> View Docker events <code>docker events</code>"},{"location":"cheat_code/#git-commands","title":"Git commands","text":"Category Command Configuration Set username <code>git config --global user.name \"&lt;User name&gt;\"</code> Set email <code>git config --global user.email \"xyz123@gmail.com\"</code> List configuration <code>git config --list</code> Set default editor <code>git config --global core.editor \"&lt;editor&gt;\"</code> Repository Initialization Initialize repository <code>git init &lt;Repo Name&gt;</code> Clone repository <code>git clone &lt;remote URL&gt;</code> Clone specific branch <code>git clone -b &lt;branch-name&gt; &lt;git-url&gt;</code> Branch Management Create branch <code>git branch &lt;branch name&gt;</code> List branches <code>git branch -a</code> Checkout branch <code>git checkout &lt;branch name&gt;</code> Create and switch to branch <code>git checkout -b &lt;branch name&gt;</code> Delete branch <code>git branch -d &lt;branch name&gt;</code> Force delete branch <code>git branch -D &lt;branch name&gt;</code> Rename branch <code>git branch -m &lt;old branch name&gt; &lt;new branch name&gt;</code> Delete remote branch <code>git push origin --delete &lt;branch name&gt;</code> Staging and Commit Add specific files to stage <code>git add &lt;filename&gt; &lt;filename&gt;</code> Add all changes to stage <code>git add .</code> View status of changes <code>git status</code> Commit staged changes <code>git commit -m \"Commit message\"</code> Add and commit simultaneously <code>git commit -am \"Commit message\"</code> Amend last commit <code>git commit --amend</code> Undo staged changes <code>git reset</code> Unstage specific file <code>git reset &lt;file_name&gt;</code> Unstage file but keep changes <code>git reset HEAD &lt;file_name&gt;</code> Undo commit and keep changes <code>git reset HEAD~1</code> Undo commit and discard changes <code>git reset --hard HEAD~1</code> Pull and Push Pull latest changes <code>git pull</code> Pull with rebase <code>git pull --rebase</code> Push changes <code>git push</code> Push specific branch <code>git push origin &lt;branch name&gt;</code> Set upstream branch <code>git push --set-upstream origin &lt;branch name&gt;</code> Merge and Revert Merge branches <code>git merge &lt;branch name&gt;</code> Revert a commit <code>git revert &lt;commit_hash&gt;</code> Logs and History View commit history <code>git log</code> View history with stats <code>git log --stat</code> View changes line by line <code>git blame &lt;filename&gt;</code> View changes between commits <code>git diff &lt;commit1-sha&gt; &lt;commit2-sha&gt;</code> View changes in branch comparison <code>git diff &lt;branch1&gt; &lt;branch2&gt;</code> Remote Management View remotes <code>git remote -v</code> Add new remote <code>git remote add &lt;name&gt; &lt;url&gt;</code> Change remote URL <code>git remote set-url &lt;name&gt; &lt;new-url&gt;</code> Remove remote <code>git remote remove &lt;name&gt;</code> Fetch changes <code>git fetch</code> Cleaning and Optimization Remove untracked files <code>git clean -fd</code> Remove untracked files and dirs <code>git clean -fdx</code> Prune remote-tracking branches <code>git remote prune origin</code> Optimize repository <code>git gc</code> Rebase and Cherry-Pick Start rebase <code>git rebase &lt;branch&gt;</code> Abort rebase <code>git rebase --abort</code> Continue rebase after conflict <code>git rebase --continue</code> Cherry-pick a commit <code>git cherry-pick &lt;commit-hash&gt;</code> Tags Create a tag <code>git tag &lt;tag-name&gt;</code> Create annotated tag <code>git tag -a &lt;tag-name&gt; -m \"Tag message\"</code> Push tags to remote <code>git push origin &lt;tag-name&gt;</code> List tags <code>git tag</code> Delete local tag <code>git tag -d &lt;tag-name&gt;</code> Delete remote tag <code>git push origin --delete &lt;tag-name&gt;</code> Miscellaneous Stash changes <code>git stash</code> View stashes <code>git stash list</code> Apply last stash <code>git stash apply</code> Delete a stash <code>git stash drop &lt;stash@{index}&gt;</code> Create patch <code>git format-patch &lt;commit-range&gt;</code> Apply patch <code>git apply &lt;patch-file&gt;</code> Merge branch <code>vscode &gt;</code> <code>git clone url</code> <code>git checkout main branch</code> <code>git pull</code> <code>git checkout feature branch</code> <code>git merge main_branch</code>  ## if you want to update feature branch with main  ## you will get merge conflicts in vscode  ## resolve merge conflicts  <code>git commit</code>  ##check branch name and commit to your feature branch  <code>git push feature branch</code> Clone other's code and push to your repo <code>git clone</code> <code>git remote -v</code> <code>git remote set-url origin &lt;add-your-url&gt;</code> <code>git remote -v</code> <code>git push origin</code>"},{"location":"cheat_code/#kubernetes-commands","title":"Kubernetes commands","text":"Category Command Minikube Operations Start Minikube <code>minikube start</code> Host Minikube dashboard <code>minikube dashboard</code> Get Minikube dashboard URL <code>minikube dashboard --url</code> Namespace Commands Create a namespace <code>kubectl create namespace &lt;namespace&gt;</code> List all namespaces <code>kubectl get namespaces</code> Switch to a namespace <code>kubectl config set-context --current --namespace=&lt;namespace&gt;</code> Display pods in a namespace <code>kubectl get pods -n &lt;namespace&gt;</code> Delete all pods in a namespace <code>kubectl delete pods --all -n &lt;namespace&gt;</code> Deployment and Apply Commands Create a deployment <code>kubectl create deployment &lt;deployment-name&gt; --image=&lt;image-name&gt;</code> Example <code>kubectl create deployment nginx-deploy --image=nginx</code> Deploy using a YAML file <code>kubectl apply -f &lt;file.yaml&gt;</code> Edit a deployment in VS Code <code>$env:KUBE_EDITOR=\"code --wait\" &gt; kubectl edit deployment &lt;deployment-name&gt;</code> Delete a deployment <code>kubectl delete deployment &lt;deployment-name&gt;</code> Scale a deployment <code>kubectl scale deployment &lt;deployment-name&gt; --replicas=&lt;number&gt;</code> Restart a deployment <code>kubectl rollout restart deployment &lt;deployment-name&gt;</code> Display Resource Information Display nodes <code>kubectl get nodes</code> Display services <code>kubectl get services</code> Display pods <code>kubectl get pods</code> Display deployments <code>kubectl get deployments</code> Display ReplicaSets <code>kubectl get replicasets</code> Display config maps <code>kubectl get configmaps</code> Display storage classes <code>kubectl get storageclasses</code> Display CRDs <code>kubectl get crds</code> Display secrets <code>kubectl get secrets</code> Display Secret Provider class <code>kubectl get secretproviderclass</code> Display events <code>kubectl get events</code> Debug Commands Describe a pod <code>kubectl describe pod &lt;pod-name&gt;</code> Describe a service <code>kubectl describe service &lt;service-name&gt;</code> Describe a config map <code>kubectl describe configmap &lt;configmap-name&gt;</code> Debug pod logs <code>kubectl logs &lt;pod-name&gt;</code> Debug pod logs with namespace <code>kubectl logs &lt;pod-name&gt; -n &lt;namespace&gt;</code> Debug init container logs <code>kubectl logs &lt;pod-name&gt; -c &lt;init-container-name&gt;</code> Debug pod status with wide view <code>kubectl get pods -o wide</code> Interact with a pod <code>kubectl exec -it &lt;pod-name&gt; -- /bin/bash</code> Copy files from a pod <code>kubectl cp &lt;pod-name&gt;:&lt;path-in-pod&gt; &lt;local-path&gt;</code> Port-forward to access a pod <code>kubectl port-forward &lt;pod-name&gt; &lt;local-port&gt;:&lt;pod-port&gt;</code> Apply and Manage Configurations Apply a configuration file <code>kubectl apply -f &lt;file.yaml&gt;</code> Delete resources from a file <code>kubectl delete -f &lt;file.yaml&gt;</code> Check rollout status <code>kubectl rollout status deployment/&lt;deployment-name&gt;</code> Undo a rollout <code>kubectl rollout undo deployment/&lt;deployment-name&gt;</code> Clean-up Commands Delete all pods <code>kubectl delete pods --all</code> Delete all resources in namespace <code>kubectl delete all --all -n &lt;namespace&gt;</code> Delete a resource <code>kubectl delete &lt;resource-type&gt; &lt;resource-name&gt;</code> Force delete a pod <code>kubectl delete pod &lt;pod-name&gt; --grace-period=0 --force</code> Miscellaneous Commands Check cluster info <code>kubectl cluster-info</code> Check current context <code>kubectl config current-context</code> List all contexts <code>kubectl config get-contexts</code> Set a default namespace for a context <code>kubectl config set-context --current --namespace=&lt;namespace&gt;</code> View resource usage <code>kubectl top nodes</code> Monitor pod resource usage <code>kubectl top pods -n &lt;namespace&gt;</code>"},{"location":"cheat_code/#helm-commands","title":"Helm commands","text":"Execute Command Helm repo add bitnami <code>helm repo add bitnami https://charts.bitnami.com/bitnami</code> Helm repo update <code>helm repo update</code> Helm repo list <code>helm repo list</code> Minikube start <code>minikube start</code> Kubectl create namespace <code>kubectl create ns \"namespace\"</code> Helm install kube-state-metrics <code>helm install kube-state-metrics bitnami/kube-state-metrics -n metrics</code> Helm create chart <code>helm create \"chart-name\"</code> Helm lint <code>helm lint .</code> Helm template with debug <code>helm template --dry-run --debug \"release-name\" .</code> Helm install status <code>helm ls -n \"namespace\"</code> Kubectl get all <code>kubectl get all -n \"namespace\"</code> Helm install with namespace <code>helm install demo-001 . -n development</code> Helm upgrade <code>helm upgrade \"release-name\" .</code> Helm upgrade with namespace <code>helm upgrade demo-001 . -n development</code> Helm history <code>helm history \"release-name\"</code> Helm rollback <code>helm rollback \"release-name\"</code> Helm rollback with revision <code>helm rollback \"release-name\" \"revision-number\"</code> Helm upgrade with specific version <code>helm upgrade kube-state-metrics bitnami/kube-state-metrics --version 0.4.0 -n metrics</code> Helm delete <code>helm delete \"release-name\"</code> Helm install with updated values <code>helm install \"release-name\" --set data.type=\"9090\"</code> Kubectl port-forward for kube-state-metrics <code>kubectl port-forward svc/kube-state-metrics 8080:8080 -n metrics</code> Helm show chart <code>helm show chart bitnami/kube-state-metrics</code> Helm show values <code>helm show values bitnami/kube-state-metrics</code> Helm uninstall <code>helm uninstall \"release-name\" . -n \"namespace\"</code>"},{"location":"cheat_code/#openshift-commands","title":"Openshift commands","text":"Execute Command OpenShift login <code>oc login</code> Create new project <code>oc new-project &lt;project-name&gt;</code> Switch to specific project <code>oc project &lt;project-name&gt;</code> Check the project name <code>oc project</code> Current status of project <code>oc status</code> Display pods <code>oc get pods</code> Describe pod <code>oc describe pod &lt;pod-name&gt;</code> Display logs <code>oc logs &lt;pod-name&gt;</code> Display service of project <code>oc get svc</code> Display specific service of project <code>oc describe svc &lt;service-name&gt;</code> Expose a service to the internet <code>oc expose svc &lt;service-name&gt;</code> Delete a specific service. <code>oc delete svc &lt;service-name&gt;</code> Create new application <code>oc new-app</code> Edit a deployment configuration <code>oc edit dc &lt;deployment-config&gt;</code> Scale a deployment configuration <code>oc scale dc &lt;deployment-config&gt; --replicas=&lt;number&gt;</code> Rollout to latest version of deployment configuration <code>oc rollout latest &lt;deployment-config&gt;</code>"},{"location":"cheat_code/#terraform","title":"Terraform","text":"Category Command General Commands Terraform version <code>terraform --version</code> Terraform help <code>terraform --help</code> Formatting Commands Format files <code>terraform fmt</code> Format files recursively <code>terraform fmt --recursive</code> Show format changes <code>terraform fmt --diff</code> Check formatting <code>terraform fmt --check</code> Initialization Commands Initialize project <code>terraform init</code> Initialize without plugins <code>terraform init -get-plugins=false</code> Initialize without state lock <code>terraform init -lock=false</code> Migrate state to backend <code>terraform init -migrate-state</code> Reconfigure backend <code>terraform init -reconfigure</code> Upgrade modules and providers <code>terraform init -upgrade</code> Module Commands Download modules <code>terraform get</code> Update modules <code>terraform get -update</code> Validation Commands Validate configuration <code>terraform validate</code> Validate in JSON format <code>terraform validate -json</code> Planning Commands Generate execution plan <code>terraform plan</code> Save execution plan to file <code>terraform plan -out=&lt;path&gt;</code> Plan for destruction <code>terraform plan -destroy</code> Plan with variables <code>terraform plan -var=\"key=value\"</code> Plan with variable file <code>terraform plan -var-file=\"filename.tfvars\"</code> Plan with refresh disabled <code>terraform plan -refresh=false</code> Apply Commands Apply changes <code>terraform apply</code> Auto-approve apply <code>terraform apply --auto-approve</code> Apply with variables <code>terraform apply -var=\"key=value\"</code> Apply with variable file <code>terraform apply -var-file=\"filename.tfvars\"</code> Apply a saved plan <code>terraform apply &lt;plan file&gt;</code> Destruction Commands Destroy resources <code>terraform destroy</code> Auto-approve destroy <code>terraform destroy --auto-approve</code> Destroy with variables <code>terraform destroy -var=\"key=value\"</code> Destroy with variable file <code>terraform destroy -var-file=\"filename.tfvars\"</code> State Management Commands Display state <code>terraform show</code> Display state file <code>terraform show &lt;statefile&gt;</code> Refresh state <code>terraform refresh</code> List resources in state <code>terraform state list</code> Push state to backend <code>terraform state push</code> Remove resource from state <code>terraform state rm &lt;resource&gt;</code> Move resource in state <code>terraform state mv &lt;src&gt; &lt;dest&gt;</code> Lock state <code>terraform force-unlock &lt;lock ID&gt;</code> Import resource to state <code>terraform import &lt;address&gt; &lt;resource ID&gt;</code> Show resource state details <code>terraform state show &lt;resource&gt;</code> Resource Lifecycle Commands Force resource recreation <code>terraform taint &lt;resource&gt;</code> Remove taint from resource <code>terraform untaint &lt;resource&gt;</code> Provider Commands List providers <code>terraform providers</code> Workspace Commands List workspaces <code>terraform workspace list</code> Show current workspace <code>terraform workspace show</code> Create a new workspace <code>terraform workspace new &lt;workspace&gt;</code> Delete a workspace <code>terraform workspace delete &lt;workspace&gt;</code> Select a workspace <code>terraform workspace select &lt;workspace&gt;</code> Debugging Commands Debug a command <code>terraform -debug &lt;command&gt;</code> Show logs <code>TF_LOG=&lt;level&gt; terraform &lt;command&gt;</code> Enable detailed logs <code>TF_LOG=TRACE terraform &lt;command&gt;</code> Miscellaneous Commands Create graph of resources <code>terraform graph</code> Generate graph to file <code>terraform graph \\ dot -Tpng &gt; graph.png</code> Output resource values <code>terraform output</code> Output specific value <code>terraform output &lt;output name&gt;</code> Suppress colored output <code>terraform &lt;command&gt; -no-color</code> Show available providers <code>terraform providers schema</code>"},{"location":"cka/","title":"CKA","text":""},{"location":"cka/#understand-k8-architecture","title":"Understand K8 architecture","text":""},{"location":"cka/#vanilla-kubernetes","title":"Vanilla Kubernetes","text":"<p>\"Vanilla Kubernetes\" refers to the standard, unmodified, open-source version of Kubernetes as released by the Kubernetes community, without any vendor-specific customizations, extensions, or added features. It is the pure form of Kubernetes as maintained by the Cloud Native Computing Foundation (CNCF).</p> <p>Using Vanilla Kubernetes ensures you\u2019re working with the core features and APIs defined by the Kubernetes project, providing a consistent foundation for container orchestration.</p>"},{"location":"cka/#core-components-of-vanilla-kubernetes","title":"Core Components of Vanilla Kubernetes","text":"<p>Vanilla Kubernetes consists of several components categorized as either control plane components or node components:</p> <p>Control Plane Components : These components are responsible for managing the cluster and ensuring the desired state of the system.</p> <ol> <li> <p>API Server (kube-apiserver): Acts as the central management point for Kubernetes. It exposes the Kubernetes API, processes client requests, and communicates with other components.</p> <p>Key feature: Handles REST API calls.</p> </li> <li> <p>Controller Manager (kube-controller-manager): Runs controllers that regulate the state of the cluster, such as the Node Controller (monitoring nodes), ReplicaSet Controller (managing replicas), and others.</p> <p>Key feature: Ensures the cluster meets the desired state.</p> </li> <li> <p>Scheduler (kube-scheduler):Assigns workloads (Pods) to nodes in the cluster based on resource availability and policies.</p> <p>Key feature: Ensures optimal placement of Pods.</p> </li> <li> <p>Etcd: A distributed key-value store that serves as Kubernetes' database, storing all cluster data such as configurations, secrets, and the desired state.</p> <p>Key feature: Provides high availability and consistency.</p> </li> </ol> <p>Node Components : Node components manage the execution of workloads on individual machines.</p> <ol> <li> <p>Kubelet: A daemon running on each node, ensuring that containers are running as defined by the Pod specifications.</p> <p>Key feature: Manages container lifecycle on a node.</p> </li> <li> <p>Kube-Proxy: A network proxy that manages network rules and enables service discovery and communication between Pods.</p> <p>Key feature: Provides networking for Pods and services.</p> </li> <li> <p>Container Runtime: Executes containers and manages container lifecycle. Examples include Docker, containerd, and CRI-O.</p> <p>Key feature: Ensures Pods' containers run smoothly.</p> </li> </ol> <p>Add-Ons (Optional Components) : These are additional features often deployed in a Kubernetes cluster.</p> <ol> <li>DNS (CoreDNS): Provides service discovery and DNS resolution for services within the cluster.</li> <li>Ingress Controllers: Manages external HTTP and HTTPS access to cluster services.</li> <li>Metrics Server: Collects resource metrics for autoscaling and monitoring.</li> </ol>"},{"location":"cka/#ecosystem-of-vanilla-kubernetes","title":"Ecosystem of Vanilla Kubernetes","text":"<p>Kubernetes has a rich ecosystem of tools and technologies that enhance its functionality and usability. These tools are often open-source and community-driven, but they may also include vendor-specific solutions:</p> <ol> <li> <p>Networking</p> <p>CNI (Container Network Interface): Plugins like Calico, Flannel, and Weave Net enable networking in Kubernetes.</p> <p>Service Mesh: Istio and Linkerd provide advanced networking features like traffic routing and observability.</p> </li> <li> <p>Storage</p> <p>CSI (Container Storage Interface): Allows Kubernetes to interact with storage systems. Examples: Rook, Ceph, and AWS EBS.</p> </li> <li> <p>Observability</p> <p>Monitoring: Prometheus, Grafana, and Datadog help monitor cluster health and workloads.</p> <p>Logging: Tools like Elasticsearch, Fluentd, and Kibana (EFK stack) collect and analyze logs.</p> </li> <li> <p>Deployment and Scaling</p> <p>Helm: A package manager for Kubernetes that simplifies application deployment.</p> <p>Kustomize: Provides declarative configuration management for Kubernetes resources.</p> </li> <li> <p>Security</p> <p>RBAC (Role-Based Access Control): Manages permissions within the cluster.</p> <p>Tools: Falco, Aqua Security, and OPA (Open Policy Agent) for runtime security and policy enforcement.</p> </li> <li> <p>CI/CD Integration</p> <p>ArgoCD: A declarative GitOps continuous delivery tool for Kubernetes.</p> <p>Jenkins X: Kubernetes-native CI/CD solution.</p> </li> <li> <p>Backup and Disaster Recovery</p> <p>Tools like Velero for backing up and restoring Kubernetes resources.</p> </li> </ol> <p>Advantages of Using Vanilla Kubernetes</p> <ul> <li>Portability: No vendor lock-in; deployable across any environment supporting Kubernetes.</li> <li>Flexibility: Offers complete control over how you configure and extend Kubernetes.</li> <li>Community Support: Benefits from a vibrant open-source community and frequent updates.</li> </ul> <p>Challenges with Vanilla Kubernetes</p> <ul> <li>Complexity: Requires expertise to set up, maintain, and secure.</li> <li>Lack of Pre-Configured Features: Unlike managed Kubernetes services (e.g., GKE, AKS, EKS)</li> <li>Vanilla Kubernetes doesn\u2019t include out-of-the-box solutions for scaling, logging, or monitoring.</li> </ul>"},{"location":"cka/#understand-cluster-node-requirements-install-using-kubeadm","title":"Understand cluster node requirements install using kubeadm","text":"<p>Cluster Node Requirements for Installing Kubernetes Using kubeadm.</p> <p>Before installing a Kubernetes cluster using kubeadm, it\u2019s essential to ensure that the system requirements and prerequisites are met for the control plane and worker nodes. Below is a comprehensive guide to help you prepare.</p> <ol> <li> <p>Hardware Requirements</p> <p>Control Plane Node (Master Node)</p> <ul> <li>CPU: 2 cores or more.</li> <li>RAM: 2 GB or more (4 GB recommended for production).</li> <li>Disk Space: 10 GB or more free space.</li> <li>Network: Stable network connection (public or private).</li> </ul> <p>Worker Nodes</p> <ul> <li>CPU: 1 core or more.</li> <li>RAM: 1 GB or more (2 GB recommended for production).</li> <li>Disk Space: 10 GB or more free space.</li> <li>Network: Network connectivity to control plane and other nodes.</li> </ul> </li> <li> <p>Operating System Requirements</p> <p>Use one of the following operating systems:</p> <ul> <li>Ubuntu 20.04, 22.04</li> <li>CentOS 7 or 8</li> <li>Red Hat Enterprise Linux (RHEL) 7 or 8</li> <li>Debian 10 or 11</li> <li>Ensure the OS is up-to-date with the latest patches and security updates.</li> </ul> </li> <li> <p>Software Requirements</p> <p>Container Runtime - Kubernetes supports multiple container runtimes. Ensure one of these is installed and configured:</p> <ul> <li>containerd (preferred and recommended)</li> <li>Docker (requires CRI plugin, deprecated after Kubernetes 1.24)</li> <li>CRI-O</li> <li>Packages</li> </ul> </li> <li> <p>Installation Instructions</p> </li> <li> <p>kubectl: CLI tool to manage the cluster.</p> </li> <li>kubeadm: Used to bootstrap the cluster.</li> <li> <p>kubelet: Responsible for running containers on nodes.</p> <p>Disable Swap Kubernetes requires swap to be disabled to function correctly.</p> <pre><code>sudo swapoff -a\n</code></pre> <p>To make this change permanent, comment out any swap entries in /etc/fstab.</p> <p>Enable Bridge-NF and IP Forwarding Kubernetes networking requires specific kernel parameters. Configure them as follows:</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\n\nsudo sysctl --system\n</code></pre> <p>Firewall Configuration</p> <p>Allow traffic on ports used by Kubernetes components:</p> <pre><code>sudo firewall-cmd --permanent --add-port=6443/tcp   # API server\nsudo firewall-cmd --permanent --add-port=2379-2380/tcp # etcd server\nsudo firewall-cmd --permanent --add-port=10250/tcp # kubelet API\nsudo firewall-cmd --reload\n</code></pre> </li> <li> <p>Network Requirements</p> <p>All nodes must be able to communicate with each other.</p> <p>Required ports: - Control Plane: 6443, 2379-2380, 10250, 10251, 10252 - Worker Nodes: 10250, 30000-32767 (NodePort range)</p> </li> <li> <p>Cluster Configuration</p> <p>Node Hostnames, Each node should have a unique hostname. Update the hostname if necessary:</p> <pre><code>sudo hostnamectl set-hostname &lt;node-name&gt;\n</code></pre> <p>Time Synchronization Ensure time synchronization is set up using chrony or ntp. DNS and Host File Configuration Update the /etc/hosts file on all nodes with the control plane node's IP and hostname:</p> <pre><code>&lt;control-plane-node-ip&gt; &lt;control-plane-node-hostname&gt;\n</code></pre> </li> <li> <p>Installing Kubernetes with kubeadm</p> </li> <li> <p>Install Prerequisites         Install required packages:</p> <pre><code>```bash\nsudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl\ncurl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\necho \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\nsudo apt-mark hold kubelet kubeadm kubectl\n```\n</code></pre> </li> <li> <p>Initialize the Control Plane         On the control plane node:</p> <pre><code>```bash\nsudo kubeadm init --pod-network-cidr=10.244.0.0/16\n```\n</code></pre> </li> <li> <p>Set Up kubectl for the Admin User</p> <pre><code>```bash\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n</code></pre> </li> <li> <p>Install a Pod Network Add-On         Install a network plugin, such as Flannel:</p> <pre><code>```bash\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n```\n</code></pre> </li> <li> <p>Join Worker Nodes         On each worker node, join the cluster using the command provided by kubeadm init:</p> <pre><code>```bash\nsudo kubeadm join &lt;control-plane-ip&gt;:6443 --token &lt;token&gt; --discovery-token-ca-cert-hash &lt;hash&gt;\n```\n\nIf you don\u2019t have the join command, you can regenerate it on the control plane node:\n\n```bash\nkubeadm token create --print-join-command\n```\n</code></pre> </li> <li> <p>Verification</p> <pre><code>Check nodes in the cluster:\n\n```bash\nkubectl get nodes\n```\n</code></pre> </li> <li> <p>Check pods in all namespaces:</p> <pre><code>```bash\nkubectl get pods --all-namespaces\n```\n</code></pre> </li> </ol>"},{"location":"cka/#understanding-node-network-requirements-in-kubernetes","title":"Understanding Node Network Requirements in Kubernetes","text":"<p>Networking in Kubernetes is crucial for enabling communication between cluster nodes, workloads (Pods), and external users or systems. Properly configuring the network requirements ensures smooth operation of your Kubernetes cluster.</p> <p>Below is an overview of the key node networking requirements and concepts.</p> <ol> <li> <p>Key Kubernetes Networking Concepts</p> </li> <li> <p>Pod-to-Pod Communication    Every Pod in a Kubernetes cluster gets its own IP address, and Pods can communicate with each other directly using these IPs.    Requirement: All nodes must be able to reach every Pod on any node without NAT (Network Address Translation).</p> </li> <li>Pod-to-Service Communication    Kubernetes services provide a stable IP and DNS name for accessing Pods, even if Pod IPs change.    Requirement: Nodes must allow traffic between Pods and the ClusterIP addresses of services.</li> <li>Node-to-Node Communication    Nodes communicate to ensure the control plane and worker nodes operate cohesively.    Requirement: Nodes need to communicate over specific ports for API server, kubelet, kube-proxy, etc.</li> <li> <p>External Communication    Kubernetes must support traffic from external users or systems to access cluster workloads.    Requirement: Nodes must allow traffic for exposing services via NodePort, LoadBalancer, or Ingress.</p> </li> <li> <p>Network Requirements by Node Role</p> <p>Control Plane Node Requirements : Communication between the control plane and worker nodes is critical.</p> <p>Required ports</p> <pre><code>6443/tcp: Kubernetes API server.\n2379-2380/tcp: etcd server (key-value store).\n10250/tcp: kubelet API for managing workloads.\n10251/tcp: kube-scheduler (internal cluster communications).\n10252/tcp: kube-controller-manager (internal cluster communications).\n</code></pre> <p>Worker Node Requirements : Worker nodes communicate with the control plane and other nodes to manage workloads.</p> <p>Required ports:</p> <pre><code>10250/tcp: kubelet API.\n30000-32767/tcp: NodePort range for services.\nAny additional ports based on the container runtime (e.g., CRI or Docker).\n</code></pre> </li> <li> <p>Cluster-Wide Network Requirements</p> <ol> <li> <p>Pod Network CIDR     Kubernetes clusters use a Pod network CIDR range to assign IPs to Pods. </p> <p>For example:</p> <pre><code>Flannel default: 10.244.0.0/16\nCalico default: 192.168.0.0/16\n</code></pre> <p>Requirement: Ensure the Pod CIDR does not overlap with existing network ranges in your environment.</p> </li> <li> <p>Service CIDR</p> <p>Kubernetes services are assigned virtual IPs from a separate CIDR range. </p> <p>For example: <code>Default: 10.96.0.0/12</code> Requirement: Ensure the service CIDR does not conflict with existing network ranges.</p> </li> <li> <p>DNS</p> <p>CoreDNS, or another DNS provider, resolves service names to ClusterIPs. Requirement: Nodes must have access to the DNS service within the cluster.</p> </li> <li> <p>Required Ports</p> <p>Control Plane Node Ports - Port Range Protocol Purpose</p> <pre><code>6443 TCP Kubernetes API server\n2379-2380 TCP etcd server communication\n10250 TCP Kubelet API\n10251 TCP kube-scheduler communication\n10252 TCP kube-controller-manager communication\n</code></pre> <p>Worker Node Ports - Port Range Protocol Purpose 10250 TCP Kubelet API 30000-32767 TCP NodePort services Pod CIDR Range TCP/UDP Pod-to-Pod communication</p> </li> </ol> </li> <li> <p>Configuring Network Plugins</p> <p>Kubernetes itself does not handle networking, it relies on network plugins conforming to the Container Network Interface (CNI) specification. </p> <p>Some popular CNI plugins include:</p> <ul> <li>Flannel: Simple overlay network.</li> <li>Calico: Provides advanced networking and network policies.</li> <li>Weave Net: Offers automatic service discovery and encryption.</li> <li>Cilium: Layer 7 observability and security.</li> </ul> <p>Steps to Configure Networking Using a CNI Choose a CNI plugin compatible with your cluster. Apply the CNI plugin\u2019s YAML manifest after initializing the cluster.</p> </li> </ol> <p>Example for Flannel:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n</code></pre> <p>Verify that Pods receive IPs and can communicate across nodes.</p> <ol> <li>Additional Considerations</li> </ol> <p>Firewall Rules : Ensure firewall rules allow traffic on the required ports for Kubernetes components and networking. Allow traffic between the Pod CIDR range, Service CIDR range, and nodes.</p> <p>Cloud-Specific Networking : If running Kubernetes in a cloud environment, configure networking based on the cloud provider\u2019s requirements. Most managed Kubernetes services (like AWS EKS, Azure AKS, or GKE) have native integrations.</p> <p>Network Policies : Network policies define rules for traffic flow between Pods and services. Use tools like Calico or Cilium to enforce these policies.</p> <ol> <li>Verifying Network Connectivity</li> </ol> <p>Ping Test Between Pods : Deploy Pods on different nodes and verify they can communicate:</p> <pre><code>kubectl exec -it pod-a -- ping &lt;pod-b-ip&gt;\n</code></pre> <p>Create a test service and ensure it is reachable via its ClusterIP or NodePort.</p> <p>DNS Test : Check if DNS resolution is working:</p> <pre><code>kubectl exec -it pod-a -- nslookup &lt;service-name&gt;\n</code></pre>"},{"location":"cka/#what-different-phases-k8-execute-while-running-kubeadm-init","title":"What different phases k8 execute while running kubeadm init","text":"<p>When you run kubeadm init, it executes a series of phases to set up a Kubernetes control plane. Each phase performs a specific task to bootstrap the cluster.</p> <p>Phases of kubeadm init</p> <ol> <li>Preflight Checks     Validates system configuration (e.g., swap disabled, required ports open).     Ensures prerequisites are met.</li> <li>kubelet Configuration     Writes the kubelet configuration file (/var/lib/kubelet/config.yaml).     Enables and starts the kubelet service.</li> <li>Certificates     Generates and signs certificates for cluster components (e.g., API server, etcd).     Stores them in /etc/kubernetes/pki.</li> <li>KubeConfig Files     Creates kubeconfig files for communication with the cluster (admin.conf, kubelet.conf, etc.).     Stores them in /etc/kubernetes/.</li> <li>etcd Initialization     Bootstraps the etcd key-value store for cluster state management.</li> <li>Kubernetes API Server     Starts the API server and configures its certificates and kubeconfig.</li> <li>Control Plane Components     Deploys static Pods for: kube-apiserver, kube-controller-manager, kube-scheduler</li> <li>Bootstrap Token     Generates a token to allow worker nodes to join the cluster.</li> <li>Cluster Network     Configures basic networking (e.g., setting Pod CIDR).     Prepares for the installation of a network plugin (like Flannel or Calico).</li> <li>Mark the Node     Marks the control plane node with a specific taint to prevent scheduling Pods on it.</li> </ol>"},{"location":"cka/#installation-cluster","title":"Installation cluster","text":"<p>Refer the github, this contains script to install cluster.</p> <p>install cri : <code>sudo /cka/setup-container.sh</code></p> <p>install kubetools : <code>sudo /cka/setup-kubetools.sh</code>, this includes (kubeadm, kubelet, kubectl)</p> <p>install cluster : <code>sudo kubeadm init</code>, this need to run only on master node</p> <p>setup client in master node:</p> <pre><code>mkdir ~/.kube\nsudo cp -i /etc/kubernetes/admin.conf ~/.kube/config #to provide admin access to kubernetes cluster\nsudo chown $(id -u):$(id -g) .kube/config\n</code></pre> <p>validate</p> <pre><code>kubectl get all # this should display type: clusterIP\n</code></pre> <p>Install network add-on</p> <pre><code>kubectl create -f https://raw.githubusercontent.com/projectcalico//calico/v3.25.0/manifests/tigera-operator.yaml\n\nkubectl create -f https://raw.githubusercontent.com/projectcalico//calico/v3.25.0/manifests/custom-resources.yaml\n\nwatch kubectl get pods -n calico-system\n\n# BAckup options: \nkubectl apply -f cka/calico.yaml\n\n#validate\nkubectl get pods -n kube-system #this will display calico status\n\n# Join other nodes, execute this in worker nodes\nsudo kubeadm join &lt;join-token&gt;\n\n# validate worker nodes, execute this in master node\nkubectl get nodes #this will display worker nodes\n</code></pre>"},{"location":"cka/#k8-applications","title":"K8 Applications","text":"<p>Running Applications</p> <ol> <li>deploying k8 applications</li> <li>using deployment</li> <li>running agents with daemonsets</li> <li>understanding0 stateful and stateless applications</li> <li>the case for running individual pods</li> <li>managing pod initialization</li> <li>scaling applications</li> <li>sidecar containers for application logging</li> <li>Managing storage</li> <li>understanding k8 storage options</li> <li>accessing storage through pod volumes</li> <li>configuring pv storage</li> <li>configuring pvc</li> <li>configuring pv and pvc with pod storage</li> <li>using storage class</li> <li>storage provisioner</li> <li>using configmap and secrets</li> <li>managing application access</li> </ol>"},{"location":"cka/#doubts","title":"doubts","text":""},{"location":"database/","title":"About Database","text":""},{"location":"database/#crud-operations-in-database-mongodb-example","title":"CRUD Operations in Database (MongoDB Example)","text":"<p>CRUD stands for:</p> <ul> <li>C \u2192 Create  </li> <li>R \u2192 Read  </li> <li>U \u2192 Update  </li> <li>D \u2192 Delete  </li> </ul>"},{"location":"database/#mongodb","title":"MongoDB","text":""},{"location":"database/#what-is-mongodb","title":"What is MongoDB?","text":"<p>MongoDB is a NoSQL, document-oriented database that stores data in flexible, JSON-like documents (called BSON \u2013 Binary JSON).</p> <p>Instead of storing data in rows and columns like traditional relational databases, MongoDB stores data as:</p> <pre><code>{\n  \"name\": \"Ramakrishna\",\n  \"phone\": \"9876543210\",\n  \"city\": \"Hyderabad\"\n}\n</code></pre> <p>These documents are grouped into collections (similar to tables in SQL databases).</p>"},{"location":"database/#how-is-mongodb-different-from-other-databases","title":"How is MongoDB Different from Other Databases?","text":"<p>The biggest comparison is usually between MongoDB and relational databases like:</p> <ul> <li>MySQL  </li> <li>PostgreSQL  </li> <li>Oracle Database  </li> </ul> <p>Let\u2019s compare:</p> <p>1\ufe0f\u20e3 Data Structure</p> Feature MongoDB Relational DB (MySQL, PostgreSQL) Data Model Document (JSON-like) Tables (rows &amp; columns) Schema Flexible (dynamic) Fixed schema Joins Limited / different approach Strong JOIN support <p>2\ufe0f\u20e3 Schema Flexibility</p> <p>MongoDB</p> <ul> <li>No fixed schema required  </li> <li>Each document can have different fields  </li> <li>Easy to modify structure  </li> </ul> <p>Relational Database</p> <ul> <li>Must define schema before inserting data  </li> <li>Altering table structure can be complex  </li> </ul> <p>Example:</p> <p>MongoDB allows:</p> <pre><code>{ \"name\": \"Ram\", \"phone\": \"1234\" }\n{ \"name\": \"Krishna\", \"phone\": \"5678\", \"city\": \"Delhi\" }\n</code></pre> <p>In MySQL, both rows must match the same table structure.</p> <p>3\ufe0f\u20e3 Scalability</p> <p>MongoDB</p> <ul> <li>Designed for horizontal scaling  </li> <li>Supports sharding (splitting data across servers)  </li> <li>Good for big data &amp; distributed systems  </li> </ul> <p>Traditional SQL Database</p> <ul> <li>Mostly vertical scaling (increase CPU/RAM)  </li> <li>Horizontal scaling is more complex  </li> </ul> <p>4\ufe0f\u20e3 Performance</p> <p>MongoDB performs very well for:</p> <ul> <li>Large volume data  </li> <li>High write operations  </li> <li>Real-time analytics  </li> <li>Rapid development  </li> </ul> <p>Relational databases are better for:</p> <ul> <li>Complex joins  </li> <li>Strong transactional systems (banking, finance)  </li> </ul>"},{"location":"database/#why-choose-mongodb","title":"Why Choose MongoDB?","text":"<p>Choose MongoDB when:</p> <p>\u2705 1. You need flexible schema Startup projects where requirements change often.</p> <p>\u2705 2. You are building web/mobile applications MongoDB works very well with:</p> <ul> <li>Node.js  </li> <li>Flask  </li> <li>Django  </li> </ul> <p>\u2705 3. You handle JSON data</p> <p>APIs return JSON \u2192 MongoDB stores JSON-like \u2192 very natural fit.</p> <p>\u2705 4. Fast prototyping</p> <p>No need to design complex schemas initially.</p> <p>\u2705 5. Big Data / Microservices</p> <p>Used in scalable systems and distributed architectures.</p>"},{"location":"database/#when-not-to-choose-mongodb","title":"When NOT to Choose MongoDB?","text":"<p>Avoid MongoDB if:</p> <p>\u274c You need heavy JOIN operations \u274c Strong ACID relational constraints are critical \u274c Complex reporting queries across many related tables  </p>"},{"location":"database/#simple-summary","title":"Simple Summary","text":"If you want Choose Structured banking system MySQL / PostgreSQL Flexible web app MongoDB Complex relational queries PostgreSQL Rapid development MongoDB"},{"location":"database/#real-world-companies-using-mongodb","title":"Real-World Companies Using MongoDB","text":"<p>Companies using MongoDB include:</p> <ul> <li>Netflix  </li> <li>Uber  </li> <li>Adobe  </li> </ul>"},{"location":"database/#final-explanation-in-simple-words","title":"Final Explanation (In Simple Words)","text":"<p>MongoDB is:</p> <ul> <li>Flexible  </li> <li>Scalable  </li> <li>JSON-friendly  </li> <li>Easy to use for modern applications  </li> </ul> <p>Traditional SQL databases are:</p> <ul> <li>Structured  </li> <li>Strict  </li> <li>Strong for financial and relational systems  </li> </ul>"},{"location":"database/#mongodb-architecture-and-components","title":"MongoDB Architecture and Components","text":"<p>What is mongod?</p> <p><code>mongod</code> is the MongoDB database server process.</p> <p>It is responsible for:</p> <ul> <li>Accepting client connections</li> <li>Managing databases and collections</li> <li>Handling read/write operations</li> <li>Performing indexing</li> <li>Managing storage and memory</li> <li>Replication and sharding coordination</li> </ul> <p>When you start MongoDB using:</p> <pre><code>mongod\n</code></pre> <p>You are starting the database server.</p>"},{"location":"database/#core-mongodb-components","title":"Core MongoDB Components","text":"<p>1\ufe0f\u20e3 mongod</p> <ul> <li>Main database server</li> <li>Handles data storage and queries</li> <li>Runs on default port <code>27017</code></li> </ul> <p>2\ufe0f\u20e3 mongosh (MongoDB Shell)</p> <ul> <li>Interactive command-line shell</li> <li>Used to interact with MongoDB</li> <li>Run queries manually</li> </ul> <p>Example:</p> <pre><code>mongosh\n</code></pre> <p>3\ufe0f\u20e3 MongoDB Drivers Drivers allow applications to communicate with MongoDB.</p> <p>Examples:</p> <ul> <li>Python Driver \u2192 <code>pymongo</code></li> <li>Node.js Driver \u2192 <code>mongodb</code></li> <li>Java Driver \u2192 MongoDB Java driver</li> </ul> <p>Applications use drivers to send queries to <code>mongod</code>.</p> <p>4\ufe0f\u20e3 MongoDB Compass</p> <ul> <li>GUI tool</li> <li>Used to visualize databases</li> <li>Useful for beginners</li> </ul> <p>5\ufe0f\u20e3 WiredTiger Storage Engine</p> <ul> <li>Default storage engine</li> <li>Manages how data is stored on disk</li> <li>Provides compression and concurrency control</li> </ul> <p>6\ufe0f\u20e3 Replica Set</p> <ul> <li>Provides High Availability</li> <li>Multiple <code>mongod</code> instances</li> <li>One Primary</li> <li>Multiple Secondary nodes</li> </ul> <p>7\ufe0f\u20e3 Sharding</p> <ul> <li>Horizontal scaling method</li> <li>Splits large data across multiple servers</li> <li>Used for Big Data applications</li> </ul>"},{"location":"database/#mongodb-architecture-diagram-simple","title":"MongoDB Architecture Diagram (Simple)","text":"<pre><code>                    +--------------------+\n                    |   Client App       |\n                    | (Flask / Node.js)  |\n                    +----------+---------+\n                               |\n                               |\n                        MongoDB Driver\n                               |\n                               v\n                    +--------------------+\n                    |       mongod      |\n                    |  (Primary Server) |\n                    +----------+---------+\n                               |\n                 -----------------------------\n                 |                           |\n                 v                           v\n        +----------------+          +----------------+\n        |  Secondary 1   |          |  Secondary 2   |\n        |   (Replica)    |          |   (Replica)    |\n        +----------------+          +----------------+\n\nData Stored on Disk using WiredTiger Storage Engine\n</code></pre>"},{"location":"database/#mongodb-workflow-how-it-works","title":"MongoDB Workflow (How It Works)","text":"<p>Step 1: Application Sends Request</p> <p>Example: User submits a form in Flask.</p> <p>Application sends request using driver:</p> <pre><code>collection.insert_one({\"name\": \"Ram\", \"city\": \"Hyderabad\"})\n</code></pre> <p>Step 2: Driver Connects to mongod</p> <ul> <li>Driver connects to MongoDB server at:</li> </ul> <pre><code>mongodb://localhost:27017\n</code></pre> <ul> <li>Request is sent to <code>mongod</code></li> </ul> <p>Step 3: mongod Processes Request</p> <ul> <li>Validates request</li> <li>Checks indexes</li> <li>Applies write operation</li> <li>Updates memory cache</li> </ul> <p>Step 4: Data Stored in Storage Engine</p> <ul> <li>WiredTiger stores data on disk</li> <li>Data is stored in BSON format</li> <li>Compression applied</li> </ul> <p>Step 5: Replication (If Replica Set Enabled)</p> <ul> <li>Primary writes data</li> <li>Secondary nodes replicate data</li> <li>Ensures high availability</li> </ul> <p>Step 6: Response Sent Back</p> <ul> <li>mongod sends acknowledgment</li> <li>Driver returns result to application</li> <li>Application shows success message</li> </ul>"},{"location":"database/#internal-working-simplified","title":"Internal Working Simplified","text":"<pre><code>Client Request\n     \u2193\nMongoDB Driver\n     \u2193\nmongod Server\n     \u2193\nQuery Planner\n     \u2193\nStorage Engine (WiredTiger)\n     \u2193\nDisk\n     \u2193\nResponse Back to Client\n</code></pre>"},{"location":"database/#how-mongodb-stores-data","title":"How MongoDB Stores Data","text":"<p>MongoDB stores:</p> <ul> <li>Databases</li> <li>Collections<ul> <li>Documents (JSON-like BSON)</li> </ul> </li> </ul> <p>Example Structure:</p> <pre><code>Database: userdb\n   |\n   +-- Collection: users\n           |\n           +-- { name: \"Ram\", phone: \"1234\" }\n           +-- { name: \"Krishna\", phone: \"5678\" }\n</code></pre>"},{"location":"database/#advanced-architecture-sharded-cluster","title":"Advanced Architecture (Sharded Cluster)","text":"<pre><code>                +------------------+\n                |     Client       |\n                +---------+--------+\n                          |\n                     mongos Router\n                          |\n        ---------------------------------------\n        |                 |                   |\n   +-----------+     +-----------+      +-----------+\n   | Shard 1   |     | Shard 2   |      | Shard 3   |\n   | (mongod)  |     | (mongod)  |      | (mongod)  |\n   +-----------+     +-----------+      +-----------+\n</code></pre>"},{"location":"database/#components-in-sharded-cluster","title":"Components in Sharded Cluster","text":"<ul> <li>mongos \u2192 Query router</li> <li>Config Servers \u2192 Store metadata</li> <li>Shards \u2192 Store actual data</li> </ul>"},{"location":"database/#high-level-production-architecture","title":"High-Level Production Architecture","text":"<pre><code>                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502      End Users       \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502     Load Balancer    \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                 \u2502                   \u2502                   \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 App Server 1    \u2502 \u2502 App Server 2    \u2502 \u2502 App Server 3    \u2502\n        \u2502 (Flask/Node)    \u2502 \u2502 (Flask/Node)    \u2502 \u2502 (Flask/Node)    \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502                   \u2502                   \u2502\n                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                             \u2502     mongos     \u2502\n                             \u2502  Query Router  \u2502\n                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502                         \u2502                         \u2502\n     \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502  Shard 1  \u2502            \u2502  Shard 2   \u2502            \u2502  Shard 3  \u2502\n     \u2502 (Replica) \u2502            \u2502 (Replica)  \u2502            \u2502 (Replica) \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502                         \u2502                         \u2502\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Primary       \u2502         \u2502 Primary       \u2502         \u2502 Primary       \u2502\n   \u2502 Secondary     \u2502         \u2502 Secondary     \u2502         \u2502 Secondary     \u2502\n   \u2502 Secondary     \u2502         \u2502 Secondary     \u2502         \u2502 Secondary     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                 \u2502        Config Servers         \u2502\n                 \u2502  (Store Cluster Metadata)     \u2502\n                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"database/#mongodb-shell-commands","title":"Mongodb shell commands","text":"<p>1\ufe0f\u20e3 Start &amp; Connect</p> Task Command Start MongoDB Server <code>mongod</code> Start MongoDB Shell <code>mongosh</code> Connect to Local MongoDB <code>mongosh mongodb://localhost:27017</code> Connect with Authentication <code>mongosh -u username -p password --authenticationDatabase admin</code> <p>2\ufe0f\u20e3 Database Commands</p> Task Command Show Databases <code>show dbs</code> Switch / Create Database <code>use userdb</code> Check Current Database <code>db</code> Drop Database <code>db.dropDatabase()</code> <p>3\ufe0f\u20e3 Collection Commands</p> Task Command Show Collections <code>show collections</code> Create Collection <code>db.createCollection(\"users\")</code> Drop Collection <code>db.users.drop()</code> <p>4\ufe0f\u20e3 CREATE Operations</p> Task Command Insert One <code>db.users.insertOne({ name: \"Ram\", city: \"Hyderabad\" })</code> Insert Many <code>db.users.insertMany([{ name: \"Ram\" }, { name: \"Krishna\" }])</code> <p>5\ufe0f\u20e3 READ Operations</p> Task Command Find All <code>db.users.find()</code> Pretty Output <code>db.users.find().pretty()</code> Find One <code>db.users.findOne({ name: \"Ram\" })</code> Find with Condition <code>db.users.find({ city: \"Delhi\" })</code> Limit Results <code>db.users.find().limit(2)</code> Sort Ascending <code>db.users.find().sort({ name: 1 })</code> Sort Descending <code>db.users.find().sort({ name: -1 })</code> <p>6\ufe0f\u20e3 UPDATE Operations</p> Task Command Update One <code>db.users.updateOne({ name: \"Ram\" }, { $set: { city: \"Chennai\" } })</code> Update Many <code>db.users.updateMany({ city: \"Delhi\" }, { $set: { city: \"New Delhi\" } })</code> Replace Document <code>db.users.replaceOne({ name: \"Ram\" }, { name: \"Ram\", city: \"Bangalore\" })</code> <p>7\ufe0f\u20e3 DELETE Operations</p> Task Command Delete One <code>db.users.deleteOne({ name: \"Ram\" })</code> Delete Many <code>db.users.deleteMany({ city: \"Mumbai\" })</code> Delete All <code>db.users.deleteMany({})</code> <p>8\ufe0f\u20e3 Query Operators</p> Type Example Greater Than <code>db.users.find({ age: { $gt: 25 } })</code> Less Than <code>db.users.find({ age: { $lt: 40 } })</code> AND Condition <code>db.users.find({ $and: [{ city: \"Hyderabad\" }, { age: { $gt: 25 } }] })</code> OR Condition <code>db.users.find({ $or: [{ city: \"Delhi\" }, { city: \"Mumbai\" }] })</code> <p>9\ufe0f\u20e3 Index Commands</p> Task Command Create Index <code>db.users.createIndex({ name: 1 })</code> Show Indexes <code>db.users.getIndexes()</code> Drop Index <code>db.users.dropIndex({ name: 1 })</code> <p>\ud83d\udd1f User &amp; Admin Commands</p> Task Command Switch to Admin DB <code>use admin</code> Create User <code>db.createUser({ user: \"admin\", pwd: \"password\", roles: [\"root\"] })</code> Show Users <code>db.getUsers()</code> <p>1\ufe0f\u20e31\ufe0f\u20e3 Replica Set Commands</p> Task Command Initialize Replica Set <code>rs.initiate()</code> Check Replica Status <code>rs.status()</code> <p>1\ufe0f\u20e32\ufe0f\u20e3 Server Status</p> Task Command MongoDB Version <code>db.version()</code> Server Status <code>db.serverStatus()</code> Current Connections <code>db.serverStatus().connections</code> <p>1\ufe0f\u20e33\ufe0f\u20e3 Backup &amp; Restore (CLI)</p> Task Command Backup Database <code>mongodump --db userdb --out /backup/</code> Restore Database <code>mongorestore /backup/userdb</code>"},{"location":"devopsTools/","title":"DevOps Tools","text":""},{"location":"devopsTools/#docker","title":"Docker","text":""},{"location":"devopsTools/#what-is-docker","title":"What is Docker?","text":"<p>Docker is a software development tool and a virtualization technology that makes it easy to develop, deploy, and manage applications by using containers. A container refers to a lightweight, stand-alone, executable package of a piece of software that contains all the libraries, configuration files, dependencies, and other necessary parts to operate the application.</p>"},{"location":"devopsTools/#docker-vs-virtual-machine","title":"Docker VS Virtual Machine","text":"<ul> <li>Architecture of Operating System</li> </ul>"},{"location":"devopsTools/#usage-of-docker-over-virtual-machine","title":"Usage of Docker over Virtual Machine","text":"<ul> <li>docker use system's OS kernel</li> <li>where as Virtual MAchine creates its own OS Kernel in users system (it allocate CPU,RAM more compare to docker)</li> </ul>"},{"location":"devopsTools/#advantages-of-using-docker-over-vms","title":"Advantages of using Docker over VMs","text":"<ul> <li>Lightweight</li> <li>Faster boot up time</li> <li>Resource efficient</li> <li>Security</li> </ul>"},{"location":"devopsTools/#what-is-container","title":"What is Container","text":"<ol> <li>Container is a way of packing application along with some dependencies, specific version and necessary configuration.  </li> <li>This can be portable i.e (easily share and move).  </li> <li>So where this containers stored??     This can be upload/stored in Repositories (Docker Hub, Global Quay)</li> </ol>"},{"location":"devopsTools/#how-application-runs-before-using-docker-containers","title":"How application runs Before using docker Containers","text":"<ol> <li>Installation process different (for window-.exe, ubuntu-.zip/tar file)</li> <li>Due to Different OS (file wont execute)</li> </ol>"},{"location":"devopsTools/#after-containers","title":"After Containers","text":"<ol> <li>Own Isolated Environment</li> <li>Can install necessary packages and dependencies</li> <li>Run the application without worrying about the underlying infrastructure.</li> <li>Lightweight as compared to Virtual Machines.</li> <li>Faster than VMs because of sharing resources with other containers.</li> <li>Security - No need for installing software on server, just pull from repository.</li> <li>Resource Efficient - Only what you need.</li> <li>Portability - You can run it anywhere.</li> </ol>"},{"location":"devopsTools/#docker-command","title":"Docker Command","text":"<p>Docker image command</p> <pre><code>docker build  ## It will build Docker images by using the Dockerfile.  \ndocker pull  ## Docker pull command will pull the Docker image which is available in the docker-hub.  \ndocker images  ## It will list all the images which are pulled and build in the docker host.  \ndocker inspect  ## It will helps to debug the docker image if any errors occurred while building an image or pulling the image.  \ndocker push  ## Docker command will push the docker image into the Docker hub.  \ndocker save  ## It will save the docker image in the form of dockerfile.  \ndocker rmi  ## It will remove the docker image.  \n</code></pre> <p>Docker Container Command</p> <pre><code>docker attach  ## Connecting to an Existing Container\ndocker ps  ## To list the running containers.  \ndocker container inspect infinite  ## To Inspect the Docker containers.  \ndocker exec  ## To execute the commands in the running containers.  \ndocker cp  ## To copy the file from docker host to the docker containers\n</code></pre>"},{"location":"devopsTools/#cmd-vs-entrypoint","title":"CMD VS ENTRYPOINT","text":"<p>CMD</p> <p>Specifies the default command to run when the container starts.</p> <p>Can be overridden at runtime by passing arguments to docker run.</p> <p>Often used for providing defaults (like parameters to ENTRYPOINT).</p> <p>Example:</p> <pre><code>FROM ubuntu:20.04\nCMD [\"echo\", \"Hello from CMD!\"]\n</code></pre> <p>Run:</p> <pre><code>docker build -t cmd-example .\ndocker run cmd-example\n</code></pre> <p>\ud83d\udc49 Output: Hello from CMD!</p> <p>Now override it:</p> <pre><code>docker run cmd-example echo \"Overridden!\"\n</code></pre> <p>\ud83d\udc49 Output: Overridden!</p> <p>So CMD is flexible \u2014 it\u2019s like a default that the user can change.</p> <p>\ud83d\udd39 ENTRYPOINT</p> <p>Defines the main command that will always run.</p> <p>Arguments passed to docker run are appended to ENTRYPOINT (not overriding it, unless you use --entrypoint).</p> <p>Best for containers that behave like executables.</p> <p>Example:</p> <pre><code>FROM ubuntu:20.04\nENTRYPOINT [\"echo\", \"Hello from ENTRYPOINT!\"]\n</code></pre> <p>Run:</p> <pre><code>docker build -t entry-example .\ndocker run entry-example\n</code></pre> <p>\ud83d\udc49 Output: Hello from ENTRYPOINT!</p> <p>Now pass extra args:</p> <pre><code>docker run entry-example world\n</code></pre> <p>\ud83d\udc49 Output: Hello from ENTRYPOINT! world</p>"},{"location":"devopsTools/#docker-multi-stage-build","title":"Docker Multi-Stage Build","text":"<p>A multi-stage build in Docker allows you to use multiple FROM statements in your Dockerfile, each creating a separate stage of the build process. This approach is particularly useful for creating smaller, more efficient Docker images, as it allows you to copy only the necessary artifacts from one stage to another, leaving behind any intermediate or unnecessary files.</p> <p>Why Use Multi-Stage Builds?</p> <ol> <li>Smaller Images: By copying only the necessary files to the final image, you can significantly reduce the size of your Docker image.  </li> <li>Better Security: You can avoid including build tools and dependencies in the final image, reducing the attack surface.</li> <li>Cleaner Build Process: Each stage can focus on a specific part of the build process, making the Dockerfile easier to understand and maintain.</li> </ol>"},{"location":"devopsTools/#example-building-a-go-application-using-multi-stage-build","title":"Example: Building a Go Application Using Multi-Stage Build","text":"<p>Step 1: Create a Simple Go Application</p> <pre><code>// main.go\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    fmt.Println(\"Hello, Docker Multi-Stage Build!\")\n}\n</code></pre> <p>Step 2: Create the Dockerfile that uses a multi-stage build.</p> <pre><code># Stage 1: Build the Go application\nFROM golang:1.20 AS build\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Copy the Go source code into the container\nCOPY . .\n\n# Build the Go application\nRUN go mod init example.com/multistage &amp;&amp; go build -o app\n\n# Stage 2: Create a minimal image\nFROM alpine:latest\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Copies the compiled binary from the first stage (build) to the second stage's /app directory.\nCOPY --from=build /app/app . \n\n# Run the application\nCMD [\"./app\"]\n</code></pre>"},{"location":"devopsTools/#docker-workflow","title":"Docker workflow","text":""},{"location":"devopsTools/#packer","title":"Packer","text":""},{"location":"devopsTools/#what-is-packer","title":"What is Packer?","text":"<p>Packer is a tools which help to create customize Image from multiple platform from a single source configuration.</p> <p></p>"},{"location":"devopsTools/#stages-of-packer","title":"Stages of Packer","text":""},{"location":"devopsTools/#usage-of-packer","title":"Usage of Packer","text":"<ul> <li>Well there are to stages of create Images &gt; Mutable and Immutable</li> <li>Mutable means changing Continuously.</li> <li>Immutable means needs to configure only one time.</li> <li>Mutable is old way to configure the Images.</li> <li>Where it needs to configure after deploying the application</li> <li>If any case, we want to deploy to multiple server, configure multiple server individually may create new bugs.</li> <li>Where as Packer use Immutable, which is configure deploy deploying to server.</li> <li>Using single configure Image we can spin up multiple server.</li> </ul>"},{"location":"devopsTools/#mutable","title":"Mutable","text":"<p><code>DEPLOY &gt; SERVER &gt; CONFIGURE</code></p> <p></p> <p>Configuring after spinning up server, If any case we need to install dependency into that server we need to install it each individual server, which can lead to issues and Bugs.</p> <p></p>"},{"location":"devopsTools/#immutable","title":"Immutable","text":"<p><code>DEPLOY &gt; CONFIGURE &gt; SERVER</code></p> <p></p> <p>In Immutable Deploying and Configuration is done before hosting to server</p> <p></p> <p>In Immutable using One Packer we can spin up multiple server</p> <p></p>"},{"location":"devopsTools/#argocd","title":"ArgoCD","text":""},{"location":"devopsTools/#what-is-argocd","title":"What is ArgoCD?","text":"<p>ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. It automates the deployment of the desired application states from Git repositories directly to Kubernetes clusters. Argo CD monitors these repositories for changes, ensuring that the deployed application states match the declared states in the repository.</p> <p>Key Features:</p> <ol> <li> <p>Declarative GitOps: Manages your Kubernetes resources declaratively through Git.</p> </li> <li> <p>Automated Sync: Continuously syncs Kubernetes resources to the desired state defined in Git.</p> </li> <li> <p>Visibility: Provides a web UI and CLI for monitoring and managing application deployments.</p> </li> <li> <p>Rollback and Roll Forward: Easy rollbacks and roll forwards to any git commit.</p> </li> <li> <p>Multi-Cluster Management: Can manage applications across multiple clusters.</p> </li> </ol>"},{"location":"devopsTools/#challenges-without-argocd","title":"Challenges without ArgoCD","text":"<p>Without Argo CD or a similar GitOps tool, managing Kubernetes deployments can present several challenges:</p> <ol> <li> <p>Manual Deployment Processes - Error-Prone: Manual deployment steps can introduce human errors. Inconsistent: Different team members might follow slightly different procedures, leading to inconsistencies. Time-Consuming: Manual deployments take more time and can slow down the development cycle.</p> </li> <li> <p>Lack of Single Source of Truth - Configuration Drift: Without a single source of truth, it\u2019s easy for the actual state of the cluster to drift from the desired state. Difficult Rollbacks: Rolling back to a previous state can be challenging without a clear history of changes.</p> </li> <li> <p>Limited Visibility and Auditing - Poor Traceability: Tracking who made changes, what changes were made, and why can be difficult. Compliance Issues: Limited visibility can lead to difficulties in ensuring compliance with regulatory requirements.</p> </li> <li> <p>Inconsistent Environments - Environment Parity: Ensuring that different environments (development, staging, production) are consistent can be challenging without automated synchronization.Complex Management: Managing multiple environments manually can become complex and error-prone.</p> </li> <li> <p>Inefficient Resource Management - Resource Wastage: Manual processes can lead to inefficient use of resources, as it\u2019s harder to optimize and automate resource provisioning and scaling.</p> </li> <li> <p>Scalability Issues - Difficulty Scaling: As the number of services and clusters grows, manually managing deployments becomes increasingly difficult and less scalable. Coordination Overhead: Coordinating deployments across multiple teams and environments can lead to significant overhead.</p> </li> <li> <p>Operational Overheads - Increased Workload: DevOps teams may spend a significant amount of time on repetitive deployment tasks. Focus Diversion: Teams might be diverted from more strategic tasks to handle operational issues.</p> </li> <li> <p>Slow Incident Recovery - Delayed Response: Manually identifying and resolving issues can delay incident recovery. Lack of Automation: Without automated rollback and recovery processes, downtime can be longer.</p> </li> </ol> <p>Summary Implementing a tool like Argo CD helps mitigate these challenges by providing automated, consistent, and reliable deployment processes, ensuring a single source of truth, improving visibility and auditing, and enabling efficient resource management and scalability. This leads to faster, safer, and more efficient software delivery.</p>"},{"location":"devopsTools/#benefits","title":"Benefits","text":"<p>Benefits of using ArgoCD</p> <ol> <li> <p>Automated Deployments</p> <p>Continuous Deployment: Argo CD automates the deployment process, ensuring that changes in the Git repository are automatically applied to the Kubernetes cluster. Synchronization: Keeps your applications in sync with the desired state defined in Git, reducing the chances of configuration drift.</p> </li> <li> <p>Declarative Configuration</p> <p>Single Source of Truth: The desired state of applications is stored in Git, providing a clear and auditable source of truth. Version Control: Every change to the configuration is versioned, enabling easy rollbacks and clear change history.</p> </li> <li> <p>Improved Security</p> <p>Pull-Based Mechanism: Clusters pull configuration changes, reducing the need for direct access to the cluster and thus improving security. Access Control: Permissions can be managed via Git repository access controls, reducing the need for direct cluster access permissions.</p> </li> <li> <p>Enhanced Visibility and Monitoring</p> <p>User Interface: Argo CD provides a web UI and CLI for visualizing the state of applications and monitoring deployment status. Notifications and Alerts: Integration with various notification systems to alert users about the state of deployments.</p> </li> <li> <p>Scalability</p> <p>Multi-Cluster Management: Manage multiple Kubernetes clusters from a single Argo CD instance. Scalable Operations: Easily scale deployments across many clusters and environments.</p> </li> <li> <p>Operational Efficiency</p> <p>Reduced Manual Effort: Automates repetitive deployment tasks, freeing up developers and DevOps teams to focus on more strategic work. Consistent Environments: Ensures that all environments (development, staging, production) are consistent with each other.</p> </li> <li> <p>Improved Reliability and Resilience</p> <p>Self-Healing: Automatically detects and corrects drift from the desired state, enhancing the reliability of deployments. Easy Rollbacks: Simplified rollback to previous application states in case of issues, improving resilience.</p> </li> <li> <p>Compliance and Auditability</p> <p>Audit Trails: Every change is tracked and logged, providing a complete audit trail of who made changes and why. Compliance: Helps in meeting compliance requirements by ensuring configurations are consistent and traceable.</p> </li> <li> <p>Flexibility and Extensibility</p> <p>Custom Plugins: Supports custom resource definitions and can be extended with custom plugins for specific use cases. Integration: Integrates with other CI/CD tools and workflows, enhancing the overall DevOps pipeline.</p> </li> <li> <p>Community and Ecosystem</p> <p>Active Community: Argo CD is part of the CNCF and has an active open-source community, ensuring ongoing improvements and support. Ecosystem: Works well with other tools in the Kubernetes ecosystem, such as Helm, Kustomize, and more.</p> </li> </ol>"},{"location":"devopsTools/#components","title":"Components","text":"<p>Component of ArgoCD</p> <p></p> <ol> <li> <p>API Server</p> <p>Function: Serves as the central control plane for Argo CD. Usage: Exposes the Argo CD API, which can be accessed via CLI, Web UI, or other tools. Handles all CRUD operations for Argo CD resources such as Applications, Projects, and Repositories. Manages authentication and authorization.</p> </li> <li> <p>Repository Server</p> <p>Function: Manages interactions with Git repositories. Usage: Fetches and caches Git repository contents. Serves Git repository information to other Argo CD components, ensuring they have the necessary data to deploy applications.</p> </li> <li> <p>Controller</p> <p>Function: Watches the state of applications and ensures they are in sync with the desired state defined in Git. Usage: Continuously monitors the state of applications in the Kubernetes cluster. Compares the current state of applications with the desired state from Git. Executes synchronization operations to reconcile any differences.</p> </li> <li> <p>Application Controller</p> <p>Function: Manages the lifecycle of Argo CD applications. Usage: Tracks application resources and manages their state. Handles application creation, updates, and deletion. Manages synchronization and health status reporting.</p> </li> <li> <p>Dex</p> <p>Function: Provides OpenID Connect (OIDC) authentication. Usage: Integrates with external identity providers like GitHub, Google, LDAP, etc. Manages user authentication and provides SSO capabilities.</p> </li> <li> <p>Redis</p> <p>Function: Acts as an in-memory data store. Usage: Enhances performance by caching application state and other data. Optional but recommended for large-scale deployments.</p> </li> <li> <p>Web UI</p> <p>Function: Provides a graphical interface for interacting with Argo CD. Usage: Visualizes the state of applications and their synchronization status. Allows users to manage applications, view logs, and perform manual synchronization. Provides a user-friendly way to interact with Argo CD without needing to use the CLI.</p> </li> <li> <p>CLI</p> <p>Function: Command-line interface for interacting with Argo CD. Usage: Enables users to perform all Argo CD operations via the terminal. Useful for scripting and automation of deployment tasks. Offers commands to manage applications, projects, and repositories.</p> </li> </ol>"},{"location":"devopsTools/#ansible","title":"Ansible","text":""},{"location":"devopsTools/#ansible-role-folder-structure","title":"Ansible role folder structure","text":""},{"location":"devopsTools/#service-mesh","title":"Service Mesh","text":""},{"location":"devops_projects/","title":"DevOps Project","text":""},{"location":"devops_projects/#jenkins-sonarqube","title":"Jenkins-Sonarqube","text":"<p>Jenkins: Jenkins is an open-source automation server that helps automate the building, testing, and deploying of software. It's a Java-based platform with plugins for continuous integration.  </p> <p>Sonarqube: SonarQube is a platform for analyzing software for bugs, vulnerabilities, and code smells. It provides reports on duplicated code, coding standards, unit tests, code coverage, code complexity, comments, bugs, and security recommendations. In this project you will be guided how to host Jenkins and Sonarqube using docker, also how to automate CI/Cd pipeline and check code analysis.</p> <ul> <li> <p>here you can find README.md</p> </li> <li> <p>Repo_link</p> </li> </ul>"},{"location":"devops_projects/#build-docker-using-git-actions","title":"Build Docker using Git Actions","text":"<p>Git Actions: GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. This project is quick most simple and easy to implement on how to build a Docker container using GitHub Actions. We will use Python Image and create a Flask web server that serves an <code>index.html</code> file. GitHub Actions will be used to automate the container building process, ensuring that the container is built consistently for every new commit into branch.</p> <ul> <li> <p>here you can find README.md</p> </li> <li> <p>Repo_link</p> </li> </ul>"},{"location":"devops_projects/#host-web-application-in-azure-enabling-firewall","title":"Host Web Application in azure enabling firewall","text":"<ol> <li> <p>Create resource grp</p> </li> <li> <p>Create v-net &gt; enable bastion (create new public IP)</p> </li> <li> <p>Enable azure firewall &gt; create new policy</p> </li> <li> <p>Select CIDR range to create subnets</p> </li> <li> <p>Create virtual-machine &gt; create new key-pair &gt; select inbound port (SSH)</p> </li> <li> <p>Network interface &gt; choose subnet as default &gt; select NIC when VM deleted</p> </li> <li> <p>In VM &gt; bastion &gt; add user name and choose.pem file from local</p> </li> <li> <p>Run <code>apt-get update</code> &gt; <code>cd var/www/html</code> &gt; <code>vim index.html</code> &gt; <code>&lt;h1&gt; hello &lt;h1&gt;</code> &gt; <code>curl localhost:80</code></p> </li> <li> <p>Restart nginx &gt; <code>systemctl restart nginx</code></p> </li> <li> <p>Firewall &gt; firewall policy &gt; DNAT (destination network address translation rules &gt; add rule collection</p> </li> <li> <p>Add rule &gt; source ip address (user IP-address) &gt; destination &gt; public ip of firewall &gt; translated IP (private ip address of VM) &gt; port (80)</p> </li> </ol>"},{"location":"docker/","title":"Hands-on Docker","text":""},{"location":"docker/#python-commands","title":"python commands","text":"<pre><code>python .\\app.py\n\npython -m venv .venv\n\npip install flask\n\npython.exe .\\app.py\n</code></pre>"},{"location":"docker/#create-dockerfile","title":"create Dockerfile","text":"<pre><code># Use an official Python runtime as a parent image\nFROM python:3.8-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container\nCOPY . /app\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 8000 available to the world outside this container\nEXPOSE 8000\n\n# Define the command to run your application\nENTRYPOINT [\"/bin/bash\"]\n</code></pre>"},{"location":"docker/#docker-commands","title":"docker commands","text":"<pre><code>docker build -t image-url:docker-demo .\n(-t for &lt;name&gt;:&lt;tag&gt; | default it will take as latest)\n\ndocker run -it \"image-id\"\n\ndocker run -it -p 8000:8000 d9b65d5d6ab2\n\ndocker login global-registry\n\ndocker push image-url:docker-demo\n\n# docker image pull\ndocker image-url:docker-demo\n</code></pre>"},{"location":"jenkins-sonarqube/","title":"Demo project Jenkins-Sonarqube (Code Anaylsis)","text":"<ol> <li> <p>As we are hosting Jenkins and Sonarqube on 8080 and 9090 respectively kill host (8080 and 9090)  if it is used in backend.</p> <pre><code>netstat -ano | findstr 9090\n\nnetstat -ano | findstr 8080\n\nTCP    0.0.0.0:8080           0.0.0.0:0              LISTENING       7440\nTCP    [::]:8080              [::]:0                 LISTENING       7440\n\ntaskkill /F /PID 7440\nSUCCESS: The process with PID 7440 has been terminated.\n</code></pre> </li> <li> <p>Build docker image following to the docker-compose.yaml file.</p> <pre><code>docker-compose up -d\n</code></pre> </li> <li> <p>URL for jenkins and sonarqube</p> </li> <li> <p>Jenkins URL: http://localhost:8080     Unlock Jenkins and complete the setup wizard using the initialAdminPassword.</p> </li> <li> <p>SonarQube URL: http://localhost:9000     Default credentials: admin/admin</p> </li> <li> <p>Jenkins password</p> <pre><code>docker exec -it jenkins-container cat /var/jenkins_home/secrets/initialAdminPassword\n</code></pre> </li> <li> <p>To shutdown all containers</p> <pre><code>docker-compose down\n</code></pre> </li> <li> <p>Restart containers</p> <pre><code>docker-compose restart\nor\ndocker-compose restart &lt;service-name&gt;\nor\ndocker-compose restart jenkins\ndocker-compose restart sonarqube\n</code></pre> </li> <li> <p>Update src code</p> </li> <li> <p>Browse free-css  for css template, download any template.</p> </li> <li> <p>Create a branch in git Repo and commit the src code.</p> </li> <li> <p>Create a wehbook in github</p> </li> <li> <p>In github &gt; repository &gt; setting &gt; wehbook</p> </li> <li>Add webhook payload url to \"http://192.168.56.1:8080/github-webhook/\" (jenkins URL)</li> <li>Select<ul> <li>pull requests</li> <li>push</li> <li>Let me select individual events</li> </ul> </li> <li> <p>then select add Wehbook at bottom.</p> </li> <li> <p>To trigger pipeline in Jenkins</p> </li> <li> <p>in Jenkins dashboard click on New</p> </li> <li>Enter a item name and select Freestyle Project</li> <li>Select Source Code Management (GIT) and add Git clone URL</li> <li>Specify the respect branch</li> <li>Select GitHub hook trigger for GITScm polling</li> <li>Then click on Save</li> <li> <p>Select \"Build Now\" this will trigger a pipeline</p> </li> <li> <p>Commit any file to the git repo and Jenkins build will trigger automatically</p> </li> <li> <p>In Sonarqube select create a project manually</p> <ol> <li>Enter the details </li> <li>Select Jenkins Option to analyze your repository.</li> <li>Analysis Method &gt; Jenkins &gt; Select GIT as platform</li> <li>Select Others </li> <li>Copy the following code </li> <li>Under User Account &gt; Security &gt; Enter the name and generate token </li> </ol> </li> <li> <p>In Jenkins install Sonarqube and SSHEasy2 plugin  </p> <ol> <li>Dashboard &gt; Manage Jenkins &gt; Plugins &gt; Search [Sonarqube Scanner, SSHEasy2] install</li> </ol> </li> <li> <p>Add Tools Configuration</p> <ol> <li>Dashboard &gt; Manage Jenkins &gt; Tools</li> <li>Select SonarQube Scanner installations, Enter name and Save</li> </ol> </li> <li> <p>In Jenkins Dashboard &gt; Manage Jenkins &gt; System</p> <ol> <li>Select SonarQube servers</li> <li>Enter the input for Name and Sonarqube URL </li> <li>In Server authentication token &gt; Select Add &gt; Jenkins</li> <li>In Kind select Secret Text</li> <li>Input Secret from (step 11.6) </li> <li>In Server authentication token &gt; Select token name and Save</li> </ol> </li> <li> <p>In Jenkins Add Sonar-Qube projectkey</p> <ol> <li>Dashboard &gt; Automated-Pipeline &gt; Configuration</li> <li>Select Build Steps</li> <li>Select Execute SonarQube Scanner</li> <li>Add projectKey(from Step 11.5) in Analysis properties</li> </ol> </li> <li> <p>In Jenkins select respective Pipeline and Select Build Now</p> </li> <li> <p>In Sonarqube &gt; Project (you can watch scanning status)     </p> </li> </ol>"},{"location":"jenkins/","title":"Jenkins","text":"<p>\ud83e\uddf0 Jenkins</p>"},{"location":"jenkins/#overview","title":"Overview","text":"<p>Jenkins is an open-source automation server used to build, test, and deploy applications continuously. It supports Continuous Integration (CI) and Continuous Deployment (CD) by automating software development workflows.</p>"},{"location":"jenkins/#jenkins-architecture","title":"Jenkins Architecture","text":"<p>Jenkins Master/Controller \u2013 Orchestrates builds, schedules jobs, manages configurations.</p> <p>Jenkins Agent/Slave \u2013 Executes builds on remote machines.</p> <p>Communication happens via JNLP (Java Network Launch Protocol) or SSH.</p> <p>This architecture helps in scaling build workloads across multiple nodes.</p>"},{"location":"jenkins/#what-are-the-key-features-of-jenkins","title":"What are the key features of Jenkins?","text":"<p>Open-source and extensible via plugins</p> <p>Supports distributed builds using master-agent architecture</p> <p>Compatible with all major version control systems (Git, SVN, etc.)</p> <p>Provides Declarative and Scripted Pipelines</p> <p>Supports integration with tools like Docker, Kubernetes, Maven, Ansible, etc.</p>"},{"location":"jenkins/#what-is-a-jenkins-job","title":"What is a Jenkins job?","text":"<p>A Jenkins job is a task or unit of work executed by Jenkins.</p> <p>Examples include:</p> <p>Freestyle Project</p> <p>Pipeline Project</p> <p>Multibranch Pipeline</p> <p>Maven Project</p>"},{"location":"jenkins/#what-is-a-jenkinsfile","title":"What is a Jenkinsfile?","text":"<p>A Jenkinsfile is a text file that contains the pipeline definition written in either Declarative or Scripted syntax. It is usually stored in the root of your source code repository.</p>"},{"location":"jenkins/#how-to-start-jenkins-manually","title":"How to start Jenkins manually","text":"<pre><code>jenkins start\njenkins stop\njenkins restart\n</code></pre>"},{"location":"jenkins/#pipeline","title":"Pipeline","text":"<p>There are two main types of pipelines:</p> <p>Type                   | Description Declarative  Pipeline | Simplified, structured syntax. Recommended for most use cases. Scripted Pipeline |     Groovy-based, more flexible and programmable syntax. Ideal for complex workflows.</p>"},{"location":"jenkins/#declarative-pipeline","title":"Declarative Pipeline","text":"<p>\ud83d\udcd8 Overview</p> <p>Declarative Pipeline syntax provides a simple, structured way to define your build process. It starts with a top-level pipeline block and enforces a specific hierarchy of elements (agent, stages, steps, etc.).</p> <p>Easier to read and maintain.</p> <p>Better validation and error handling.</p> <pre><code>pipeline {\n    agent any\n\n    environment {\n        // Define global environment variables\n        BUILD_ENV = 'staging'\n    }\n\n    options {\n        // Keep only last 5 builds\n        buildDiscarder(logRotator(numToKeepStr: '5'))\n        // Set build timeout to 30 minutes\n        timeout(time: 30, unit: 'MINUTES')\n    }\n\n    stages {\n        stage('Checkout') {\n            steps {\n                echo \"Checking out source code...\"\n                checkout scm\n            }\n        }\n\n        stage('Build') {\n            steps {\n                echo \"Building the project...\"\n                sh 'make build'\n            }\n        }\n\n        stage('Test') {\n            steps {\n                echo \"Running tests...\"\n                sh 'make test'\n            }\n        }\n\n        stage('Deploy') {\n            when {\n                branch 'main'\n            }\n            steps {\n                echo \"Deploying to ${BUILD_ENV} environment...\"\n                sh 'make deploy'\n            }\n        }\n    }\n\n    post {\n        success {\n            echo 'Build and deploy succeeded!'\n        }\n        failure {\n            echo 'Build failed!'\n        }\n    }\n}\n</code></pre>"},{"location":"jenkins/#scripted-pipeline","title":"Scripted Pipeline","text":"<p>\ud83d\udcd8 Overview</p> <p>Scripted Pipeline uses Groovy code to define the pipeline. It offers full programmatic control (conditions, loops, functions, etc.).</p> <p>More powerful and flexible.</p> <p>Requires Groovy knowledge.</p> <p>Older style, still widely used.</p> <pre><code>node {\n    stage('Checkout') {\n        echo \"Checking out source code...\"\n        checkout scm\n    }\n\n    stage('Build') {\n        echo \"Building the project...\"\n        sh 'make build'\n    }\n\n    stage('Test') {\n        echo \"Running tests...\"\n        sh 'make test'\n    }\n\n    stage('Deploy') {\n        if (env.BRANCH_NAME == 'main') {\n            echo \"Deploying to production...\"\n            sh 'make deploy'\n        } else {\n            echo \"Skipping deployment for non-main branch.\"\n        }\n    }\n\n    // Post actions\n    try {\n        echo \"Build completed successfully.\"\n    } catch (err) {\n        echo \"Build failed: ${err}\"\n        currentBuild.result = 'FAILURE'\n    }\n}\n</code></pre>"},{"location":"jenkins/#how-do-you-trigger-a-jenkins-pipeline-automatically","title":"How do you trigger a Jenkins pipeline automatically?","text":"<p>SCM polling (e.g., pollSCM(' * * * '))</p> <p>Webhooks from GitHub/GitLab/Bitbucket</p> <p>Build triggers (e.g., Build after other projects are built)</p> <p>Manual trigger</p> <p>API call</p>"},{"location":"jenkins/#what-are-jenkins-plugins","title":"What are Jenkins plugins?","text":"<p>Plugins extend Jenkins functionality (e.g., for Docker, Kubernetes, Slack, SonarQube). They are managed under Manage Jenkins \u2192 Manage Plugins.</p>"},{"location":"jenkins/#what-is-blue-ocean-in-jenkins","title":"What is Blue Ocean in Jenkins?","text":"<p>Blue Ocean is a modern Jenkins UI that provides:</p> <p>Visual representation of pipelines</p> <p>Simplified creation and debugging</p> <p>Enhanced user experience</p>"},{"location":"jenkins/#troubleshooting","title":"Troubleshooting","text":"<p>How do you handle Jenkins build failures?</p> <p>Check console logs.</p> <p>Validate pipeline syntax using the Jenkinsfile validator.</p> <p>Re-run with increased log level.</p> <p>Ensure correct environment setup and plugin versions.</p>"},{"location":"jenkins/#you-have-multiple-branches-with-different-build-steps-how-do-you-handle-this","title":"You have multiple branches with different build steps \u2014 how do you handle this?","text":"<p>Use a Multibranch Pipeline or when conditions in Jenkinsfile:</p> <pre><code>stage('Build') {\n    when {\n        branch 'main'\n    }\n    steps {\n        sh 'make build-prod'\n    }\n}\n</code></pre>"},{"location":"jenkins/#what-are-agents-in-jenkins","title":"What are agents in jenkins","text":"<p>Agent in jenkins (also known as slave) is a machine that connects to Jenkins Master and execute tasks when directed by master.</p>"},{"location":"jenkins/#what-happens-when-jenkins-agent-are-offline-and-what-is-best-practice-in-this-situation","title":"What happens when Jenkins agent are offline and What is best practice in this situation?","text":"<p>If target node is offline or all agents on that particular nodes are occupied building other jobs, then the triggered job has to wait until the node comes online  or agent from another node becomes available to trigger build request.</p>"},{"location":"jenkins/#how-to-stores-credentials-in-jenkins-securely","title":"How to stores credentials in Jenkins securely?","text":"<p>Credential plugin</p> <p>Secret text plugin</p> <p>HashiCorp Vault (Jenkins allow to integrate with HashiCorp Vault)</p>"},{"location":"jenkins/#how-to-debug-if-there-is-problem-with-job-or-pipeline-fails","title":"How to debug if there is problem with job or pipeline fails","text":"<p>Using web interface: we can access logs files by going to Manage Jenkins &gt; System Logs</p> <p>Using the file system: You can access log files by going to Jenkins_Home directory on your Jenkins Server.</p>"},{"location":"jenkins/#how-do-you-integrate-static-code-analysis-tools-into-jenkins-pipeline","title":"How do you integrate Static Code Analysis tools into Jenkins pipeline","text":"<p>Install Sonarqube plugin from Jenkins Management later in pipeline add steps to execute static code analysis.</p>"},{"location":"jenkins/#explain-how-to-move-or-copy-jenkins-one-server-to-another","title":"Explain how to move or copy Jenkins one server to another?","text":"<p>Migrating Jenkins from one server to another involves transferring all configurations, jobs, plugins, and build histories safely while ensuring minimal downtime.</p> <p>There are two main approaches:</p> <p>Manual Migration (copy Jenkins home)</p> <p>Backup &amp; Restore using plugins or tools</p> <ol> <li>Manual Migration (Most Common Approach)</li> </ol> <p>Step-by-Step Process:</p> <p>Step 1: Identify Jenkins Home Directory</p> <p>Check the Jenkins home directory on the old server.</p> <pre><code>On Linux:\n\necho $JENKINS_HOME\n\n\nUsually located at:\n\n/var/lib/jenkins\n\n\nOn Windows:\n\nC:\\Program Files (x86)\\Jenkins\n</code></pre> <p>Step 2: Stop Jenkins Service</p> <p>Before copying, stop Jenkins to avoid file corruption.</p> <pre><code>On Linux:\n\nsudo systemctl stop jenkins\n\n\nOn Windows (PowerShell):\n\nnet stop jenkins\n</code></pre> <p>Step 3: Copy Jenkins Home Directory</p> <p>Copy the entire $JENKINS_HOME directory to the new server.</p> <pre><code>Linux Example:\n\nrsync -avz /var/lib/jenkins/ user@newserver:/var/lib/jenkins/\n\n\nWindows Example:\nUse an SCP tool (like WinSCP) or robocopy:\n\nrobocopy \"C:\\Program Files (x86)\\Jenkins\" \"\\\\newserver\\Jenkins\" /E\n</code></pre> <p>Step 4: Install Jenkins on New Server</p> <p>Install the same Jenkins version (to avoid plugin mismatches).</p> <p>Do not start Jenkins yet.</p> <p>Step 5: Replace Jenkins Home</p> <p>Replace the new server\u2019s $JENKINS_HOME directory with the copied one from the old server.</p> <p>Step 6: Start Jenkins</p> <pre><code>On Linux:\n\nsudo systemctl start jenkins\n\n\nOn Windows:\n\nnet start jenkins\n</code></pre> <p>Then, access Jenkins UI to verify that: Jobs, credentials, and plugins are intact. Build history and configurations are preserved.</p> <p>Step 7: Update System Configurations</p> <p>After starting Jenkins on the new server, update:</p> <p>Node/agent configurations (IP or hostname).</p> <p>Webhooks and service URLs (GitHub, Bitbucket, etc.).</p> <p>Credentials if using machine-specific secrets.</p>"},{"location":"jenkins/#if-jenkins-runs-in-docker-or-kubernetes","title":"If Jenkins Runs in Docker or Kubernetes","text":"<p>If Jenkins is containerized:</p> <p>Backup the Jenkins Home volume.</p> <p>Copy that volume to the new environment.</p> <p>Use the same image version.</p> <pre><code>docker run -d \\\n  -v /path/to/jenkins_home:/var/jenkins_home \\\n  -p 8080:8080 jenkins/jenkins:lts\n</code></pre>"},{"location":"jenkins/#how-to-implement-rolling-update-deployment-strategy-using-jenkins","title":"How to implement rolling update deployment strategy using Jenkins","text":"<p>A rolling update in Jenkins can be implemented by integrating Jenkins with Kubernetes or a deployment tool like Ansible.</p> <p>In Kubernetes, we define the rolling strategy in the deployment YAML (maxUnavailable and maxSurge), and in the Jenkins pipeline, we use kubectl set image followed by kubectl rollout status to apply and monitor the deployment.  </p> <p>This ensures zero downtime \u2014 old pods are terminated only after new pods are healthy</p> <p>If a failure occurs, Jenkins can automatically trigger a kubectl rollout undo rollback.\u201d</p>"},{"location":"jenkins/#a-user-reports-slow-response-time-and-intermittent-failures-when-accessing-jenkins-pipelines","title":"A user reports slow response time and intermittent failures when accessing Jenkins pipelines.","text":"<p>How would you investigate and improve Jenkins performance?</p> <ol> <li>Investigation \u2013 Identify the Problem Area</li> </ol> <p>a. Check Jenkins Master (Controller) Health</p> <p>Inspect CPU, Memory, Disk I/O, and Network usage.</p> <p><pre><code>top\nfree -h\niostat\ndf -h\n</code></pre> If CPU or memory usage is near 90\u2013100%, Jenkins master is overloaded.</p> <p>b. Analyze Jenkins Logs</p> <p>Check Jenkins logs for slow requests, GC issues, or plugin errors:</p> <p>Log locations:</p> <p>Linux: <code>/var/log/jenkins/jenkins.log</code></p> <p>Windows: <code>C:\\Program Files (x86)\\Jenkins\\jenkins.out.log</code></p> <p>Look for:</p> <p><pre><code>SEVERE: hudson.remoting.Channel closed\nWARNING: Slow HTTP response\njava.lang.OutOfMemoryError\n</code></pre> If you see frequent GC pauses or heap exhaustion, increase the JVM heap.</p> <p>c. Database &amp; Logs Maintenance</p> <p>Periodically clean old build data:</p> <p>Manage Jenkins \u2192 Manage Old Data \u2192 Discard Old Builds</p> <p>d. Network and Plugin Issues</p> <p>Validate network latency between Jenkins master and agents.</p> <p>Disable or update outdated plugins \u2014 some plugins (like Pipeline Graph View, Git, or Slack) can cause memory leaks.</p> <p>Check plugin performance via:</p> <p>Manage Jenkins \u2192 Manage Plugins \u2192 Installed</p>"},{"location":"jenkins/#how-do-you-ensure-faster-builds","title":"How do you ensure faster builds?","text":"<p>Use parallel stages.</p> <p>Cache dependencies.</p> <p>Use distributed builds.</p> <p>Clean up old builds.</p> <p>Use lightweight agents like Docker containers.</p>"},{"location":"kubernetes/","title":"Kubernetes \u2638\ufe0f","text":"<p>All about k8.</p>"},{"location":"kubernetes/#definition","title":"Definition","text":"<p>Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.</p> <p>For more information visit here</p>"},{"location":"kubernetes/#why-to-use-k8","title":"Why to use K8","text":"<p>Containers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn't it be easier if this behavior was handled by a system?</p> <p>That's how Kubernetes comes to the rescue! Kubernetes provides you with a framework to run distributed systems resiliently. It takes care of scaling and failover for your application, provides deployment patterns, and more. For example: Kubernetes can easily manage a canary deployment for your system.</p> <p>Kubernetes provides you with:</p> <ul> <li>Service discovery and load balancing Kubernetes can expose a container using the DNS name or   using their own IP address. If traffic to a container is high, Kubernetes is able to load balance and distribute the network traffic so that the deployment is stable.</li> <li>Storage orchestration Kubernetes allows you to automatically mount a storage system of your    choice, such as local storages, public cloud providers, and more.</li> <li>Automated roll outs and rollbacks You can describe the desired state for your deployed   containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate. For example, you can automate Kubernetes to create new containers for your deployment, remove existing containers and adopt all their resources to the new container.</li> </ul> <p>Automatic bin packing You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks. You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources.</p> <ul> <li> <p>Self-healing Kubernetes restarts containers that fail, replaces containers, kills containers   that don't respond to your user-defined health check, and doesn't advertise them to clients until they are ready to serve.</p> </li> <li> <p>Secret and configuration management Kubernetes lets you store and manage sensitive   information, such as passwords, OAuth tokens, and SSH keys. You can deploy and update secrets and application configuration without rebuilding your container images, and without exposing secrets in your stack configuration.</p> </li> </ul>"},{"location":"kubernetes/#architecture","title":"Architecture","text":""},{"location":"kubernetes/#architecture-diagram","title":"Architecture Diagram","text":""},{"location":"kubernetes/#k8-workflow-diagram","title":"K8 workflow diagram","text":""},{"location":"kubernetes/#components","title":"Components","text":""},{"location":"kubernetes/#master-components","title":"Master Components","text":"<p>The master components form the control plane of the Kubernetes cluster, responsible for managing the cluster's overall operations, making high-level decisions (e.g., scheduling), and monitoring/responding to cluster events.</p> <ol> <li> <p>API Server (kube-apiserver)</p> <p>Serves as the front-end interface for the Kubernetes control plane. Exposes the Kubernetes API, enabling interaction between all components within the cluster. Validates and processes API objects such as pods, services, and replication controllers, ensuring proper configuration and functionality.</p> </li> <li> <p>Etcd</p> <p>A distributed key-value store that serves as Kubernetes primary datastore for cluster information. Stores critical data, including configuration details, cluster state, and metadata, accessible by all cluster components. Guarantees strong consistency and durability, making it a vital component for reliable cluster operations.</p> </li> <li> <p>Controller Manager (kube-controller-manager)</p> <p>Manages the cluster\u2019s state by running various controller processes.</p> <p>Examples include:</p> <ul> <li>Node Controller: Handles situations where nodes become unavailable.</li> <li>Replication Controller: Ensures the desired number of pod replicas.</li> <li>Endpoint Controller: Manages endpoint objects for services.</li> </ul> <p>Combines multiple logically independent controllers into a single process, reducing operational complexity.</p> </li> <li> <p>Scheduler (kube-scheduler)</p> <p>Allocates newly created pods to suitable nodes based on resource availability, affinity/anti-affinity rules, and other constraints. Continuously monitors for unscheduled pods and assigns them to appropriate nodes to maintain cluster efficiency and balance.</p> </li> </ol>"},{"location":"kubernetes/#node-components","title":"Node Components","text":"<p>Node components run on every node in the cluster and manage the containers running on those nodes.</p> <ol> <li> <p>kubelet</p> <p>An agent that runs on each node in the cluster. Ensures that the containers described by pod specs are running and healthy. Communicates with the Kubernetes API server and reports the status of the node and the pods running on it.</p> </li> <li> <p>kube-proxy</p> <p>A network proxy that runs on each node. Manages the network rules on nodes, allowing network communication to your pods from inside or outside the cluster. Handles the forwarding of requests to the appropriate pod and ensures that services are accessible.</p> </li> <li> <p>Container Runtime</p> <p>The software responsible for running the containers (e.g., Docker, containerd, CRI-O). Kubernetes supports multiple container runtimes, and the kubelet uses the Container Runtime Interface (CRI) to communicate with them.  </p> </li> </ol>"},{"location":"kubernetes/#additional-components","title":"Additional Components","text":"<ol> <li> <p>Pod</p> <p>The smallest and simplest Kubernetes object, representing a single instance of a running process in the cluster. Typically contains one or more containers that share the same network namespace and storage.</p> </li> <li> <p>Service</p> <p>A Service is an abstraction that defines a logical set of pods and enables network access to them. It provides a stable endpoint (IP and DNS) to access a group of pods, regardless of the underlying changes to the pod instances (e.g., pods scaling up or down)  </p> </li> <li> <p>ConfigMaps and Secrets:</p> <p>ConfigMaps: Used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or configuration files.  </p> <p>Secrets: Similar to ConfigMaps but specifically designed to store sensitive data like passwords, tokens, and keys.  </p> </li> <li> <p>Namespaces:</p> <p>Virtual clusters backed by the same physical cluster. Useful for dividing cluster resources between multiple users, teams, or projects.</p> </li> <li> <p>Persistent Volumes (PVs) and Persistent Volume Claims (PVCs):</p> <p>PV: Storage resources in the cluster, such as an external disk, network storage, etc. PVC: Requests for storage by a user, which are then matched to available PVs.</p> </li> <li> <p>Ingress</p> <p>An Ingress is a more advanced resource that manages external HTTP(S) traffic into the cluster. It provides rules to control how external clients access services within the cluster.</p> <p>Key Features:</p> <ul> <li>Path-Based Routing: Routes incoming traffic based on the URL path (e.g., /api or /static).</li> <li>Host-Based Routing: Routes traffic based on the host name <code>(e.g., api.example.com vs www.example.com)</code>.</li> <li>TLS Support: Manages HTTPS traffic with SSL/TLS certificates.</li> <li>Custom Rules: Supports more advanced traffic management compared to a Service</li> </ul> </li> </ol>"},{"location":"kubernetes/#deployment","title":"Deployment","text":"<ul> <li>deployment</li> </ul>"},{"location":"kubernetes/#node","title":"Node","text":"<ul> <li>nodes</li> </ul>"},{"location":"kubernetes/#pod","title":"Pod","text":"<ul> <li>pods</li> </ul>"},{"location":"kubernetes/#service","title":"Service","text":""},{"location":"kubernetes/#cluster-ip","title":"Cluster IP","text":"<ul> <li>ClusterIP is the default and most common service type.</li> <li>Kubernetes will assign a cluster-internal IP address to ClusterIP service. This makes the service only reachable within the cluster.</li> <li>You cannot make requests to service (pods) from outside the cluster.</li> <li>You can optionally set cluster IP in the service definition file.</li> </ul> <p>Use Cases: Inter service communication within the cluster. For example, communication between the front-end and back-end components of your app.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-backend-service\nspec:\n  type: ClusterIP # Optional field (default)\n  clusterIP: 10.10.0.1 # within service cluster ip range\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre>"},{"location":"kubernetes/#nodeport","title":"NodePort","text":"<ul> <li>NodePort service is an extension of ClusterIP service. A ClusterIP Service, to which the NodePort Service routes, is automatically created.</li> <li>It exposes the service outside of the cluster by adding a cluster-wide port on top of ClusterIP.</li> <li>NodePort exposes the service on each Node\u2019s IP at a static port (the NodePort). Each node proxies that port into your Service. So, external traffic has access to fixed port on each Node. It means any request to your cluster on that port gets forwarded to the service.</li> <li>You can contact the NodePort Service, from outside the cluster, by requesting <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>.</li> <li>Node port must be in the range of 30000\u201332767. Manually allocating a port to the service is optional. If it is undefined, Kubernetes will automatically assign one.</li> <li>If you are going to choose node port explicitly, ensure that the port was not already  used by another service.</li> </ul> <p>Use Cases : When you want to enable external connectivity to your service. Using a NodePort gives you the freedom to set up your own load balancing solution, to configure environments that are not fully supported by Kubernetes, or even to expose one or more nodes\u2019 IPs directly. Prefer to place a load balancer above your nodes to avoid node failure.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-frontend-service\nspec:\n  type: NodePort\n  selector:\n    app: web\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 8080\n    nodePort: 30000 # 30000-32767, Optional field\n</code></pre>"},{"location":"kubernetes/#loadbalancer","title":"LoadBalancer","text":"<ul> <li>LoadBalancer service is an extension of NodePort service. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created.</li> <li>It integrates NodePort with cloud-based load balancers.</li> <li>It exposes the Service externally using a cloud provider\u2019s load balancer.</li> <li>Each cloud provider (AWS, Azure, GCP, etc) has its own native load balancer implementation.</li> <li>The cloud provider will create a load balancer, which then automatically routes requests to your Kubernetes Service.</li> <li>Traffic from the external load balancer is directed at the backend Pods. The cloud  provider decides how it is load balanced.</li> <li>The actual creation of the load balancer happens asynchronously.</li> <li>Every time you want to expose a service to the outside world, you have to create a new LoadBalancer and get an IP address.</li> </ul> <p>Use Cases: When you are using a cloud provider to host your Kubernetes cluster.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-frontend-service\nspec:\n  type: LoadBalancer\n  clusterIP: 10.0.171.123\n  loadBalancerIP: 123.123.123.123\n  selector:\n    app: web\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre>"},{"location":"kubernetes/#externalname","title":"ExternalName","text":"<ul> <li>Services of type ExternalName map a Service to a DNS name, not to a typical selector such as my-service.</li> <li>You specify these Services with the <code>spec.externalName</code> parameter.</li> <li>It maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record with its value.</li> <li>No proxy of any kind is established.</li> </ul> <p>Use Cases: This is commonly used to create a service within Kubernetes to represent an external datastore like a database that runs externally to Kubernetes. You can use that ExternalName service (as a local service) when Pods from one namespace to talk to a service in another namespace.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: ExternalName\n  externalName: my.database.example.com\n</code></pre>"},{"location":"kubernetes/#ingress","title":"Ingress","text":"<ul> <li>ingress</li> </ul>"},{"location":"kubernetes/#architecture-of-ingress","title":"Architecture of Ingress","text":""},{"location":"kubernetes/#ingress-yaml","title":"Ingress YAML","text":""},{"location":"kubernetes/#ingress-vs-internal-service","title":"Ingress Vs Internal Service","text":""},{"location":"kubernetes/#ingress-yaml-for-path","title":"Ingress YAML for path","text":""},{"location":"kubernetes/#volumes","title":"Volumes","text":""},{"location":"kubernetes/#architecture-of-persistent-volume","title":"Architecture of Persistent Volume","text":""},{"location":"kubernetes/#backup-volumes","title":"Backup Volumes","text":"<ul> <li> <p>Volumes</p> </li> <li> <p>We also use Git Repo as Persistent Volume - link</p> </li> </ul>"},{"location":"kubernetes/#storageclass","title":"StorageClass","text":"<p>Follow the docs for further info of StorageClass</p> <p>Template for storage class</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: azurefile-sc\nprovisioner: kubernetes.io/azure-file\nreclaimPolicy: Retain\nmountOptions:\n  - dir_mode=0777\n  - file_mode=0777\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre> <p>Key definition:</p> <ul> <li> <p>volumeBindingMode:   This field specifies when volume binding should occur. In this case, Immediate means that a volume should be provisioned and bound as soon as a PersistentVolumeClaim (PVC) is created. This is in contrast to WaitForFirstConsumer, where the binding is delayed until a pod using the PVC is scheduled onto a node.</p> </li> <li> <p>reclaimPolicy : in Persistent Volumes specifies what should happen to the underlying storage when the associated PersistentVolume (PV) is released or the PersistentVolumeClaim (PVC) is deleted:</p> </li> <li> <p>Retain: When the reclaimPolicy is set to Retain, the PV is not automatically deleted when the associated PVC is deleted. The PV is marked as released, and it's up to the cluster administrator to decide what to do with the data on the storage.</p> </li> <li> <p>Delete: When the reclaimPolicy is set to Delete, the PV is automatically deleted when the associated PVC is deleted. The storage resources associated with the PV are also deleted.</p> </li> </ul>"},{"location":"kubernetes/#pv-and-pvc","title":"PV and PVC","text":"<p>PersistentVolume (PV): A piece of storage in the cluster that has been provisioned by an administrator or dynamically through Storage Classes. PVs are used to store data persistently.</p> <p>PersistentVolumeClaim (PVC): A request for storage by a user. PVCs abstract the details of how storage is provided and consumed, making it easy to request and use persistent storage in pods.</p>"},{"location":"kubernetes/#rbac","title":"RBAC","text":"<p>service account: user groups role: define permissions to user role binding: connect role with service account  </p>"},{"location":"kubernetes/#daemonset","title":"Daemonset","text":"<p>A DaemonSet in Kubernetes ensures that a specific Pod runs on all (or selected) nodes in a cluster. It is primarily used for running background processes or system-level tasks that are required on every node.</p> <p>By understanding and leveraging DaemonSets, administrators can efficiently handle logging, monitoring, networking, and other system-wide tasks.</p> <p>Key Features of DaemonSet</p> <ul> <li>Node-Wide Coverage: Ensures a Pod is scheduled on every eligible node in the cluster.</li> <li>Automatic Updates: When a new node is added to the cluster, the DaemonSet automatically schedules its Pods on the new node.</li> <li>Selective Node Targeting: By using node selectors, taints, and tolerations, DaemonSets can restrict Pods to specific nodes.</li> <li>Resiliency: If a node is deleted or fails, the DaemonSet recreates the Pod on a healthy node.</li> </ul> <p>Use Cases of DaemonSet</p> <ol> <li>Log Collection: Running log collection agents (e.g., Fluentd or Logstash) on every node.</li> <li>Monitoring: Deploying monitoring agents (e.g., Prometheus Node Exporter) to collect node-level metrics.</li> <li>Networking: Running network-related applications like CNI plugins or proxy daemons (e.g., Calico or Istio).</li> <li>Storage: Deploying storage daemons for distributed storage solutions (e.g., Ceph, GlusterFS).</li> </ol> <p>How DaemonSet Works : A DaemonSet controller ensures that a single copy of the specified Pod is running on every eligible node:</p> <ol> <li>When the DaemonSet is created, the controller schedules a Pod on each matching node.</li> <li>If a new node is added, the controller automatically schedules the Pod on that node.</li> <li>If a node is removed, the associated Pod is deleted.</li> </ol>"},{"location":"kubernetes/#hpa-and-vpa","title":"HPA and VPA","text":"<p>Horizontal Pod Autoscaler (HPA) : The Horizontal Pod Autoscaler (HPA) automatically adjusts the number of Pods in a Deployment, ReplicaSet, or StatefulSet based on observed CPU, memory usage, or custom metrics. It ensures that the application can handle workload variations without over-provisioning or under-provisioning.</p> <p>How It Works</p> <ul> <li>HPA monitors the target resource's metrics (e.g., CPU utilization).</li> <li>Based on the observed metrics and the scaling configuration, HPA increases or decreases the number of Pods in the resource.</li> </ul> <p>Use Case - HPA is ideal for scaling stateless applications or services that need to handle fluctuating workloads, such as web servers or APIs.</p> <p>Key Features</p> <ul> <li>Scales Pods horizontally by increasing or decreasing their count.</li> <li>Supports both resource metrics (CPU/memory) and custom/external metrics.</li> </ul> <p>Example YAML</p> <pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nginx-hpa\nspec:\n  scaleTargetRef: # Specifies the target resource (Deployment, ReplicaSet, or StatefulSet).\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx-deployment\n  minReplicas: 1\n  maxReplicas: 10 # Define the minimum and maximum number of Pods.\n  metrics: # Sets the metric type and target utilization (e.g., CPU utilization at 50%).\nCommands to Deploy HPA\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 50\n</code></pre> <p>Vertical Pod Autoscaler (VPA) : The Vertical Pod Autoscaler (VPA) adjusts the resource requests and limits (CPU and memory) of Pods automatically based on actual usage. It ensures that Pods have sufficient resources to run efficiently without wasting resources.</p> <p>How It Works</p> <ul> <li>VPA monitors the resource usage of Pods.</li> <li>Based on the observed usage, VPA recommends or directly updates the resource requests and limits for the Pods.</li> <li>In update mode, Pods are restarted to apply the new resource requests and limits.</li> </ul> <p>Use Case - VPA is ideal for workloads with predictable scaling patterns or that need fine-tuned resource management, such as batch jobs or databases.</p> <p>Key Features</p> <ul> <li>Adjusts resource requests and limits (CPU/memory) for Pods.</li> <li>Operates in three modes:   Off: Only provides recommendations.   Auto: Updates resource requests/limits and restarts Pods.   Initial: Sets resource requests/limits only for new Pods.</li> </ul> <p>Example YAML</p> <pre><code>apiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: nginx-vpa\nspec:\n  targetRef: # Specifies the target resource for VPA (e.g., Deployment).\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx-deployment\n  updatePolicy:\n    updateMode: \"Auto\" # Updates resource requests/limits and restarts Pods.\n</code></pre>"},{"location":"kubernetes/#stateful-vs-stateless-in-kubernetes","title":"Stateful vs Stateless in Kubernetes","text":"<p>In Kubernetes, workloads can be classified as either stateful or stateless, depending on how they handle and persist data. Understanding the difference is critical for designing applications that run effectively in a Kubernetes environment.</p> <p>Stateless Applications: Stateless applications do not store any data or state between sessions or transactions. Each request is independent and does not rely on previous interactions.</p> <p>Characteristics</p> <ul> <li>No dependency on underlying storage.</li> <li>Can be scaled easily as there is no need to synchronize state.</li> <li>Requests can be processed by any instance of the application.</li> <li>Commonly used for APIs, web servers, and microservices.</li> </ul> <p>Examples</p> <ul> <li>Web servers (e.g., Nginx, Apache).</li> <li>RESTful APIs.</li> <li>Batch job workers.</li> </ul> <p>Kubernetes Implementation:</p> <p>Deploy using Deployments or ReplicaSets. Stateless pods can be scaled horizontally using kubectl scale or Horizontal Pod Autoscaler (HPA). Use ephemeral storage like /tmp or environment variables for temporary data.</p> <p>Stateful Applications: Stateful applications maintain a state or rely on persisted data across sessions or interactions. This state might be specific to a user, session, or process.</p> <p>Characteristics:</p> <ul> <li>Depend on persistent storage (e.g., databases, file systems).</li> <li>Require a unique identity for each instance (pod) to preserve state.</li> <li>Often need order or consistency in the deployment and scaling process.</li> <li>More complex to scale compared to stateless applications.</li> </ul> <p>Examples:</p> <ul> <li>Databases (e.g., MySQL, PostgreSQL, MongoDB).</li> <li>Message queues (e.g., RabbitMQ, Kafka).</li> <li>Stateful systems like Elasticsearch.</li> </ul> <p>Kubernetes Implementation: Deploy using StatefulSets to ensure each pod gets a unique identity (e.g., mypod-0, mypod-1). Use PersistentVolume (PV) and PersistentVolumeClaim (PVC) for storage. Data is preserved even if pods are deleted or rescheduled.</p> <p>Use Cases.</p> <p>Stateless Applications:</p> <p>Use cloud-native storage solutions for external state (e.g., S3, RDS). Leverage Kubernetes' scalability features for performance optimization.</p> <p>Stateful Applications:</p> <p>Use StatefulSets to ensure unique pod identities and persistent data. Employ proper backup strategies for PersistentVolumes. Configure pod anti-affinity to distribute pods across nodes for high availability.</p>"},{"location":"kubernetes/#deployment-strategy","title":"Deployment Strategy","text":"<p>Kubernetes provides several deployment strategies to manage application updates, ensuring minimal downtime, stability, and user experience during the deployment process. Below is a detailed explanation of Canary, Blue-Green, Recreate, and Rolling Update strategies, along with real-world scenarios for when and why they are used.</p> <ol> <li> <p>Canary Deployment</p> <p>Description: In a canary deployment, a new version of the application is gradually rolled out to a small subset of users or traffic. If it performs well, the rollout continues until the new version fully replaces the old version.</p> <p>Steps:</p> <ul> <li>Deploy the new version to a small percentage of users (e.g., 5-10% of traffic).  </li> <li>Monitor metrics like performance, error rates, or user feedback.  </li> <li>Gradually increase the traffic directed to the new version until it fully replaces the old one.</li> </ul> <p>Use Case: Scenario: A retail website launching a new search feature. The search functionality is critical, and any errors could impact customer satisfaction. Why Use: Canary deployment ensures that only a small fraction of users experience issues, allowing rollback with minimal impact if the new feature causes errors.</p> <p>Pros:   - Minimizes risk by exposing new changes to a small subset of users initially.   - Real-world testing under production conditions.   - Easier to detect issues early in the rollout process.  </p> <p>Cons:   - Requires advanced traffic management tools (e.g., Istio or NGINX for traffic splitting).   - Monitoring and rollback processes need to be well-defined.    </p> </li> <li> <p>Blue-Green Deployment</p> <p>Description: Two environments, \"Blue\" (current version) and \"Green\" (new version), are maintained simultaneously. Once the new version is verified in the Green environment, traffic is switched from Blue to Green.</p> <p>Steps:</p> <ul> <li>Deploy the new version to the Green environment.  </li> <li>Test the Green environment for functionality, performance, and stability.  </li> <li>Redirect all traffic from the Blue to the Green environment (via a load balancer or DNS change).  </li> <li>The Blue environment serves as a backup in case of rollback.</li> </ul> <p>Use Case:</p> <p>Scenario: A banking application rolling out a new version of its transaction processing service. Downtime is unacceptable, and rollback must be quick. Why Use: Blue-Green ensures zero downtime during deployment and allows for an instant rollback by switching back to the Blue environment.</p> <p>Pros: - Zero downtime during deployment. - Instant rollback by switching traffic back to the old environment. - Both environments are fully isolated, reducing the risk of deployment errors.</p> <p>Cons: - Requires double the infrastructure (temporary) for both environments. - Higher cost during deployment phases due to resource duplication.  </p> </li> <li> <p>Recreate Deployment</p> <p>Description: The current version of the application is terminated completely, and the new version is deployed in its place. This is a \"replace all\" strategy.</p> <p>Steps: - Terminate all pods running the old version of the application. - Deploy the new version.</p> <p>Use Case: Scenario: A batch processing job where downtime is acceptable and maintaining multiple versions concurrently is unnecessary. Why Use: Recreate is simple, cost-effective, and appropriate when the application doesn't need to handle live user traffic during deployment.</p> <p>Pros: - Simple to implement with minimal complexity. - No need to manage traffic splitting or multiple versions. - Suitable for stateless or low-traffic applications.</p> <p>Cons: - Causes downtime as the old version is terminated before the new version becomes available. - No gradual transition, so any errors immediately impact all users.</p> </li> <li> <p>Rolling Update Deployment</p> <p>Description: A gradual replacement of old pods with new pods, ensuring some pods of the old version remain available while new pods are deployed.</p> <p>Steps: - Deploy the new version pod-by-pod, gradually replacing old pods. - Maintain a specified number of pods running at all times during the update. - Continue until all old pods are replaced.</p> <p>Use Case: Scenario: A video streaming platform updating its content delivery service. Continuous service is critical, and downtime is unacceptable. Why Use: Rolling updates ensure zero downtime by maintaining the availability of old pods while the new version is rolled out.</p> <p>Pros: - Zero downtime during deployment. - Gradual replacement reduces the risk of introducing issues. - Supports controlled rollout by limiting the number of simultaneous updates.</p> <p>Cons: - Rollback is slower compared to Canary or Blue-Green. - Not suitable for breaking changes that require all pods to be updated simultaneously.</p> </li> </ol> <p>Comparison Table:</p> Strategy Downtime Risk Rollback Complexity Resource Requirements Best For Canary No Low Easy (stop traffic to new pods) Moderate Gradual rollouts for feature testing. Blue-Green No Very Low Very Easy (switch traffic back) High (requires two environments) Critical applications needing zero downtime. Recreate Yes High Moderate Low Simple updates where downtime is acceptable. Rolling Update No Moderate Moderate Low Applications requiring zero downtime with gradual updates."},{"location":"kubernetes/#k8-pipeline-architecture","title":"k8-pipeline architecture","text":""},{"location":"kubernetes/#scenario","title":"Scenario","text":"<ol> <li> <p>How to fix K8 deployment   </p> </li> <li> <p>How to manage k8 resource (so that pods should not exceeds resources / should not consume whole cluster/namespace Quota   </p> </li> <li> <p>How to upgrade k8 version</p> </li> </ol>"},{"location":"kubernetes/#k8-yaml-generator","title":"K8 yaml generator","text":"<p>Link here</p>"},{"location":"learnings/","title":"Learnings","text":""},{"location":"learnings/#az","title":"Az","text":""},{"location":"learnings/#app-service","title":"App Service","text":"<p>Azure Web App is a Platform-as-a-Service (PaaS) offering from Microsoft Azure that allows you to host web applications, RESTful APIs, and mobile backends without managing the underlying infrastructure. It is part of the Azure App Service platform, which provides a range of features for building and hosting web applications.</p> <p>You can host az web app using cli, for this index.html page should be present in parent dir.</p> <pre><code>az webapp up -g $resourceGroup -n $appName --html --location EastUS\n</code></pre>"},{"location":"learnings/#steps-to-setup-app-service","title":"Steps to setup app service","text":"<p>Create Azure Web App to host Py flask web application, using Azure CLI</p> <ol> <li> <p>az login</p> </li> <li> <p>create resource group</p> <pre><code>resourceGroup=$(az group list --query \"[].{id:name}\" -o tsv)\nappName=az204app$RANDOM\n</code></pre> </li> <li> <p>Verify resource group</p> <pre><code>echo $resourceGroup\n</code></pre> </li> <li> <p>Create dir and cd dir</p> </li> <li> <p>Git clone this repo</p> </li> <li> <p>Run az web app create command</p> <pre><code>az webapp up -g $resourceGroup -n $appName --runtime \"PYTHON:3.9\" --location CentralUS\n</code></pre> <p>this will create web app service and zip src code, it will recognize that the src is python structure as py and requirements.txt will be present.</p> </li> <li> <p>It will show the following logs and url hosted to access flask app.</p> <pre><code>ramakrishnabotla04 [ ~/demo/azure ]$ az webapp up -g $resourceGroup -n $appName --runtime \"PYTHON:3.9\" --location CentralUS\nWebapp 'az204app18227' already exists. The command will deploy contents to the existing app.\nCreating AppServicePlan 'ramakrishnabotla04_asp_5823' or Updating if already exists\nReadonly attribute name will be ignored in class &lt;class 'azure.mgmt.web.v2023_01_01.models._models_py3.AppServicePlan'&gt;\nCreating zip with contents of dir /home/ramakrishnabotla04/demo/azure ...\nGetting scm site credentials for zip deployment\nStarting zip deployment. This operation can take a while to complete ...\nDeployment endpoint responded with status code 202\nPolling the status of async deployment. Start Time: 2024-08-25 13:44:55.291459+00:00 UTC\nStatus: Building the app... Time: 3(s)\nStatus: Building the app... Time: 19(s)\nStatus: Build successful. Time: 36(s)\nStatus: Starting the site... Time: 52(s)\nStatus: Site started successfully. Time: 69(s)\nYou can launch the app at http://az204app18227.azurewebsites.net\nSetting 'az webapp up' default arguments for current directory. Manage defaults with 'az configure --scope local'\n--resource-group/-g default: learn-e2a9e9ac-ede2-4607-8219-42db48e581fa\n--sku default: F1\n--plan/-p default: ramakrishnabotla04_asp_5823\n--location/-l default: centralus\n--name/-n default: az204app18227\n{\n\"URL\": \"http://az204app18227.azurewebsites.net\",\n\"appserviceplan\": \"ramakrishnabotla04_asp_5823\",\n\"location\": \"centralus\",\n\"name\": \"az204app18227\",\n\"os\": \"Linux\",\n\"resourcegroup\": \"learn-e2a9e9ac-ede2-4607-8219-42db48e581fa\",\n\"runtime_version\": \"PYTHON|3.9\",\n\"runtime_version_detected\": \"-\",\n\"sku\": \"PAID\",\n\"src_path\": \"//home//ramakrishnabotla04//demo//azure\"\n}\n</code></pre> </li> <li> <p>Access py flask app using url</p> <pre><code>http://az204app18227.azurewebsites.net\n</code></pre> </li> </ol>"},{"location":"learnings/#setup-project-configure-complete-cicd","title":"Setup Project configure complete CI/CD","text":"<p>Host voting app src provided by docker, docker sample voting app</p>"},{"location":"learnings/#steps-to-setup-project","title":"Steps to setup project","text":"<p>Need to create 3 different pipelines for vote result worker</p> <p>Project CI, setup VM and use as Agent.</p> <ol> <li>create a new project (import from github)</li> <li>create a new resource grp</li> <li>create vm and container registry to store docker image</li> <li>create pool in az board settings &gt; agentpool.</li> <li>After creating agentpool &gt; agent_pool_name &gt; new agent &gt; select the OS to install agent install script.</li> <li>login in VM (with password or ssh) and install agent script.</li> <li>to create and push docker image. Install docker in VM.</li> <li>create new pipeline to build and push to az Container registry for vote Docker image</li> </ol> <p>Project CD setup hosting K8 using Azure AKS and install Argocd to deploy K8 components.</p> <ol> <li>create k8 service in azure portal.</li> <li>choose VM to install K8 cluster and to manage pods in it.</li> <li>login to K8 and install Argocd.</li> <li>access argocd and configure repo connection</li> <li>deploy k8-specifications.</li> <li> <p>to pull Image from Private registry. Configure K8 ImagePullSecrets</p> <pre><code>kubectl create secret docker-registry &lt;secret-name&gt; \\\n--namespace &lt;namespace&gt; \\\n--docker-server=&lt;container-registry-name&gt;.azurecr.io \\\n--docker-username=&lt;service-principal-ID&gt; \\\n--docker-password=&lt;service-principal-password&gt;\n</code></pre> </li> <li> <p>port forward the vote svc to access externally.</p> </li> </ol>"},{"location":"learnings/#argocd","title":"Argocd","text":""},{"location":"learnings/#handle-multi-cluster-and-configure-argocd","title":"Handle multi-cluster and configure Argocd","text":"<p>Configuring Argo-cd to multi-cluster is also known as Hub-Spoke Model.</p>"},{"location":"learnings/#what-is-the-hub-and-spoke-model-strategy","title":"What is the hub and spoke model strategy","text":"<p>The hub and spoke model is a model of distribution for goods in which there's a single centralized hub. All the goods sent to customers come from this one hub before being sent to distribution centers and then to consumers. The hub is the central location, while the spokes are the small distribution centers.</p>"},{"location":"learnings/#setup","title":"Setup","text":"<ol> <li>Create 3 clusters as hub and 2 spoke</li> <li>kubectl config get-contexts (to display all created cluster)</li> <li> <p>login to hub cluster</p> <p><code>kubectl config use-context &lt;cluster-name&gt;</code></p> </li> <li> <p>Verify cluster login</p> <p><code>kubectl config current-context</code></p> </li> <li> <p>create namespace and install Argocd</p> </li> <li> <p>port forward, setup HTTP to access Argo-cd UI.</p> </li> <li> <p>Add cluster to Argocd, this is done by argocd cli.</p> </li> </ol> <p>a. <code>kubectl config get-clusters</code> display all available cluster    b. <code>agrocd login &lt;ip-address:port-or-url&gt;</code> login to argocd    c. <code>argocd add cluster &lt;cluster-name&gt;</code></p>"},{"location":"linux/","title":"All About Linux","text":""},{"location":"linux/#linux-folder-usage","title":"Linux folder usage","text":""},{"location":"linux/#file-permissions","title":"File Permissions","text":""},{"location":"linux/#umask","title":"Umask","text":"<p>The umask (short for \"user file creation mode mask\") is a Linux/Unix system setting that determines the default permissions for newly created files and directories. It effectively \"masks out\" certain permission bits, preventing them from being set when new files or directories are created.</p> <p>Understanding File and Directory Permissions In Linux, file and directory permissions are represented by three sets of bits, each corresponding to the user (owner), group, and others. These are usually represented as three digits in octal (base-8), where each digit can be a combination of:</p> <p>4: Read (r) 2: Write (w) 1: Execute (x)</p> <p>For example:</p> <p>7 (4 + 2 + 1) means read, write, and execute permissions. 6 (4 + 2) means read and write permissions. 5 (4 + 1) means read and execute permissions.  </p> <p>666 defined as read and write for everyone.</p> <p>Default Permissions Before umask Files: The default permissions for a newly created file are usually 666 (read and write for everyone). Files do not have execute permissions by default. Directories: The default permissions for a newly created directory are 777 (read, write, and execute for everyone). How umask Works The umask subtracts (or masks out) permissions from these defaults. The umask value is subtracted from the default permissions to determine the actual permissions of a new file or directory.</p> <p>For example:</p> <p>A umask of 022 masks out the write permission for the group and others: Files: 666 (default) - 022 = 644 (read and write for the owner, read-only for group and others). Directories: 777 (default) - 022 = 755 (read, write, and execute for the owner, read and execute for group and others).</p> <p>Common umask Values 022: Common default value, resulting in 755 for directories and 644 for files. 002: Allows group members to have write permissions, resulting in 775 for directories and 664 for files. 077: Highly restrictive, giving full access only to the owner, resulting in 700 for directories and 600 for files.  </p>"},{"location":"linux/#chmod","title":"chmod","text":"<p>Chmod is use for Modify permission of file/directory</p> <pre><code>#drwxrwxrwx &gt; rwx abbreviates as Read, Write , Delete/Execute\n\nchmod 777 \"file-name/folder-name\"\n</code></pre> <p>Chown is use for providing permissions or changing the ownership to other User</p> <pre><code>#change Owner/User\nchown \"user-name\" \"file/folder-name\"\n\n#change Owner and Group\nchown \"user-name\":\"group-name\" \"file/folder-name\"\n</code></pre> <p>Chmod calculator</p>"},{"location":"linux/#steps-to-add-new-user","title":"Steps to Add New User","text":"<pre><code># To add user in linux\nsudo adduser &lt;user-name&gt;  #it will ask for password enter new password\n\n# After adding the user, you can switch to the new user's shell environment using the following command:\nsu - &lt;user-name&gt;\n\n# after login with new user, try to execute\napt-get update #you may be get error as incident will be reported.\n\n# update the permissions for new user, intially logout from the new user\nexit\n\n# from the root user, execute the following command\nsudo visudo\n\n# update members of the admin group\n\n%&lt;username&gt; ALL=(ALL) NOPASSWD:ALL     #add this line\n</code></pre>"},{"location":"linux/#change-host-name","title":"Change host name","text":"<pre><code>sudo hostnamectl set-hostname &lt;enter-hostname&gt;\n/bin/bash\n</code></pre>"},{"location":"linux/#linux-commands","title":"Linux commands","text":""},{"location":"linux/#mostly-used-commands","title":"Mostly Used Commands","text":"Description Command Help for commands <code>man &lt;command&gt;</code> Change directory <code>cd &lt;directory&gt;</code> List directory contents <code>ls</code> Print working directory <code>pwd</code> Create a new directory <code>mkdir &lt;directory&gt;</code> Remove an empty directory <code>rmdir &lt;directory&gt;</code> Delete directory (recursively) <code>rm -r &lt;directory&gt;</code> Remove files <code>rm &lt;file&gt;</code> Concatenate and display file content <code>cat &lt;file&gt;</code> Create an empty file <code>touch &lt;file&gt;</code> Edit/create file with content <code>vim &lt;file&gt;</code> Vim commands Insert: <code>i</code>, Save and exit: <code>ESC :wq</code>, Exit without saving: <code>ESC :q!</code> Move or rename files and directories <code>mv &lt;source&gt; &lt;destination&gt;</code> Copy file <code>cp &lt;source-file&gt; &lt;destination-file&gt;</code> Copy file to a directory <code>cp &lt;file&gt; &lt;directory&gt;</code> Modify file/directory permissions <code>chmod &lt;permissions&gt; &lt;file&gt;</code> Change file/directory ownership <code>chown &lt;user&gt; &lt;file&gt;</code> Change ownership and group of file <code>chown &lt;user&gt;:&lt;group&gt; &lt;file&gt;</code>"},{"location":"linux/#file-compression-and-archiving","title":"File Compression and Archiving","text":"Description Command Create a tar archive <code>tar cf &lt;archive.tar&gt; &lt;file/dir&gt;</code> Extract a tar archive <code>tar -xvf &lt;archive.tar&gt;</code> Compress files using gzip <code>gzip &lt;file&gt;</code> Decompress files using gunzip <code>gunzip &lt;file&gt;</code> Create a compressed tar file <code>tar -czvf &lt;archive.tar.gz&gt; &lt;file/dir&gt;</code> Extract a compressed tar file <code>tar -xzvf &lt;archive.tar.gz&gt;</code> Compress files using bzip2 <code>bzip2 &lt;file&gt;</code> Extract a ZIP file <code>unzip &lt;file.zip&gt;</code>"},{"location":"linux/#networking-commands","title":"Networking Commands","text":"Description Command Test network connectivity <code>ping &lt;host&gt;</code> Display network configuration <code>ifconfig</code> Display network connections <code>netstat</code> Display system routes <code>route</code> DNS lookup <code>nslookup &lt;domain&gt;</code> Detailed DNS lookup <code>dig &lt;domain&gt;</code> Trace network path <code>traceroute &lt;host&gt;</code> Secure copy files <code>scp &lt;source&gt; &lt;user&gt;@&lt;host&gt;:&lt;destination&gt;</code> Synchronize files/directories remotely <code>rsync &lt;source&gt; &lt;destination&gt;</code> SSH into a remote machine <code>ssh &lt;user&gt;@&lt;host&gt;</code>"},{"location":"linux/#system-monitoring-and-management","title":"System Monitoring and Management","text":"Description Command Display disk usage <code>df -h</code> Show directory/file disk usage <code>du -sh</code> Display CPU usage <code>top</code> Display CPU, RAM usage <code>htop</code> Display RAM memory usage <code>free -h</code> Display uptime and load averages <code>uptime</code> Display running processes <code>ps aux</code> Display real-time system processes <code>top</code> Terminate a process <code>kill &lt;PID&gt;</code> Force terminate a process <code>kill -9 &lt;PID&gt;</code> Display kernel messages <code>dmesg</code> View system journal logs <code>journalctl</code>"},{"location":"linux/#user-and-group-management","title":"User and Group Management","text":"Description Command Add a new user <code>useradd &lt;username&gt;</code> Change user password <code>passwd &lt;username&gt;</code> Switch to another user <code>su &lt;username&gt;</code> Display current user <code>whoami</code> Add a new group <code>groupadd &lt;group&gt;</code> Delete a group <code>groupdel &lt;group&gt;</code> Modify a user <code>usermod</code> Modify a group <code>groupmod</code>"},{"location":"linux/#utilities","title":"Utilities","text":"Description Command Search for files <code>find &lt;path&gt; -name &lt;pattern&gt;</code> Search for text in files <code>grep &lt;pattern&gt; &lt;file&gt;</code> Count lines, words, characters in file <code>wc &lt;file&gt;</code> Display first N lines of a file <code>head &lt;file&gt;</code> Display last N lines of a file <code>tail &lt;file&gt;</code> Sort lines in a file <code>sort &lt;file&gt;</code> Remove duplicate lines in a file <code>uniq &lt;file&gt;</code> Redirect output to multiple files <code>tee &lt;file&gt;</code> Monitor command output <code>watch &lt;command&gt;</code> Display a calendar <code>cal</code>"},{"location":"linux/#miscellaneous","title":"Miscellaneous","text":"Description Command Display system date and time <code>date</code> Display hardware information <code>lshw</code> Display PCI devices <code>lspci</code> Display USB devices <code>lsusb</code> Locate files <code>locate &lt;file&gt;</code> Find command binary path <code>which &lt;command&gt;</code> Display environment variables <code>env</code> Print a message to terminal <code>echo &lt;message&gt;</code> Display command history <code>history</code> Schedule recurring tasks <code>crontab -e</code> Schedule one-time tasks <code>at &lt;time&gt;</code> Mount a filesystem <code>mount &lt;device&gt; &lt;directory&gt;</code> Unmount a filesystem <code>umount &lt;device&gt;</code> Create symbolic link <code>ln -s &lt;target&gt; &lt;link&gt;</code> Change root directory <code>chroot &lt;directory&gt;</code> Generate checksum <code>sha256sum &lt;file&gt;</code>"},{"location":"linux/#other-commands","title":"Other commands","text":"<ul> <li> <p>Clean up the packages:</p> <p><code>rm -rf /var/lib/apt/lists/*</code> - This command used to clean up the package list cache after installing packages, especially in Docker, to reduce image size.</p> <p>Advantages: Reduces image size and keeps environments clean.</p> <p>Disadvantages: Deletes package metadata, requiring you to run apt-get update again before installing or upgrading packages. Not suitable for long-running environments where package management might be needed later.</p> </li> <li> <p>dev/null</p> <p>In Linux, /dev/null is a special device file known as the \"null device\" or \"null file.\" It discards anything written to it and immediately returns an end-of-file (EOF) to any process that reads from it. You can think of /dev/null as a \"black hole\" for data\u2014it simply deletes anything sent to it. This can be useful in several scenarios, especially when you want to suppress output/error or ignore specific data streams.</p> <p>Common Uses of /dev/null:</p> <ol> <li> <p>Suppress Command Output.     You may want to run a command without displaying any output. By redirecting the output to /dev/null, you effectively ignore it.</p> <p>Example: Suppresses standard output</p> <pre><code>ls &gt; /dev/null\n</code></pre> <p>In this example, ls would normally list directory contents, but by redirecting &gt; it to /dev/null, the output is discarded and not shown in the terminal.</p> </li> <li> <p>Suppress Error Messages.  </p> <p>Sometimes, you only want to discard error messages, which are typically directed to the standard error (stderr) stream (file descriptor 2).</p> <p>Example: Suppresses only the error output  </p> <pre><code>ls nonexistentfile 2&gt; /dev/null\n</code></pre> <p>This command tries to list a file that doesn\u2019t exist. Normally, it would produce an error message, but by redirecting 2&gt; to /dev/null, you discard only the error output and keep the standard output unaffected.</p> </li> <li> <p>Suppress All Output (Standard and Error)</p> <p>In some cases, you may want to suppress both standard output and error messages from a command.</p> <p>Example: Suppresses both standard output and error output  </p> <pre><code>ls nonexistentfile &gt; /dev/null 2&gt;&amp;1\n</code></pre> <p>Here, &gt; redirects standard output, and 2&gt;&amp;1 redirects standard error to the same place as standard output, which is now /dev/null. This effectively hides all output from ls, including any error messages.</p> </li> <li> <p>Run Command in Background Without Output</p> <p>When running a command in the background, the output can clutter the terminal. Sending output to /dev/null avoids this.</p> <p>Example: Runs <code>ping</code> in the background without showing output</p> <pre><code>ping -c 4 google.com &gt; /dev/null 2&gt;&amp;1 &amp;\n</code></pre> <p>This runs ping in the background for 4 packets, discarding all output, so you won\u2019t see any results or messages in the terminal.</p> </li> <li> <p>Check if a File or Directory Exists Without Output</p> <p>Sometimes, you only need to check if a file or directory exists without producing any output. /dev/null helps here by discarding the output of the command.</p> <p>Example:</p> <pre><code># Checks if a file exists without output\nif ls /path/to/file &gt; /dev/null 2&gt;&amp;1; then\n    echo \"File exists.\"\nelse\n    echo \"File does not exist.\"\nfi\n</code></pre> <p>In this example, ls checks if the file exists. If it does, the script echoes \"File exists.\" If not, it says \"File does not exist.\" The command's output is sent to /dev/null, so you don\u2019t see anything unless there is a specific message in the if or else statement.</p> <p>Summary Suppress command output: command &gt; /dev/null Suppress error messages: command 2&gt; /dev/null Suppress both output and error: command &gt; /dev/null 2&gt;&amp;1 Run in background without output: command &gt; /dev/null 2&gt;&amp;1 &amp;</p> </li> </ol> </li> </ul>"},{"location":"networking/","title":"Computer Networking","text":""},{"location":"networking/#protocol","title":"Protocol","text":"<p>Protocol refers to a set of rules and conventions that define how data is transmitted, received, and processed between devices and systems connected to the network. These rules ensure that devices can understand and communicate with each other effectively, enabling the seamless exchange of information across the internet.</p>"},{"location":"networking/#tcp-transmission-control-protocol","title":"TCP (Transmission Control Protocol)","text":"<p>Imagine sending a letter through the postal service. TCP is like a reliable, organized courier service that guarantees the letter's delivery. TCP ensures that data is transmitted in a precise, ordered manner, and it makes sure that the data arrives without errors. If a packet of data is lost or damaged during transmission, TCP requests the missing data to be sent again until everything is received correctly. It is commonly used for applications that require accurate data delivery, like web pages, emails, and file downloads.</p>"},{"location":"networking/#http-hypertext-transfer-protocol","title":"HTTP (Hypertext Transfer Protocol)","text":"<p>HTTP is like a set of rules that web browsers and web servers follow to communicate with each other. It's the language they use to exchange web pages and other resources. When you type a website address (URL) into your browser and hit Enter, your browser sends an HTTP request to the web server asking for the webpage. The server responds with the requested webpage, and your browser displays it for you to see. HTTP is the foundation of the World Wide Web, allowing us to access websites and navigate the internet.</p>"},{"location":"networking/#udp-user-datagram-protocol","title":"UDP (User Datagram Protocol)","text":"<p>UDP is like a fast, simple, but less reliable delivery service. It's useful for scenarios where speed is more important than guaranteed delivery. Unlike TCP, UDP doesn't ensure that data arrives in order or without errors. It just sends the data as quickly as possible. If some data packets are lost during transmission, UDP does not request retransmission. It is commonly used for real-time applications like video streaming, online gaming, and VoIP (Voice over Internet Protocol), where slight delays are acceptable and retransmission might cause more issues than it solves.</p>"},{"location":"networking/#router","title":"Router","text":"<p>Every Router/Modem has Global IP address, this IP address is shared to each device(computer/mobile) that connected to Router/Modem. for eg: If you run multiple applications as google, whatsapp and send request to internet, the device configure to send back response to particular Application using Port number. In simple IP address is to figure out which device/router you are using and Port number is to find from which Application response need to send.  </p>"},{"location":"networking/#port-number","title":"Port Number","text":"<p>Well-Known Ports (0-1023): Port numbers from 0 to 1023 are reserved for well-known services. Many of these ports are standardized for specific applications and protocols. For example, port 80 is commonly used for HTTP, port 443 for HTTPS, and port 22 for SSH.</p> <p>Registered Ports (1024-49151): Port numbers from 1024 to 49151 are assigned to registered services. These ports are used for various applications and services, and some have been officially registered with the Internet Assigned Numbers Authority (IANA). for eg: SQL = 1433</p> <p>MongoDB = 27017</p> <p>Dynamic/Private Ports (49152-65535): Port numbers from 49152 to 65535 are considered dynamic or private ports. They are used for temporary or private purposes, and they are less likely to be officially registered with IANA.</p>"},{"location":"networking/#how-to-check-open-ports-in-linux-and-window","title":"How to check open ports in linux and window","text":"<p>Linux</p> <pre><code>sudo netstat -tuln\n\nor\n\nsudo netstat -tulnp | grep :&lt;port_number&gt;\n\nsudo kill -9 &lt;PID&gt;  ## Delete local host running backend using netstat\n</code></pre> <p>Explanation:</p> <p>-t: Show TCP ports.</p> <p>-u: Show UDP ports.</p> <p>-l: Show only listening ports.</p> <p>-n: Show numerical addresses instead of resolving host names.</p> <p>Windows</p> <pre><code>netstat -ano | findstr &lt;port-number&gt;\n\nor\n\nnetstat -a -o | find \"9090\"\n\ntaskkill /F /PID 12345 ## Delete local host running backend using netstat\n</code></pre> <p>Explanation:</p> <p>-a: Displays all connections and listening ports.</p> <p>-n: Displays addresses and port numbers in numerical form.</p> <p>-o: Displays the process ID associated with each connection.</p>"},{"location":"networking/#internet-speed","title":"Internet speed","text":""},{"location":"networking/#osi-model","title":"OSI Model","text":"<p>The OSI (Open Systems Interconnection) model is a conceptual framework that standardizes the functions of a telecommunication or computing system into seven distinct layers. The model was developed by the International Organization for Standardization (ISO) to promote interoperability and facilitate communication between different systems and devices. Each layer in the OSI model represents a specific set of functions and services, and it helps in understanding and troubleshooting network communication.</p> <p>The seven layers of the OSI model, from the bottom to the top, are as follows:</p>"},{"location":"networking/#physical-layer-layer-1","title":"Physical Layer (Layer 1)","text":"<p>The physical layer is responsible for transmitting raw bits over a physical medium, such as cables or wireless signals. It defines the electrical, mechanical, and procedural aspects of data transmission, such as voltage levels, cable types, and physical connectors.</p>"},{"location":"networking/#data-link-layer-layer-2","title":"Data Link Layer (Layer 2)","text":"<p>The data link layer is responsible for reliable data transfer between directly connected devices on the same network segment. It provides error detection and correction, and it ensures that data frames are transmitted and received correctly.</p>"},{"location":"networking/#network-layer-layer-3","title":"Network Layer (Layer 3)","text":"<p>The network layer is responsible for routing data packets between different networks. It handles logical addressing, like IP addresses, and determines the best path for data to reach its destination.</p>"},{"location":"networking/#transport-layer-layer-4","title":"Transport Layer (Layer 4)","text":"<p>The transport layer provides end-to-end communication between devices on different networks. It ensures reliable data delivery, flow control, and error recovery using protocols like TCP (Transmission Control Protocol) and UDP (User Datagram Protocol).</p>"},{"location":"networking/#session-layer-layer-5","title":"Session Layer (Layer 5)","text":"<p>The session layer establishes, maintains, and terminates connections (sessions) between applications on different devices. It manages dialogues between applications, synchronization, and check pointing for data exchange.</p>"},{"location":"networking/#presentation-layer-layer-6","title":"Presentation Layer (Layer 6)","text":"<p>The presentation layer is responsible for data representation and encryption/decryption if needed. It translates data between the application format and the network format, ensuring that both systems can understand each other.</p>"},{"location":"networking/#application-layer-layer-7","title":"Application Layer (Layer 7)","text":"<p>The application layer is the topmost layer that directly interacts with user applications and services. It provides network services to end-users, such as email, file transfer, web browsing, and other network applications.</p>"},{"location":"networking/#tcpip","title":"TCP/IP","text":"<p>The TCP/IP model, also known as the Internet Protocol Suite, is a conceptual framework used to understand and implement network communication in the context of the internet. It was developed by the United States Department of Defense and is the foundation of the modern internet and many other computer networks. The TCP/IP model consists of four layers, each with specific functionalities:</p>"},{"location":"networking/#application-layer","title":"Application Layer","text":"<p>The Application Layer is the topmost layer and directly interacts with user applications and services. It provides network services to end-users and enables applications to communicate with the network. Protocols at this layer include HTTP (for web browsing), SMTP (for email), FTP (for file transfer), and DNS (for domain name resolution).</p>"},{"location":"networking/#transport-layer","title":"Transport Layer","text":"<p>The Transport Layer is responsible for end-to-end communication between devices on different networks. It ensures reliable data delivery, flow control, and error recovery. Two main protocols at this layer are TCP (Transmission Control Protocol) and UDP (User Datagram Protocol). TCP provides reliable and ordered data delivery, while UDP is used for faster but less reliable data transmission.</p>"},{"location":"networking/#internet-layer","title":"Internet Layer","text":"<p>The Internet Layer is responsible for routing data packets between different networks. It handles logical addressing, such as IP addresses, and determines the best path for data to reach its destination. The core protocol at this layer is IP (Internet Protocol), which provides the unique addressing scheme necessary for devices to communicate across the internet.</p>"},{"location":"networking/#link-layer","title":"Link Layer","text":"<p>The Link Layer, also known as the Network Interface Layer or Data Link Layer, is responsible for reliable data transfer between directly connected devices on the same network segment. It defines how data frames are transmitted and received over the physical medium, such as Ethernet or Wi-Fi. Various technologies and protocols, such as Ethernet, Wi-Fi (IEEE 802.11), and PPP (Point-to-Point Protocol), operate at this layer.</p>"},{"location":"networking/#osi-vs-tcpip","title":"OSI vs TCP/IP","text":""},{"location":"networking/#https-request","title":"HTTPS Request","text":""},{"location":"networking/#https-status-code","title":"HTTPS Status Code","text":""},{"location":"networking/#smtp-sender-mail-transfer-protocol","title":"SMTP: (Sender Mail Transfer Protocol)","text":""},{"location":"networking/#http-hyper-text-transfer-protocol","title":"HTTP: Hyper Text Transfer Protocol","text":""},{"location":"networking/#https","title":"HTTPS","text":""},{"location":"networking/#http-vs-https","title":"HTTP vs HTTPS","text":""},{"location":"networking/#dns-architecture","title":"DNS Architecture","text":""},{"location":"networking/#load-balancer-algorithm","title":"Load balancer Algorithm","text":""},{"location":"networking/#some-of-the-important-port-number","title":"Some of the Important Port Number","text":"<ol> <li>\ud835\udc03\ud835\udc28\ud835\udc1c\ud835\udc24\ud835\udc1e\ud835\udc2b \ud835\udc03\ud835\udc1a\ud835\udc1e\ud835\udc26\ud835\udc28\ud835\udc27: -Port: 2375 (unencrypted) and 2376 (encrypted)</li> <li>\ud835\udc09\ud835\udc1e\ud835\udc27\ud835\udc24\ud835\udc22\ud835\udc27\ud835\udc2c: -Runs on \ud835\udc07\ud835\udc13\ud835\udc13\ud835\udc0f \ud835\udc29\ud835\udc28\ud835\udc2b\ud835\udc2d 8080 \ud835\udc28r \ud835\udc07\ud835\udc13\ud835\udc13\ud835\udc0f\ud835\udc12 \ud835\udc29\ud835\udc28\ud835\udc2b\ud835\udc2d 8443</li> <li>\ud835\udc07\ud835\udc13\ud835\udc13\ud835\udc0f (\ud835\udc07\ud835\udc32\ud835\udc29\ud835\udc1e\ud835\udc2b\ud835\udc2d\ud835\udc1e\ud835\udc31\ud835\udc2d \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc2c\ud835\udc1f\ud835\udc1e\ud835\udc2b \ud835\udc0f\ud835\udc2b\ud835\udc28\ud835\udc2d\ud835\udc28\ud835\udc1c\ud835\udc28\ud835\udc25):- Default Port: 80</li> <li>\ud835\udc07\ud835\udc13\ud835\udc13\ud835\udc0f\ud835\udc12 (\ud835\udc07\ud835\udc32\ud835\udc29\ud835\udc1e\ud835\udc2b\ud835\udc2d\ud835\udc1e\ud835\udc31\ud835\udc2d \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc2c\ud835\udc1f\ud835\udc1e\ud835\udc2b \ud835\udc0f\ud835\udc2b\ud835\udc28\ud835\udc2d\ud835\udc28\ud835\udc1c\ud835\udc28\ud835\udc25 \ud835\udc12\ud835\udc1e\ud835\udc1c\ud835\udc2e\ud835\udc2b\ud835\udc1e): - Default Port: 443</li> <li>\ud835\udc12\ud835\udc12\ud835\udc07 (\ud835\udc12\ud835\udc1e\ud835\udc1c\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc12\ud835\udc21\ud835\udc1e\ud835\udc25\ud835\udc25): - Default Port: 22</li> <li>\ud835\udc05\ud835\udc13\ud835\udc0f (\ud835\udc05\ud835\udc22\ud835\udc25\ud835\udc1e \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc2c\ud835\udc1f\ud835\udc1e\ud835\udc2b \ud835\udc0f\ud835\udc2b\ud835\udc28\ud835\udc2d\ud835\udc28\ud835\udc1c\ud835\udc28\ud835\udc25): - Control Port: 21 - Data Port: 20</li> <li>\ud835\udc12\ud835\udc0c\ud835\udc13\ud835\udc0f (\ud835\udc12\ud835\udc22\ud835\udc26\ud835\udc29\ud835\udc25\ud835\udc1e \ud835\udc0c\ud835\udc1a\ud835\udc22\ud835\udc25 \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc27\ud835\udc2c\ud835\udc1f\ud835\udc1e\ud835\udc2b \ud835\udc0f\ud835\udc2b\ud835\udc28\ud835\udc2d\ud835\udc28\ud835\udc1c\ud835\udc28\ud835\udc25): - Port: 25</li> <li>\ud835\udc03\ud835\udc0d\ud835\udc12 (\ud835\udc03\ud835\udc28\ud835\udc26\ud835\udc1a\ud835\udc22\ud835\udc27 \ud835\udc0d\ud835\udc1a\ud835\udc26\ud835\udc1e \ud835\udc12\ud835\udc32\ud835\udc2c\ud835\udc2d\ud835\udc1e\ud835\udc26): - Port: 53</li> <li>\ud835\udc11\ud835\udc03\ud835\udc0f (\ud835\udc11\ud835\udc1e\ud835\udc26\ud835\udc28\ud835\udc2d\ud835\udc1e \ud835\udc03\ud835\udc1e\ud835\udc2c\ud835\udc24\ud835\udc2d\ud835\udc28\ud835\udc29 \ud835\udc0f\ud835\udc2b\ud835\udc28\ud835\udc2d\ud835\udc28\ud835\udc1c\ud835\udc28\ud835\udc25): - Port: 3389</li> <li>\ud835\udc0c\ud835\udc32\ud835\udc12\ud835\udc10\ud835\udc0b \ud835\udc03\ud835\udc1a\ud835\udc2d\ud835\udc1a\ud835\udc1b\ud835\udc1a\ud835\udc2c\ud835\udc1e: - Port: 3306</li> <li>\ud835\udc0f\ud835\udc28\ud835\udc2c\ud835\udc2d\ud835\udc20\ud835\udc2b\ud835\udc1e\ud835\udc12\ud835\udc10\ud835\udc0b \ud835\udc03\ud835\udc1a\ud835\udc2d\ud835\udc1a\ud835\udc1b\ud835\udc1a\ud835\udc2c\ud835\udc1e: - Port: 5432</li> <li>\ud835\udc0c\ud835\udc28\ud835\udc27\ud835\udc20\ud835\udc28\ud835\udc03\ud835\udc01: - Port: 27017</li> <li>\ud835\udc0a\ud835\udc2e\ud835\udc1b\ud835\udc1e\ud835\udc2b\ud835\udc27\ud835\udc1e\ud835\udc2d\ud835\udc1e\ud835\udc2c \ud835\udc00\ud835\udc0f\ud835\udc08 \ud835\udc12\ud835\udc1e\ud835\udc2b\ud835\udc2f\ud835\udc1e\ud835\udc2b: - Port: 6443</li> <li>\ud835\udc0d\ud835\udc20\ud835\udc22\ud835\udc27\ud835\udc31: - Default HTTP Port: 80 - Default HTTPS Port: 443</li> <li>\ud835\udc04\ud835\udc25\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc22\ud835\udc1c\ud835\udc2c\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc1c\ud835\udc21: - Port: 9200</li> <li>\ud835\udc00\ud835\udc29\ud835\udc1a\ud835\udc1c\ud835\udc21\ud835\udc1e \ud835\udc13\ud835\udc28\ud835\udc26\ud835\udc1c\ud835\udc1a\ud835\udc2d: - Default HTTP Port: 8080 - Default HTTPS Port: 8443</li> <li>\ud835\udc0f\ud835\udc2b\ud835\udc28\ud835\udc26\ud835\udc1e\ud835\udc2d\ud835\udc21\ud835\udc1e\ud835\udc2e\ud835\udc2c: - Port: 9090</li> <li>\ud835\udc06\ud835\udc2b\ud835\udc1a\ud835\udc1f\ud835\udc1a\ud835\udc27\ud835\udc1a: - Default HTTP Port: 3000</li> <li>\ud835\udc06\ud835\udc22\ud835\udc2d: - SSH Port: 22 - Git Protocol Port: 9418</li> </ol>"},{"location":"networking/#api","title":"API","text":"<p>Api Architecture Styles</p> <p></p> <p></p>"},{"location":"networking/#proxy","title":"Proxy","text":"<p>A proxy server acts as an intermediary between a client and the internet.</p>"},{"location":"networking/#how-it-works","title":"How it works","text":"<ol> <li> <p>Client Request: When a client (like a web browser) wants to access a resource (like a webpage), it sends a request to the proxy server instead of directly contacting the target server.</p> </li> <li> <p>Forwarding the Request: The proxy server receives the request and forwards it to the target server on behalf of the client.</p> </li> <li> <p>Response Handling: The target server processes the request and sends the response back to the proxy server.</p> </li> <li> <p>Returning the Response: The proxy server receives the response and then sends it back to the client.</p> </li> </ol> <p>Benefits of Using a Proxy</p> <ul> <li>Anonymity: Hides the client's IP address from the target server.</li> <li>Caching: Stores copies of frequently accessed resources to speed up future requests.</li> <li>Access Control: Filters and controls user access to certain websites or content.</li> <li>Security: Can provide additional security measures, such as encryption or threat detection.</li> </ul> <p>Types of Proxies</p> <ul> <li> <p>Forward Proxies: Handle requests from clients to external servers.</p> </li> <li> <p>Reverse Proxies: Handle requests from clients to a server, often used for load balancing and caching.</p> </li> <li> <p>Transparent Proxies: Intercept communication without modifying requests or responses.</p> </li> <li> <p>Anonymous Proxies: Hide the client's IP address but may identify themselves as a proxy.</p> </li> </ul> <p>By acting as a go-between, proxies enhance security, performance, and control over internet traffic.</p> <p>In the realm of networking, proxies play a crucial role in enhancing security, performance, and content delivery. Let\u2019s understand the key distinctions between the two types of proxies: forward and reverse.</p>"},{"location":"networking/#forward-proxy","title":"Forward proxy","text":"<p>Forward proxy is a client-side proxy that acts on behalf of clients. In this proxy, the client makes a request to the forward proxy to connect with the servers. After that, the forward proxy makes a request to the servers to get the response and send it back to the client.</p> <p>Forward proxy protects the client's identity by not letting the servers know about clients. In simple words, servers think that the forward proxy makes all the requests, while it can be coming from multiple clients.</p> <p>It is used for client anonymity, traffic control, encryption, caching, etc.</p>"},{"location":"networking/#reverse-proxy","title":"Reverse proxy","text":"<p>REverse proxy is a server-side proxy that sits in front of servers. In this, the client makes a request to the reverse proxy. Then the reverse proxy makes a request to the servers and returns the response to the client.</p> <p>Reverse proxy protects the server\u2019s identity by not letting the clients know about servers. In simple words, clients think that the reverse proxy serves all the requests, while behind there can be multiple servers.</p> <p></p> <p>It is used for server anonymity, load balancing, DDoS protection, etc.</p>"},{"location":"networking/#some-of-the-best-documentation-links","title":"Some of the Best Documentation Links","text":"<ul> <li>GeeksForGeeks </li> <li>AWS-Networking</li> </ul>"},{"location":"networking/#troubleshoot-networking","title":"Troubleshoot Networking","text":"<p>This guide provides common network troubleshooting commands to help diagnose and resolve network issues.</p> <ol> <li> <p>Ipconfig</p> <p>The <code>ipconfig</code> command is used to display the IP configuration of your network interfaces. To get detailed information about all interfaces, use the following:</p> <pre><code>ipconfig /all   #for windows\nifconfig        #for linux\n</code></pre> <p>This command will display the IP address, subnet mask, default gateway, DNS servers, and other important network-related information for all network adapters on your system.</p> </li> <li> <p>NSLOOKUP (Forward DNS Lookup)</p> <p>NSLOOKUP is used to query the DNS (Domain Name System) to resolve a domain name to an IP address. For a forward DNS lookup, you can use the following command:</p> <pre><code>NSLOOKUP google.com\n</code></pre> <p>This command will resolve google.com to its corresponding IP address.</p> </li> <li> <p>NSLOOKUP (Reverse DNS Lookup)</p> <p>To perform a reverse DNS lookup, you can use an IP address and query the DNS to get the associated domain name. Here\u2019s the command for a reverse DNS lookup:</p> <pre><code>NSLOOKUP 142.152.162.172\n</code></pre> <p>This will resolve the given IP address to its associated domain name.</p> </li> <li> <p>Ping</p> <p>The ping command is used to check if a network device (such as a router or server) is reachable. It sends a small data packet to the target device and waits for a response. Use the following syntax to ping a device:</p> <pre><code>ping google.com\n</code></pre> <p>This will test the connection to Google\u2019s servers.</p> </li> <li> <p>Tracert (Traceroute)</p> <p>tracert (on Windows) or traceroute (on Linux/Mac) is used to trace the path packets take to reach a destination. It shows each hop along the route from your computer to the target address.</p> <p>Example command:</p> <pre><code>tracert google.com\n</code></pre> <p>This command shows the route taken by packets from your computer to Google's servers, including the time it takes to reach each hop.</p> </li> <li> <p>Netstat</p> <p>The netstat command is used to display network statistics, including active connections, listening ports, and protocol usage. To view all network connections and statistics, run:</p> <pre><code>netstat -a\n</code></pre> <p>You can use various options with netstat to filter and display specific network information.</p> </li> </ol>"},{"location":"openshift/","title":"Openshift","text":""},{"location":"openshift/#installation","title":"installation","text":""},{"location":"openshift/#self-managed","title":"self-managed","text":"<p>To install openshift on VM it should have OS (RHEL, RHCOS).</p>"},{"location":"openshift/#service-managed","title":"service-managed","text":"<ol> <li>AWS (ROSA)</li> <li>AZURE (ARO : Azure Redhat Openshift)</li> </ol> <p>HA (High Availability) To install openshift for HA (High Availability), we need to have atleast 3 control plane, each control plane must contain minimum (32 CPU, 32 GB RAM).</p> <p>SNO (Single Node Openshift)</p> <p>This installation is cost efffective, but this can be used when Org. have 70 users it needs (64 GB Ram, 16-32 CPU)</p>"},{"location":"openshift/#features","title":"Features","text":"<p>Openshift offers Advance feature as:</p> <ol> <li>Operators</li> <li>GITOPS</li> <li>Networking (CRO, SDN)</li> <li>CI\\CD</li> <li>Observisibilty</li> <li>User Management (SSO)</li> <li>User Interface</li> </ol>"},{"location":"openshift/#openshift-route","title":"Openshift route","text":"<p>Route has rich tls configuration</p> <p>Types of TLS Termination</p> <ol> <li> <p>Edge Termination:</p> <p>In Edge termination, TLS is terminated at the OpenShift router.</p> <p>The traffic between the client and the OpenShift router is encrypted, but traffic from the OpenShift router to the pod (your service) is unencrypted.</p> <p>This is useful when you want to offload the TLS termination to the OpenShift infrastructure (router).</p> </li> <li> <p>Passthrough Termination:</p> <p>With Passthrough termination, the router does not terminate TLS. Instead, it passes encrypted  TLS traffic directly to the backend pod.</p> <p>The pod itself is responsible for handling TLS termination and decryption.</p> <p>This is useful if you want your application to fully control encryption.</p> </li> <li> <p>Re-encrypt Termination:</p> <p>In Re-encrypt termination, TLS is terminated at the OpenShift router, and a new encrypted connection is created between the router and the backend pod.</p> <p>This ensures that data remains encrypted between the client, router, and pod.</p> </li> </ol>"},{"location":"packer/","title":"Packer","text":"<ul> <li>Packer</li> <li>What is Packer?</li> <li>Stages of Packer</li> <li>Usage of Packer<ul> <li>Mutable and Immutable stage</li> <li>Mutable</li> <li>Immutable</li> </ul> </li> </ul>"},{"location":"packer/#what-is-packer","title":"What is Packer?","text":"<p>Packer is a tools which help to create customize Image from multiple platform from a single source configuration.</p> <p></p>"},{"location":"packer/#stages-of-packer","title":"Stages of Packer","text":""},{"location":"packer/#usage-of-packer","title":"Usage of Packer","text":""},{"location":"packer/#mutable-and-immutable-stage","title":"Mutable and Immutable stage","text":"<p>WHY to use PACKER.????</p> <ul> <li> <p>Well there are to stages of create Images &gt; Mutable and Immutable</p> </li> <li> <p>Mutable means changing Continuosly.</p> </li> <li> <p>Immutable means needs to configure only one time.</p> </li> <li> <p>Mutable is old way to configure the Images.</p> </li> <li> <p>Where it needs to cofingure after deploying the application </p> </li> <li> <p>If any case, we want to deploy to multiple server, configure multiplt server individually may create new bugs.</p> </li> <li> <p>Where as Packer use Immutable, which is configure deploy deplying to server.</p> </li> <li> <p>Using single configure Image we can spin up multiple server.</p> </li> </ul>"},{"location":"packer/#mutable","title":"Mutable","text":"<p>DEPLOY &gt; SERVER &gt; CONFIGURE</p> <p></p> <p>Configuring after spinning up server, If any case we need to install dependency into that server we need to isntall it each individual server, which can lead to issues and Bugs.</p> <p></p>"},{"location":"packer/#immutable","title":"Immutable","text":"<p>DEPLOY &gt; CONFIGURE &gt; SERVER</p> <p></p> <p>In Immutable Deploying and Configuration is done before hosting to server</p> <p></p> <p>In Immutable using One Packer we can spin up multiple server</p> <p></p>"},{"location":"publish/","title":"To be known","text":""},{"location":"publish/#git","title":"GIT","text":""},{"location":"publish/#git-shallow","title":"Git Shallow","text":"<p>Git shallow allows to clone large size repositories with lesser time.</p> <p>A \"shallow clone\" refers to a clone of a repository that only contains a limited history of the repository's commits. When you perform a shallow clone, Git retrieves only a subset of the commits from the remote repository, truncating the history beyond a certain depth. This can be useful when you are only interested in the recent history of a project and don't need the entire commit history.</p> <p>Benefits of Using Shallow Clones</p> <ol> <li>Reduced Cloning Time: Shallow clones significantly decrease the time it takes to clone large repositories by limiting the data transferred.</li> <li>Lower Storage Requirements: By fetching only a subset of the commit history, shallow clones consume less disk space.</li> </ol> <pre><code>git clone --depth 1 &lt;repository-url&gt;\n# The --depth option is used to specify the depth of the clone, i.e., the number of most recent commits to fetch.\n</code></pre> <p>To convert a shallow clone to a full clone with the entire commit history, you can use command:  </p> <pre><code>git fetch --unshallow\n</code></pre>"},{"location":"publish/#git-lfs","title":"Git lfs","text":"<p>Git LFS (Large File Storage) is an extension to Git that deals with large files by replacing them with text pointers inside the Git repository, while storing the actual file content in an external storage system. This allows you to version control large binary files, such as audio, video, datasets, and other large assets, without causing significant bloat in your Git repository.</p>"},{"location":"publish/#git-fork","title":"Git Fork","text":"<p>Workflow of Forking a Repository</p> <p>Fork the Repository:</p> <p>On a platform like GitHub, you click the \"Fork\" button, which creates a personal copy of the original repository under your account.</p> <ol> <li> <p>Clone the Fork:</p> <p>Clone your forked repository to your local machine for development. The command might look like this:</p> <pre><code>git clone https://github.com/yourusername/forked-repo.git\n</code></pre> </li> <li> <p>Add Upstream Remote:</p> <p>You can add the original repository as a remote to keep track of changes:</p> <pre><code>git remote add upstream https://github.com/originaluser/original-repo.git\n</code></pre> </li> <li> <p>Work on the Fork:</p> <p>Make changes in your local copy, commit them, and push them to your remote fork on GitHub:</p> <pre><code>git add .\ngit commit -m \"Your commit message\"\ngit push origin branch_name\n</code></pre> <p>Submit a Pull Request:</p> </li> <li> <p>Once your changes are ready, you can submit a pull request from your forked repository to the original repository for the maintainers to review and potentially merge into the main project.</p> <p>Syncing with Upstream:</p> <p>If the original repository (upstream) gets updated, you can fetch those changes and merge them into your forked repository:</p> <pre><code>git fetch upstream\ngit merge upstream/main\n</code></pre> </li> </ol>"},{"location":"publish/#git-config-to-set-git-credentials","title":"Git config to set git credentials","text":"<p>Streamlining Git Authentication with Global and System Configurations</p> <p>Managing credentials securely and conveniently is essential for developers working with Git, especially in collaborative environments. Manually entering credentials every time you clone or interact with repositories can be frustrating and disrupt workflow and think if you want to automate git clone/fetch in runtime. Fortunately, Git provides configurations that allow you to set up credentials once so that they\u2019re automatically used for subsequent interactions. This guide will show you how to set up Git credentials using the <code>git config</code> command, making your development process smoother and more efficient.</p>"},{"location":"publish/#the-challenge-repeated-authentication-requests","title":"The Challenge: Repeated Authentication Requests","text":"<p>Without configured Git credentials, every time you attempt to clone, pull, or push to a repository, Git will prompt you to enter your username and password. This repetitive process can be:</p> <ul> <li>Time-consuming: Typing credentials every time you work with Git repositories can take up valuable time.</li> <li>Disruptive: Stopping to enter credentials disrupts flow, especially in high-iteration development cycles.</li> <li>Error-prone: Frequent manual entries increase the chance of typos and authentication errors, leading to potential lockouts or delays.</li> </ul> <p>For developers, automating this process can boost productivity and reduce friction in daily work.</p>"},{"location":"publish/#solution-configuring-git-with-persistent-credentials","title":"Solution: Configuring Git with Persistent Credentials","text":"<p>Git offers two configuration levels that can store credentials, so you don\u2019t need to re-enter them each time:</p> <ol> <li>Global Configuration: Applies settings for the current user across all repositories.</li> <li>System Configuration: Applies settings globally across all users on the system.</li> </ol> <p>Using these configuration options with credential storage enables you to securely store Git credentials in a file, which Git will reference for every operation requiring authentication.</p>"},{"location":"publish/#steps-to-set-up-persistent-git-credentials","title":"Steps to Set Up Persistent Git Credentials","text":"<p>Create a credentials file that will securely store your Git username and password.</p> <ol> <li>Open your terminal.</li> <li> <p>Enter the following command to create a credentials file in your home directory:</p> <pre><code>echo \"https://&lt;your-username&gt;:&lt;your-password&gt;@&lt;git-server-url&gt;\" &gt; ~/.git-credentials\n</code></pre> <p>Replace <code>&lt;your-username&gt;, &lt;your-password&gt;, and &lt;git-server-url&gt;</code> with your actual Git server details. Ensure the file path (~/.git-credentials) is correct; this will be used in later steps. You can set Custom path eg: /tmp/.git-credentials</p> </li> <li> <p>Configure Git to Use the Credentials File</p> <p>Next, use the git config command to tell Git where to find this credentials file.</p> <p>Using Global Configuration To set this up for the current user on all repositories:</p> <pre><code>git config --global credential.helper \"store --file ~/.git-credentials\"\n</code></pre> <p>This command saves the location of the credentials file in the Git configuration, so Git uses these credentials whenever it needs authentication.</p> <p>Using System Configuration To make this setting apply to all users on the system:</p> <pre><code>sudo git config --system credential.helper \"store --file /path/to/.git-credentials\"\n</code></pre> <p>Be sure to replace /path/to/.git-credentials with the path to your credentials file.</p> </li> <li> <p>Verifying the Setup and try to clone a repository that requires authentication:</p> <p>Use <code>git config --list</code> to verify git config.</p> <p>which should output based on level you set.</p> <pre><code>credential.helper=store --file=/path/to/.git-credentials\n</code></pre> <p>Now try to clone repository, this should clone without prompts for credentials.</p> <pre><code>git clone https://&lt;git-server-url&gt;/&lt;repository&gt;\n</code></pre> <p>If configured correctly, Git should not prompt for a username or password, as it will automatically pull these from the credentials file.</p> </li> </ol> <p>Benefits of Using Persistent Git Credentials</p> <ol> <li> <p>Improved Productivity</p> <p>With credentials saved, you can focus entirely on development without repetitive prompts for authentication. This is especially beneficial when working across multiple repositories or making frequent pushes and pulls.</p> </li> <li> <p>Enhanced Workflow Efficiency</p> <p>By reducing the need for authentication interruptions, you streamline the coding and testing workflow, ultimately leading to faster development cycles.</p> </li> <li> <p>Reduced Risk of Credential Errors</p> <p>Centralized credential storage minimizes the likelihood of authentication errors, lockouts, or typos during login attempts.</p> </li> <li> <p>Better Security Control</p> <p>Storing credentials in a designated file enables easier control over security, as you know where your credentials are stored and can manage their permissions effectively.</p> <p>Security Considerations. Protect the Credentials File: Ensure your credentials file has restricted permissions (e.g., chmod 600 ~/.git-credentials) to prevent unauthorized access. Consider Credential Manager Options: For additional security, consider using Git credential managers like git-credential-manager or native OS keychain integrations for encrypted storage.</p> </li> </ol>"},{"location":"publish/#docker","title":"Docker","text":""},{"location":"publish/#cmd-vs-entrypoint","title":"CMD Vs ENTRYPOINT","text":"<p>CMD and ENTRYPOINT are instructions used in Dockerfiles to define the command that will be run when a container is started. However, they serve different purposes and have distinct behaviors.</p>"},{"location":"publish/#cmd-instruction","title":"CMD Instruction","text":"<p>The CMD instruction in a Dockerfile sets the default command and/or parameters for the container. It provides defaults for an executing container, but these defaults can be overridden by specifying the command and parameters at runtime when the container is started.</p> <pre><code>FROM ubuntu:latest  \nCMD [\"echo\", \"Hello, World!\"]  \n</code></pre> <p>In this example, if no command is specified when running the container, it will execute the default echo \"Hello, World!\". However, a user can override the CMD instruction by specifying a different command when running the container:</p> <pre><code>docker run my-image echo Goodbye, World!\n</code></pre>"},{"location":"publish/#entrypoint-instruction","title":"ENTRYPOINT Instruction","text":"<p>The ENTRYPOINT instruction in a Dockerfile sets the main command to be run when the container starts. Unlike CMD, the ENTRYPOINT instruction does not allow for default parameters that can be overridden. Instead, any parameters specified at runtime are passed as arguments to the command defined in ENTRYPOINT.</p> <pre><code>FROM ubuntu:latest\nENTRYPOINT [\"echo\", \"Hello, World!\"]  \n</code></pre> <p>In this example, if no command is specified when running the container, it will execute the default echo \"Hello, World!\". However, if a user provides a command, it will be treated as arguments to the ENTRYPOINT:</p> <p><code>docker run my-image \"Goodbye, World!\"</code></p>"},{"location":"publish/#docker-squash","title":"Docker-Squash","text":"<p>In this article, detailed explain about Image Layering and how to use docker-squash command to reduce the size of Image.</p>"},{"location":"publish/#automated-nginx-reverse-proxy-for-docker","title":"Automated Nginx Reverse Proxy for Docker","text":""},{"location":"publish/#dockerizer","title":"Dockerizer","text":"<p>Dockerizer is the concept to use Docker in more efficient way, so that developer and reuse the code and additional install packages as per dependency.  </p> <p>Reason for using Dockerizer:</p> <ol> <li>In Dockerfile each RUN command create new layer (which can create huge Image size while pushing to Registry).</li> <li>Dockerizer contains Ansible &amp; Packer concept which then help to create Image more efficient way,  installing required dependency/packages and making code reusability.</li> </ol>"},{"location":"publish/#reduce-docker-image-size","title":"Reduce Docker Image Size","text":""},{"location":"publish/#kubernetes","title":"Kubernetes","text":""},{"location":"publish/#storage-class-and-persistent-volume","title":"Storage Class and Persistent Volume","text":""},{"location":"publish/#goodbye-etcd-hello-postgresql-running-kubernetes-with-an-sql-database","title":"Goodbye etcd, Hello PostgreSQL: Running Kubernetes with an SQL Database","text":"<p>Etcd is the brain of every Kubernetes cluster, the key-value storage keeping track of all the objects in a cluster. It's intertwined and tightly coupled with Kubernetes, and it might seem like an inseparable part of a cluster, or is it?</p> <p>In this article it is explained how we could replace etcd with PostgreSQL database, as well as why and when it might make sense to do so.</p>"},{"location":"publish/#migrate-data-from-one-pvc-to-other-this-can-done-within-same-namespace","title":"Migrate data from one PVC to other (this can done within same Namespace)","text":"<p>Well, for hosting any web applications or storing output files we need some kind of dataStorage. At some point our local storage wont be efficient approach when it comes to Prod Version &amp; for multiple user.</p> <p>Thanks to Cloud Service Companies, that provides the data-storage here there are many Storageclass available from where we can store and retrieve our data. Some of the StorageClass as AzureFile, AzureBlob, net-trident-nfs.</p> <p>Infact, backup and mounting data from one storage to Storage, has became a easy job todo.</p> <p>Here is the simple code snippet to create a StorageClass, PVC</p> <ol> <li> <p>Create StorageClass yaml</p> <pre><code>kind: StorageClass\napiVersion: storage.k8s.io/v1beta1\nmetadata:\n    name: storageclassname\nprovisioner: csi.trident.netapp.io\nparameters:\n    resourceGroup: resourcegrpname #if required\n    snapshots: 'false'             #if required\nreclaimPolicy: Retain\nvolumeBindingMode: Immediate\n</code></pre> </li> <li> <p>Create PVC pointing to storage class</p> <pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n    name: pvcname\nspecs:\n    accessModes:\n        - ReadWriteMany\n    volumeMode: Filesystem\n    resources:\n        requests:\n            storage: 10Gi\n    storageClassName: name-of-storageclass\n    volumeMode: Filesystem\n</code></pre> </li> <li> <p>To copy data to new StorageClass we need to create a new PVC pointing to newStorageClass, the above PVC yaml snippets can be helpful to create new PVC</p> </li> <li> <p>Once PVC gets create, nxt is to start copying PVC by deploying helper job. Here we need to mention two PVC name for copying data from src to destination in Job</p> </li> <li> <p>Helper Job yaml file for copying data ( in this Job we need to mention to folder path within the pod to copy files one PVC to other)</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n    name: job-name\nspec:\n    parallelism: 1\n    completions: 1\n    activeDeadlineSeconds: 1800\n    backoffLimit: 5\n    template:\n        metadata:\n        labels:\n            app: job-name\n        spec:\n            volumes:\n                - name: src\n                persistentVolumeClaim:\n                    claimName: srcPVC-name  #from where data need to copy\n                - name: dst\n                persistentVolumeClaim:\n                    claimName: dstPVC-name  #data to be copied\n            containers:\n            - name: copy-data\n            image: ubuntu/busybox\n            command: [\"/bin/bash\",\"-c\"]\n            args: [\"cp -R /src/* /dst/\"]  # using cp command\n            args: [\"rsync -a --progress --dry-run /mnt/src/ /mnt/dst/; echo 'will sleep now for 120 seconds. To stop process delete job..'; sleep 120; rsync -a --progress /mnt/src/ /mnt/dst/; echo 'rsync process fininshed'\"]        #using rsync \n            volumeMounts:\n                - name: src\n                mountPath: \"/mnt/src\"\n                - name: dst\n                mountPath: \"/mnt/dst\"\n            restartPolicy: OnFailure\n</code></pre> </li> <li> <p>To execute the job <code>oc apply -f job.yaml</code></p> </li> </ol>"},{"location":"publish/#fetch-secrets-from-az-keyvault-to-use-secrets-in-pods","title":"Fetch secrets from Az-KeyVault to use secrets in Pods","text":"<p>Secrets key plays a vital role when its comes to use credentials for authentication purpose and various implement.</p> <p>Here Azure-KeyVault is one of the best service for (Iaas,Saas,Paas) , using Az we can also store secret values.</p> <p>Azure Key Vault provider for Secrets Store CSI Driver allows you to get secret contents stored in an Azure Key Vault instance and use the Secrets Store CSI driver interface to mount them into Kubernetes pods.</p> <p>Following steps mentioned about fetching KV from Azure</p> <pre><code>##add helm repo for Secrets Store CSI Driver and the Azure Keyvault Provider\nhelm repo add csi-secrets-store-provider-azure https://azure.github.io/secrets-store-csi-driver-provider-azure/charts\n\n# deploy the secret-store-csi-driver and Azure KV Provider\nhelm install csi csi-secrets-store-provider-azure/csi-secrets-store-provider-azure\n\nNote: The helm charts hosted in Azure/secrets-store-csi-driver-provider-azure repo include the \nSecrets Store CSI Driver helm charts as a dependency. Running the above helm install command\nwill install both the Secrets Store CSI Driver and Azure Key Vault provider\n\n# ensure driver and provider pods are running\nkubectl get pods\n\n#if using service principal connection (Configure Service Principal to access Keyvault)\nAdd your service principal credentials as a Kubernetes secrets\n\noc create secret generic secrets-store-creds --from-literal clientid=&lt;service_principal_client_id&gt;\n\nor deploy by using yaml\n\nkind: Secret\napiVersion: v1\nmetadata:\n    name: service-principal\n    lables:\n        #update labels\ndata:\n    clientid:\n    clientsecret:\ntype: Opaque\n\n# secret class provider yaml\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n    name: azure-kv\nspec:\nprovider: azure\nparameters:\n    usePodIdentity: \"false\"               # [OPTIONAL] if not provided, will default to \"false\"\n    useVMManagedIdentity: \"false\"         # [OPTIONAL available for version &gt; 0.0.4] if not provided, will default to \"false\"\n    userAssignedIdentityID: \"client_id\"   # [OPTIONAL available for version &gt; 0.0.4] use the client id to specify which user assigned managed identity to use. If using a user assigned identity as the VM's managed identity, then specify the identity's client id. If empty, then defaults to use the system assigned identity on the VM\n    clientID: \"client_id\"                 # [OPTIONAL available for version &gt; 1.1.0] client id of the Azure AD Application or managed identity to use for workload identity\n    keyvaultName: \"kvname\"                # the name of the KeyVault\n    cloudName: \"\"                         # [OPTIONAL available for version &gt; 0.0.4] if not provided, azure environment will default to AzurePublicCloud\n    cloudEnvFileName: \"\"                  # [OPTIONAL available for version &gt; 0.0.7] use to define path to file for populating azure environment\n    objects:  |\n    array:\n        - |\n        objectName: secret1\n        objectAlias: SECRET_1           # [OPTIONAL available for version &gt; 0.0.4] object alias\n        objectType: secret              # object types: secret, key or cert. For Key Vault certificates, refer to https://azure.github.io/secrets-store-csi-driver-provider-azure/configurations/getting-certs-and-keys/ for the object type to use\n        objectVersion: \"\"               # [OPTIONAL] object versions, default to latest if empty\n        objectVersionHistory: 5         # [OPTIONAL] if greater than 1, the number of versions to sync starting at the specified version.\n        filePermission: 0755                # [OPTIONAL] permission for secret file being mounted into the pod, default is 0644 if not specified.\n        - |\n        objectName: key1\n        objectAlias: \"\"                 # If provided then it has to be referenced in [secretObjects].[objectName] to sync with Kubernetes secrets \n        objectType: key\n        objectVersion: \"\"\n    tenantID: \"tid\"                       # the tenant ID of the KeyVault\n\n# deploy pod\nkind: Pod\napiVersion: v1\nmetadata:\n    name: demo-pod\nspec:\n    containers:\n    - image: nginx\n        name: nginx\n    volumeMounts:\n    - name: secrets-store-inline\n    mountPath: \"/mnt/secrets-store\"\n    readOnly: true\n    volumes:\n        - name; secrets-store-inline\n        csi:\n            driver: secrets-store.csi.k8s.io\n            readOnly: true\n            volumeAttributes:\n                secretProvider: \"azure-kv\"\n            nodePublishSecretRef:              #only required if using service principal          \n                name: \"secret-yaml-metadata-name\" #only required if using service principal \n(note: didnt specified commands for pod, pod will completed as soon as hosted)\n</code></pre> <p>Deploy pod</p> <pre><code>kubectl apply -f pod.yaml\n\n# check pod status\nkubectl get pods\n\n# show secrets held in secrets-store\nkubectl exec \"pod-name\" -- ls /mnt/secrets-store/\n\n# print a test secret held in secrets-store\nkubectl exec \"pod-name\" -- cat /mnt/secrets-store/secret1\n</code></pre>"},{"location":"publish/#volume-snapshot","title":"Volume Snapshot","text":"<p>Volume snapshot using Openshift, this feature assist to create a snap of volume in given time, so that those volume can utilised for test/prod by specifying the volumesnapshot name</p> <p>Following are steps to create Snapshot using Openshift</p> <ol> <li> <p>select PVC for which you want to take snapshot, click on the options and select \"create Snapshot\"</p> <p></p> </li> <li> <p>Select the \"VolumeSnapshotClass\" (will be provided by Openshift)</p> <p>note : for creating volumeSnapshot, a VolumeSnapshotClass should be available.</p> <p></p> </li> <li> <p>Select \"Create\"     </p> </li> <li> <p>After successful creation of snapshots, verify it by selecting VolumeSnapshots      </p> </li> <li> <p>now to restore the volumeSnapshot, create a new PVC (provided in options).     </p> </li> <li> <p>select the StorageClass for PVC     </p> </li> <li> <p>after successful creation of PVC , verify it by selecting PersistentVolumeClaims     </p> </li> <li> <p>now to use volume create by VolumeSnapshot, update the PVC name to pod, which is created using VolumeSnapshot.    </p> </li> </ol>"},{"location":"publish/#deployment-rollout-strategies","title":"Deployment rollout strategies","text":"<p>tutorial link</p> <p>byte-byte go link</p> <p>source-code link</p>"},{"location":"publish/#hpa-vs-keda","title":"HPA vs KEDA","text":"<p>To share these hard-earned lessons with you. In this article, we're going to dissect HPA and KEDA, compare their strengths and weaknesses, and dive into real-world scenarios. My goal is to arm you with the knowledge to make informed decisions right from the get-go, so you know exactly when to use HPA and when to switch gears to KEDA.</p> <ul> <li> <p>What is HPA?</p> <p>HPA automatically adjusts the number of pod replicas in a deployment or replica set based on observed metrics like CPU or memory usage. You set a target\u2014like 70% CPU utilization\u2014and HPA does the rest, scaling the pods in or out to maintain that level. It's like putting your scaling operations on cruise control.</p> </li> <li> <p>Why Was HPA Devised?</p> <p>Back in the day, before the cloud-native era, scaling was often a manual and painful process. You'd have to provision new servers, configure them, and then deploy your application. This was time-consuming, error-prone, and not very agile.</p> <p>When Kubernetes came along, it revolutionized how we think about deploying and managing applications. But Kubernetes needed a way to handle automatic scaling to truly make the platform dynamic and responsive to the actual needs of running applications. That's where HPA comes in.</p> <p>Simplicity: HPA is designed to be simple and straightforward. You don't need a Ph.D. in distributed systems to set it up. Just specify the metric and the target, and you're good to go.</p> <p>Resource Efficiency: Before autoscaling, you'd often over-provision resources to handle potential spikes in traffic, which is wasteful. HPA allows you to use resources more efficiently by scaling based on actual needs.</p> <p>Operational Ease: With HPA, the operational burden is reduced. You don't have to wake up in the middle of the night to scale your application manually; HPA has got your back.</p> <p>Built-In Metrics: Initially, HPA was designed to work with basic metrics like CPU and memory, which are often good enough indicators for many types of workloads.</p> <p>So, in a nutshell, HPA was devised to make life easier for DevOps folks like us, allowing for more efficient use of resources and simplifying operational complexities. It's like the Swiss Army knife of Kubernetes scaling for straightforward use-cases. What do you think? Want to dive deeper into any aspect of HPA?</p> <p>So... When to Use HPA?  Predictable Workloads: If you're dealing with an application that has a fairly predictable pattern\u2014like a web app that gets more traffic during the day and less at night\u2014HPA is a solid choice. You can set it to scale based on CPU or memory usage, which are often good indicators of load for these types of apps.</p> <p>Simple Metrics: HPA is great when you're looking at straightforward metrics like CPU and memory. If you don't need to scale based on more complex or custom metrics, HPA is easier to set up and manage.</p> <p>Quick Setup: If you're in a situation where you need to get autoscaling up and running quickly, HPA is your friend. Being a native Kubernetes feature, it's well-documented and supported, making it easier to implement.</p> <p>Stateless Applications: HPA is particularly well-suited for stateless applications where each pod is interchangeable. This makes it easier to scale pods in and out without worrying about maintaining state.</p> <p>Built-In Kubernetes Support: Since HPA is a built-in feature, it comes with the advantage of native integration into the Kubernetes ecosystem, including monitoring and logging through tools like Prometheus and Grafana.</p> </li> <li> <p>What is KEDA?</p> <p>KEDA stands for Kubernetes Event-Driven Autoscaling. Unlike HPA, which is more about scaling based on system metrics like CPU and memory, KEDA is designed to scale your application based on events. These events could be anything from the length of a message queue to the number of unprocessed database records.</p> <p>KEDA works by deploying a custom metric server and custom resources in your Kubernetes cluster. It then integrates with various event sources like Kafka, RabbitMQ, Azure Event Hubs, and many more, allowing you to scale your application based on metrics from these systems.</p> </li> <li> <p>Why Was KEDA Devised?</p> <p>Event-Driven Architectures: Modern applications are increasingly adopting event-driven architectures, where services communicate asynchronously through events. Traditional autoscalers like HPA aren't designed to handle this kind of workload.</p> <p>Complex Metrics: While HPA is great for simple metrics, what if you need to scale based on the length of a Kafka topic or the number of messages in an Azure Queue? That's where KEDA comes in.</p> <p>Zero to N Scaling: One of the coolest features of KEDA is its ability to scale your application back to zero when there are no events to process. This can lead to significant cost savings.</p> <p>Extensibility: KEDA is designed to be extensible, allowing you to write your own scalers or use community-contributed ones. This makes it incredibly flexible and adaptable to various use-cases.</p> <p>Multi-Cloud and On-Premises: KEDA supports a wide range of event sources, making it suitable for both cloud and on-premises deployments.</p> <p>The Gap that KEDA Fills Over HPA</p> <p>While HPA is like your reliable sedan, KEDA is more like a tricked-out sports car with all the bells and whistles. It was devised to fill the gaps left by HPA, particularly for applications that are event-driven or that require scaling based on custom or external metrics.</p> <p>So, if you're dealing with complex, event-driven architectures, or if you need to scale based on metrics that HPA doesn't support out of the box, KEDA is your go-to. It's like the next evolution in Kubernetes autoscaling, designed for the complexities of modern, cloud-native applications.</p> </li> </ul>"},{"location":"publish/#real-world-scenarios","title":"Real-World Scenarios","text":""},{"location":"publish/#real-cases-for-using-hpa-over-keda","title":"Real Cases for Using HPA Over KEDA","text":"<ol> <li> <p>Basic Web Application</p> <p>Scenario: You're running a simple web application that serves static content and has predictable spikes in traffic, like during a marketing campaign.</p> <p>In this case, the scaling needs are straightforward and based on CPU or memory usage. HPA is easier to set up and manage for this kind of scenario. You don't need the event-driven capabilities that KEDA offers.</p> </li> <li> <p>Internal Business Application</p> <p>Scenario: You have an internal application used by employees for tasks like data entry, which sees higher usage during business hours and lower usage otherwise.</p> <p>Again, the load pattern is predictable and can be managed easily with simple metrics like CPU and memory. HPA's native integration with Kubernetes makes it a straightforward choice, without the need for the more complex setup that KEDA might require.</p> </li> <li> <p>Stateless Microservices</p> <p>Scenario: You're running a set of stateless microservices that handle tasks like authentication, logging, or caching. These services have a consistent load and don't rely on external events.</p> <p>These types of services often scale well based on system metrics, making HPA a good fit. Since they're stateless, scaling in and out is less complex, and HPA can handle it easily.</p> </li> <li> <p>Traditional RESTful API</p> <p>Scenario: You have a RESTful API that serves mobile or web clients. The API has a steady rate of requests but might experience occasional spikes.</p> <p>In this case, you can set up HPA to scale based on request rates or CPU usage, which are good indicators of load for this type of application. KEDA's event-driven scaling would be overkill for this scenario.</p> </li> <li> <p>Why Choose HPA in These Cases?</p> <p>Simplicity: HPA is easier to set up and manage for straightforward scaling needs. If you don't need to scale based on complex or custom metrics, HPA is the way to go.</p> <p>Native Support: Being a built-in Kubernetes feature, HPA has native support and a broad community, making it easier to find help or resources.</p> <p>Resource Efficiency: For applications with predictable workloads, HPA allows you to efficiently use your cluster resources without the need for more complex scaling logic.</p> <p>Operational Ease: HPA requires less ongoing maintenance and has fewer components to manage compared to KEDA, making it a good choice for smaller teams or simpler applications.</p> </li> </ol>"},{"location":"publish/#real-cases-for-using-keda-over-hpa","title":"Real Cases for Using KEDA Over HPA","text":"<ol> <li> <p>Event-Driven ML Inference</p> <p>Scenario: You have a machine learning application for real-time fraud detection. Transactions are events funneled into an AWS SQS queue.</p> <p>Why KEDA Over HPA: With KEDA, you can dynamically adjust the number of inference pods based on the SQS queue length, ensuring timely fraud detection. HPA's system metrics like CPU or memory wouldn't be as effective for this use-case.</p> </li> <li> <p>IoT Data Processing</p> <p>Scenario: Your IoT application collects sensor data that's sent to an Azure Event Hub for immediate processing.</p> <p>Why KEDA Over HPA: Here, KEDA's strength lies in its ability to adapt to the number of unprocessed messages in the Azure Event Hub, ensuring real-time data processing. Traditional HPA scaling based on CPU or memory wouldn't be as responsive to these event-driven requirements.</p> </li> <li> <p>Real-time Chat Application</p> <p>Scenario: You manage a chat application where messages are temporarily stored in a RabbitMQ queue before being delivered to users.</p> </li> <li> <p>Why KEDA Over HPA?</p> <p>KEDA excels in this scenario by dynamically adjusting resources based on the RabbitMQ queue length, ensuring prompt message delivery. This is a level of granularity that HPA, with its focus on system metrics, can't offer.</p> <p>Stream Processing with Kafka</p> <p>Scenario: Your application consumes messages from a Kafka topic, and the rate of incoming messages can fluctuate significantly.</p> <p>Why KEDA Over HPA: In this case, KEDA's ability to scale based on the Kafka topic length allows it to adapt to varying loads effectively. HPA, which isn't designed for such custom metrics, wouldn't be as agile.</p> </li> <li> <p>Why Choose KEDA in These Cases?</p> <p>Event-Driven Flexibility: KEDA is tailored for scenarios where system metrics aren't the best indicators for scaling, offering a more nuanced approach.</p> <p>Custom Metrics Support: Unlike HPA, KEDA can interpret a wide range of custom metrics, making it versatile for complex scaling needs.</p> <p>Resource Optimization: KEDA's ability to scale down to zero pods when idle can lead to significant cost savings.</p> <p>Adaptability: The platform's extensible design allows for custom scalers, making it adaptable to a wide range of use-cases.</p> </li> </ol>"},{"location":"publish/#conclusion","title":"Conclusion","text":"<p>So there you have it, folks! We've journeyed through the world of Kubernetes autoscaling, dissecting both HPA and KEDA to understand their strengths, limitations, and ideal use-cases. From my early days of being enamored with HPA's simplicity to discovering the event-driven magic of KEDA, it's been a ride full of lessons.</p> <p>If you're dealing with predictable workloads and need a quick, straightforward solution, HPA is your reliable workhorse. It's like your trusty old hammer; it might not have all the bells and whistles, but it gets the job done efficiently.</p> <p>On the flip side, if your application lives in the fast-paced realm of event-driven architectures or requires scaling based on custom metrics, KEDA is your Swiss Army knife. It's built for the complexities and nuances of modern, cloud-native applications.</p> <p>Remember, choosing between HPA and KEDA isn't about which is better overall, but which is better for your specific needs. So take stock of your application's requirements, your team's expertise, and your long-term scaling strategy before making the call.</p> <p>As you venture into your next Kubernetes project, I hope this guide serves as a useful roadmap for your autoscaling decisions. And hey, since you're all about diving deeper, maybe explore setting up these autoscaling strategies in a hands-on way. Trust me, there's no better teacher than experience.</p>"},{"location":"publish/#cicd-deployment-architecture","title":"CICD deployment architecture","text":""},{"location":"python/","title":"Python","text":"<p>All about python modules, syntax and function</p>"},{"location":"python/#virtual-environment","title":"Virtual Environment","text":"<pre><code># Create a virtual environment\npython -m venv myenv\n\n# Activate the virtual environment\nsource myenv/bin/activate  # On Unix or MacOS\n\nmyenv\\Scripts\\activate  # On Windows\n\n# Deactivate the virtual environment\ndeactivate\n</code></pre>"},{"location":"python/#package-management","title":"Package Management","text":"<pre><code># Install a package\npip install package_name\n\n# List installed packages\npip list\n\n# Create requirements.txt\npip freeze &gt; requirements.txt\n\n# Install packages from requirements.txt\npip install -r requirements.txt\n</code></pre>"},{"location":"python/#for-loop","title":"For-loop","text":"<pre><code># print even\nlist=[4,5,25,6,8,8,9,2,2,3,97,52,96,32,84,19]\nfor i in list:\n    if i%2 == 0:\n        print(i, end=\"\")\n\n# using while loop\ncandy=10\nwithdrawal=12\ni=1\nwhile i &lt;= withdrawal:\n    if i &gt; candy:\n        print(f\"{i} : out of stock\")\n        break\n    print(f\"{i} : withdrawal\")\n    i+=1\n\n# create list by taking input from user\n\nlist=[]\nenter_range=int(input(\"enter the range to create list of elements\"))\nfor i in range(enter_range):\n    enter_num=input(f\"enter element {i+1} :\")\n    list.append(enter_num)\nprint(list)\n</code></pre>"},{"location":"python/#args-and-kwargs","title":"Args and Kwargs","text":"<pre><code>##args\n\ndef sum(*args):\n    total = 0\n    for i in args:\n        total = total+i\n    print(total)\n\nsum(5,6,7,8,9)  ##we can pass n number of arguments\n\n###for key word arguments\n\ndef student(**data):\n    for key,value in data.items():\n        print(f\"{key} {value}\")\n\n##callable = student(key=value,key=value)\n\nstudent(name=\"John\",age=30)\nstudent(name=\"Ram\",age=26)\n</code></pre>"},{"location":"python/#dictionary","title":"Dictionary","text":"<pre><code>person = {\n    \"name\":\"ram\",\n    \"age\":25,\n    \"gender\":\"male\",\n    \"profession\":\"engineer\"\n}\n\n## find value with key\nprint(person.get(\"age\"))\n\n##print all dict\nprint(person)\n\n#or using for loop\nfor i,j in person.items():\n    print(f\"keys: {i} , values :{j}\")\n\n##print all keys\nprint(person.keys())\n\n##print all values\nprint(person.values())\n\n##items return list containing all values in tuple \nprint(person.items())\n\n\n##update dict\nperson.update({\"salary\":\"100000\"})\nprint(person)\n\nprint(person.get(\"amount\"))\n</code></pre>"},{"location":"python/#functional-programming","title":"Functional Programming","text":""},{"location":"python/#lambda","title":"Lambda","text":"<pre><code>syntax &gt; lambda arguments : expression \n</code></pre> <pre><code>##passing argument in lambda\n\nnumbers=[2,4,6,8,10]\n\ndef multiple(x):\n\n    for i in x:\n        i=i*2\n        print(i,end=\" \")\n\nmultiple(numbers)\n\n##passing argument in lambda using map\n\ndoubled_map = list(map(lambda x : x*2,nums))\nprint(doubled_map)\n</code></pre>"},{"location":"python/#filter","title":"Filter","text":"<p>The filter() function applies a given function (like a lambda function) to each element of an iterable and filters the elements for which the function returns True. It returns a filter object (which can be converted to a list).</p> <p>Purpose: To select elements that meet a condition (i.e., elements for which the filtering function returns True). Use Case: When you want to filter out elements that don't satisfy a condition or predicate.</p> <pre><code>##filter is similar to mapping\n\nnums_list = [1,2,3,4,5,6,7,8,9]\neven=[]\nfor x in nums_list:\n    if x % 2 == 0:\n        even.append(x)\n\nprint(even)\n\n#with filter\n\ndef even(x):\n    return x%2==0\n\neven_list= list(filter(even,nums_list))\nprint(even_list)\n</code></pre>"},{"location":"python/#mapping","title":"Mapping","text":"<p>The map() function applies a given function (like a lambda function) to every element of an iterable (e.g., list, tuple) and returns a map object (which can be converted to a list).</p> <p>Purpose: To transform or modify each element in the iterable. Use Case: When you want to apply a function to all elements, such as performing operations, transformations, or conversions on each element.</p> <pre><code>double_num = []\nnumbers = (5,6,7,8,9)\n\nfor num in numbers:\n    double_num.append(num * 2)\n\nprint(double_num)\n\n##using map\n\ndef double(num):\n    return num*2\n\ndouble_List = list(map(double,numbers))\nprint(double_List)\n</code></pre>"},{"location":"python/#list-comprehension","title":"List Comprehension","text":"<pre><code>syntax &gt; [return-value for-loop if-condition]\n</code></pre> <pre><code>## with out list comprehension \n## print number which are divisible by 3\n\nls=[]\nfor i in range(100):\n    if i%3==0:\n        ls.append(i)\nprint(ls)\n\n## using list comprehension\n\nls=[i for i in range(100) if i%3==0]\nprint(ls)\n</code></pre>"},{"location":"python/#dictionary-comprehension","title":"Dictionary Comprehension","text":"<pre><code>syntax &gt; {return-value for-loop if-condition}\n</code></pre> <pre><code>dict1={i:f\"item{i}\" for i in range(100)}\nprint(dict1)\n\nnums_List=[5,6,7,8]\n\n## list comprehension for maping\n\nmapping = list(x*2 for x in nums_List)\nprint(mapping)\n\n## list comprehension for filtering\n\nfiltering = list(x for x in nums_List if x%2==0)\nprint(filtering)\n</code></pre>"},{"location":"python/#iteration","title":"Iteration","text":"<p>Iteration just means looping through something \u2014 like a list, tuple, or string \u2014 one item at a time.</p> <p>In Python, objects you can loop through (like lists, strings, etc.) are called iterables.</p> <p>To actually get each item one by one, Python uses something called an iterator.</p> <p>Example: Iteration using a list</p> <pre><code>fruits = [\"apple\", \"banana\", \"cherry\"]\n\n# normal for loop (uses iteration internally)\nfor fruit in fruits:\n    print(fruit)\n</code></pre> <p>Output:</p> <pre><code>apple\nbanana\ncherry\n</code></pre> <p>How it works internally</p> <p>You can use iter() and next() to see what\u2019s happening under the hood:</p> <pre><code>fruits = [\"apple\", \"banana\", \"cherry\"]\niterator = iter(fruits)  # get an iterator object\n\nprint(next(iterator))  # apple\nprint(next(iterator))  # banana\nprint(next(iterator))  # cherry\n# print(next(iterator))  # would raise StopIteration error\n</code></pre> <p>Each time you call next(), Python gives the next item until there\u2019s nothing left.</p>"},{"location":"python/#generator","title":"Generator","text":"<p>A generator is a special kind of iterator that you create yourself using a function and the yield keyword.</p> <p>Generators don\u2019t store all values in memory \u2014 they generate one value at a time and remember where they left off.</p> <p>That means they\u2019re great for large datasets or infinite sequences.</p> <p>Example</p> <pre><code>def count_up_to(n):\n    count = 1\n    while count &lt;= n:\n        yield count     # yield pauses the function and returns a value\n        count += 1\n\n# create generator\nnumbers = count_up_to(3)\n\nprint(next(numbers))  # 1\nprint(next(numbers))  # 2\nprint(next(numbers))  # 3\n# print(next(numbers))  # StopIteration\n</code></pre> <p>Each yield gives one value, then pauses the function until the next call.</p> <p>Using a generator in a loop</p> <pre><code>for num in count_up_to(3):\n    print(num)\n</code></pre> <p>Output:</p> <pre><code>1\n2\n3\n</code></pre> <p>\ud83e\udde9 Small Real Example</p> <pre><code>def read_file(filename):\n    with open(filename) as f:\n        for line in f:\n            yield line.strip()  # one line at a time\n\n# create generator object once\nlines = read_file(\"test.txt\")\n\n# now read lines one by one\nprint(next(lines))\nprint(next(lines))\nprint(next(lines))\nprint(next(lines))\n\n## or\n\ndef read_file(filename):\n    with open(filename) as f:\n        for line in f:\n            yield line.strip()  # one line at a time\n\nfor line in read_file(\"big_log.txt\"):\n    print(line)\n</code></pre>"},{"location":"python/#decorator","title":"Decorator","text":"<p>Decorators are a powerful and elegant feature in Python that allows you to modify or extend the behavior of functions or methods without changing their actual code.</p> <pre><code>def first(func):    ##passing the func as an argument\n    def second():\n        print(\"execute the first line\")\n        func() ##call the funct\n        print(\"execute the second line\")\n    return second()\n\n@first   ###decorate\ndef middle():\n    print(\"execute the middle line\")\n\n# mid = first(middle)  ##another way to call decorate\nmiddle()\n</code></pre>"},{"location":"python/#manage-files","title":"Manage Files","text":"<p>Using open function (in this function you need to explictly close the function with close())</p> <pre><code>f = open(\"./read_write_demo.txt\",\"r\")  ##r=read\nprint(f.read())   ##read() to print the txt \nf.close() ##close\n</code></pre> <p>Using with open function</p> <pre><code>def download(url,name):\n    print(f\"downloading {name} from {url}\")\n    response=requests.get(url)\n    with open(f\"{name}.jpg\",\"wb\") as f:\n        f.write(response.content)\n    print(f\"download {name} finished\")\n</code></pre>"},{"location":"python/#specail-variables","title":"Specail Variables","text":"<p>if name == \"main\":</p> <p>File: example.py</p> <pre><code>def greet():\n    print(\"Hello from greet()!\")\n\nprint(\"Top-level code is running!\")\n\nif __name__ == \"__main__\":\n    print(\"This runs only when example.py is executed directly.\")\n    greet()\n</code></pre> <p>If you run directly: python example.py</p> <p>Output:</p> <pre><code>Top-level code is running!\nThis runs only when example.py is executed directly.\nHello from greet()!\n</code></pre> <p>If you import it in another file as &gt; import example</p> <p>Output:</p> <pre><code>Top-level code is running!\n</code></pre> <p>\ud83d\udc49 Notice that the part under if name == \"main\": did not run!</p> <p>new</p> <p>new is a special method for object creation in Python. It is called before init and is responsible for creating a new instance. Rarely overridden unless you need control over object creation (e.g., implementing singletons, immutable objects, or metaclasses).</p> <p>Basic example:</p> <pre><code>class MyClass:\n    def __new__(cls, *args, **kwargs):\n        print(\"Creating instance...\")\n        instance = super().__new__(cls)  # Actually create the instance\n        return instance\n\n    def __init__(self, value):\n        print(\"Initializing instance...\")\n        self.value = value\n\nobj = MyClass(42)\n</code></pre> <p>Output:</p> <pre><code>Creating instance...\nInitializing instance...\n</code></pre> <p>\u2705 Key points:</p> <p>new returns the new instance.</p> <p>init initializes the instance.</p> <p>new is more \"low-level\" than init.</p> <p>Concept Role</p> <p>name    Tells whether a module is run directly (main) or imported main    Special name for the top-level script new Responsible for creating a new instance of a class</p>"},{"location":"python/#logging","title":"Logging","text":"<pre><code>import logging\n\n##using getlogger to create seperate log files\nlogger=logging.getLogger(__name__)\nlogger.setLevel(logging.INFO) ##set logging level\n\n## formatter is to set logs format\nlog_format=logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n\n## fileHandler is user to create a file\nlog_file=logging.FileHandler(\"example.log\")\n\nlog_file.setFormatter(log_format)\nlogger.addHandler(log_file)\n\n## further code\ndef something():\n    logger.info(\"this is a something function\")\n</code></pre>"},{"location":"python/#deep-and-shallow-copy","title":"Deep and Shallow Copy","text":"<p>Deep copy</p> <p>In the case of deep copy, a copy of the object is copied into another object. It means that any changes made to a copy of the object do not reflect in the original object.</p> <p></p> <p>Shallow copy</p> <p>In Shallow copy, a reference of an object is copied into another object. It means that any changes made to a copy of an object do reflect in the original object</p> <p></p>"},{"location":"python/#public-protected-and-private-methods","title":"Public, Protected and Private Methods","text":"<p>Public, Private, and Protected are access modifiers that define how variables and methods of a class can be accessed.</p> <ul> <li>Public Access Level: No restrictions. Public members can be accessed from anywhere\u2014inside or outside the class.</li> </ul> <p>Convention: Any attribute or method without an underscore prefix is considered public.  </p> <pre><code>class MyClass:\n    def __init__(self):\n        self.public_var = \"I am public\"\n\nobj = MyClass()\nprint(obj.public_var)  # Accessible\n</code></pre> <ul> <li>Protected</li> </ul> <p>Access Level: Meant to be accessible only within the class and its subclasses. However, it can still be accessed outside the class (Python doesn\u2019t enforce strict access control).</p> <p>Convention: Single underscore <code>_</code> prefix is used to indicate a protected member. This is just a convention and not strictly enforced.</p> <pre><code>class MyClass:\n    def __init__(self):\n        self._protected_var = \"I am protected\"\n\nclass SubClass(MyClass):\n    def access_protected(self):\n        return self._protected_var\n\nobj = MyClass()\nprint(obj._protected_var)  # Accessible (but not recommended)\n\nsub_obj = SubClass()\nprint(sub_obj.access_protected())  # Proper way to access protected members\n</code></pre> <ul> <li>Private</li> </ul> <p>Access Level: Accessible only within the class. Private members are not directly accessible outside the class. Convention: Double underscore <code>__</code> prefix is used to make an attribute private.</p> <p>Name Mangling: Python \"mangles\" the name of private members to make them harder to access. This is done by renaming the member to _ClassName__attributeName.</p> <pre><code>class Example:\n    def __init__(self):\n        self.__private_attribute = \"I am private!\"\n\n    def __private_method(self):\n        return \"This is a private method!\"\n\n    def access_private(self):\n        # Accessing private members within the class\n        return self.__private_method()\n\nobj = Example()\n\n# Accessing private members directly will raise an AttributeError\n# print(obj.__private_attribute)  # Uncommenting this will raise an error\n# print(obj.__private_method())   # Uncommenting this will raise an error\n\n# Accessing private members indirectly via a public method\nprint(obj.access_private())  # Allowed\n</code></pre>"},{"location":"python/#class","title":"Class","text":""},{"location":"python/#basic-class-structure","title":"Basic class structure","text":"<p>init is a special method, also known as a constructor, that is automatically called when a new instance (object) of a class is created. It is used to initialize the object's attributes (i.e., variables) and set up any necessary state.</p> <pre><code>class Student:\n    ##self hold the value of instant obj\n\n    def __init__(self,name,age) -&gt; None:   ##here self represent to b1\n        self.name = name\n        self.age = age\n\n    def __str__(self) -&gt; str:   ##__str__() function controls what should be returned when the class object is representd as string.\n        return f\"{self.name}\"\n\n    def hello(self):  ## funct is called methods in class\n        print(\"heellooo\")\n\n    def get_name(self):\n        return self.name\n\n    def get_details(self):\n        print(\"name\",self.name)\n        print(\"age\",self.age)\n\n\n\n## for every class we need to define obj\n## here b1 var is obj for class Book\n\nb1 = Student(\"RAM\",26)  ##b1 here is obj\nb1.hello()   ##obj.method\nprint(b1.get_name())    ##obj.method\nb1.get_details()\n\n\nb2=Student(\"Krishna\",11)   ##use the same class by creating another obj\nb2.get_details()\n</code></pre>"},{"location":"python/#isinstance","title":"Isinstance","text":"<pre><code>x = isinstance(\"Hello\", (float, int, str, list, dict, tuple))\nprint(x)\n</code></pre> <pre><code>class Myclass:\n    name = \"John\"\n\nobj = Myclass()\nx = isinstance(obj, Myclass)\n</code></pre>"},{"location":"python/#dataclass","title":"Dataclass","text":"<pre><code>#without dataclass\n\nclass Person():\n\n    def __init__(self, first_name, last_name, age):\n        self.first_name = first_name\n        self.last_name = last_name\n        self.age = age\n\nperson=Person(\"Ram\",\"krishna\",26)\nprint(person.first_name)\n\n##with dataclass\n\nfrom dataclasses import dataclass   ##import dataclass\n\n@dataclass\nclass Book:\n\n    title : str    ###variables are define without using __init__ instance\n    author : str\n    price : float\n\nbook=Book(\"Peaceofmind\",\"Unknown\",50.50)\nprint(book.price)\n</code></pre>"},{"location":"python/#method-static","title":"Method-Static","text":"<p>Class method :A class method is a method which is bound to the class and not the object of the class.</p> <p>Static method : A static method is used when we want to create a function without using self as instance-(just to create a independent function)</p> <pre><code>from datetime import date\n\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    # a class method to create a\n    # Person object by birth year.\n    @classmethod\n    def fromBirthYear(cls, name, year):\n        return cls(name, date.today().year - year)\n\n    # a static method to check if a\n    # Person is adult or not.\n    @staticmethod\n    def isAdult(age):\n        return age &gt; 18\n\n    ##static method eg 2\n    ##this function is independent of the class ,created without using self as instance\n    @staticmethod\n    def thankyou(msg):\n        return msg\n\nperson1 = Person('ram', 21)\nperson2 = Person.fromBirthYear('ram', 1997)\n\nprint(person1.age)\nprint(person2.age)\n\n# print the result\nprint(Person.isAdult(22))\n\n# print thankyuu msg\nprint(Person.thankyou(\"thanks for looking up this file\"))\n</code></pre>"},{"location":"python/#inheritance","title":"Inheritance","text":"<pre><code>class Publisher:\n\n    def __init__(self,title,price) -&gt; None:\n        self.title=title\n        self.price=price\n\nclass Author(Publisher):   ##add class name to inherit\n\n    def __init__(self,title,price,pages,period) -&gt; None:\n        super().__init__(title,price)   ###add module super()to fetch the var for Publisher class\n        self.period=period\n        self.pages=pages\n\nclass Book1(Publisher):    ##add class name to inherit\n\n    def __init__(self,title,price,author) -&gt; None:\n        super().__init__(title,price)\n        self.author=author    ##adding variable rather then class\n\nclass Magazine1(Author):\n\n    def __init__(self,title,author,pages,period) -&gt; None:   ##using Author class to fetch the values\n        super().__init__(title,author,pages,period)\n\nclass Newspaper1(Author):\n\n    def __init__(self,title,price,pages,period) -&gt; None:\n        super().__init__(title,price,pages,period)\n\n\nb1=Book1(\"PeaceofMind\",\"Unknown\",100)\n\nm1=Magazine1(\"Vogue\",\"Kiran\",20,15)\n\nn1=Newspaper1(\"TOI\",\"toi\",5,10)\n\nprint(b1.author)\n\nprint(m1.period)\n</code></pre>"},{"location":"python/#polymorphism-using-method-overiding","title":"Polymorphism using Method-Overiding","text":"<p>Polymorphism in Python refers to the ability of different object types to be used interchangeably because they implement the same methods or behavior. It allows functions or methods to operate on objects of different types, as long as they support the same interface or method names.</p> <pre><code>class Animal:\n    def speak(self):\n        raise NotImplementedError(\"Subclasses must implement this method\")\n\nclass Dog(Animal):\n    def speak(self):\n        return \"Woof!\"\n\nclass Cat(Animal):\n    def speak(self):\n        return \"Meow!\"\n\n# Creating objects of the child classes\ndog = Dog()\ncat = Cat()\n\n# Both objects have a speak method, but they behave differently\nprint(dog.speak())  # Output: Woof!\nprint(cat.speak())  # Output: Meow!\n</code></pre> <p>In this example:</p> <p>Both Dog and Cat are subclasses of Animal, and they override the speak() method to provide their own implementation. The same method name (speak()) exhibits different behaviors depending on the object type.</p>"},{"location":"python/#async","title":"Async","text":"<p>In Python, async is used for asynchronous programming. It allows you to write code that can run tasks concurrently without blocking the execution of other code.</p> <p>Normally, Python executes code line by line, which can be slow if some operations take time (like downloading a file or waiting for a database).</p> <p>With async, you can start a task, let other tasks run while waiting, and come back to the first task when it\u2019s done.</p> <p>This is particularly useful for I/O-bound operations (network calls, file reading, API calls) but not for CPU-bound operations.</p> <p>Core concepts</p> <ul> <li>async def: Defines an asynchronous function (also called a coroutine).</li> <li>await: Waits for a coroutine to finish without blocking the event loop.</li> <li>asyncio: Python\u2019s built-in library to run async tasks concurrently.</li> </ul> <p>Example</p> <pre><code>import asyncio\n\n# Define an async function\nasync def say_hello():\n    print(\"Hello\")\n    await asyncio.sleep(2)  # Simulate a 2-second I/O operation\n    print(\"World!\")\n\n# Another async function\nasync def say_numbers():\n    for i in range(5):\n        print(i)\n        await asyncio.sleep(1)\n\n# Main coroutine\nasync def main():\n    # Run tasks concurrently\n    task1 = asyncio.create_task(say_hello())\n    task2 = asyncio.create_task(say_numbers())\n\n    # Wait for both tasks to finish\n    await task1\n    await task2\n\n# Run the main coroutine\nasyncio.run(main())\n</code></pre> <p>Output</p> <pre><code>Hello\n0\n1\n2\nWorld!\n3\n4\n</code></pre> <p>What happens here:</p> <ol> <li>say_hello() waits for 2 seconds without blocking other tasks.</li> <li>say_numbers() continues printing numbers every second.</li> </ol> <p>Async is not parallelism (no multiple CPU cores), it's concurrency (efficient waiting for I/O).</p>"},{"location":"python/#multithreading","title":"Multithreading","text":"<p>Multithreading means running multiple threads (smaller units of a process) at the same time within a single program. Each thread can perform a task \u2014 for example, downloading files, processing data, or handling network requests \u2014 concurrently.</p> <p>Why Use Multithreading?</p> <ol> <li>Multithreading helps when your program spends a lot of time waiting \u2014 such as:</li> <li>Downloading data from the internet</li> <li>Reading/writing files</li> <li>Waiting for user input</li> <li>Talking to a database or API</li> </ol> <p>Python threads share the same memory, so they can communicate easily.</p> <p>Important: Due to Python\u2019s GIL (Global Interpreter Lock), true parallel CPU computation is limited. But for I/O-bound tasks (like network or disk operations), multithreading gives a big speed boost.</p> <p>\ud83e\udde9 How to Use Multithreading (Simple Example)</p> <pre><code>### example 1\n\nimport threading\nimport time\n\ndef func(seconds):\n    print(f\"executing function {seconds}\")\n    time.sleep(seconds)\n\nt1 = threading.Thread(target=func,args=[4])\n\nt2 = threading.Thread(target=func,args=[6])\n\ntime1=time.perf_counter()\n\n## start multithreading\nt1.start()\nt2.start()\n\n## wait until background process complete\nt1.join()\nt2.join()\n\ntime2=time.perf_counter()\nprint(time2-time1)\n\n### example2\nimport threading\nimport time\n\ndef worker(task_id):\n    print(f\"Thread {task_id} starting...\")\n    time.sleep(2)  # Simulate some work\n    print(f\"Thread {task_id} finished!\")\n\n# Create a list to hold our threads\nthreads = []\n\n# Create and start 3 threads\nfor i in range(3):\n    t = threading.Thread(target=worker, args=(i,))\n    t.start()\n    threads.append(t)\n\n# Wait for all threads to finish\nfor t in threads:\n    t.join()\n\nprint(\"All threads are done!\")\n</code></pre> <p>Output:</p> <pre><code>Thread 0 starting...\nThread 1 starting...\nThread 2 starting...\nThread 0 finished!\nThread 1 finished!\nThread 2 finished!\nAll threads are done!\n</code></pre> <p>\u23f1 You\u2019ll notice that all threads start together, and the program only waits for all of them to complete at the end \u2014 instead of doing them one by one.</p> <p>When to Use?</p> <ul> <li>Downloading multiple files or web pages</li> <li>Handling many network connections</li> <li>Waiting for I/O operations</li> </ul>"},{"location":"python/#multiprocessing","title":"Multiprocessing","text":"<p>Multiprocessing allows Python programs to run multiple processes in parallel. This is especially useful for CPU-bound tasks (like complex calculations), where you need to fully utilize multiple CPU cores.</p> <p>Unlike multithreading, which shares memory, multiprocessing creates separate memory spaces for each process, allowing true parallelism.</p> <p>Why Use Multiprocessing?</p> <ul> <li>Parallelism: True parallelism \u2014 each process runs on a separate core.</li> <li>CPU-bound tasks: Great for tasks that are heavy on CPU (e.g., image processing, data analysis, etc.), as Python's Global Interpreter Lock (GIL) prevents multiple threads from running in parallel on multiple CPU cores.</li> <li>Better resource utilization: Use multiple cores to speed up your program when it\u2019s limited by CPU.</li> </ul> <p>Example code</p> <pre><code>import multiprocessing\nimport time\n\ndef square_number(n):\n    \"\"\"Function to calculate square of a number.\"\"\"\n    print(f\"Squaring {n}...\")\n    time.sleep(2)  # Simulate a time-consuming task (e.g., heavy calculation)\n    result = n * n\n    print(f\"The square of {n} is {result}\")\n\nif __name__ == \"__main__\":\n    # Create a list of numbers to square\n    numbers = [1, 2, 3, 4, 5]\n\n    # Create a list to hold processes\n    processes = []\n\n    # Start multiple processes\n    for number in numbers:\n        p = multiprocessing.Process(target=square_number, args=(number,))\n        p.start()  # Start process\n        processes.append(p)\n\n    # Wait for all processes to finish\n    for p in processes:\n        p.join()\n\n    print(\"All tasks finished.\")\n</code></pre>"},{"location":"python/#difference-between-multithreading-and-multiprocessing-in-python","title":"Difference Between Multithreading and Multiprocessing in Python","text":"<p>Comparison of Multithreading, Multiprocessing, and Asynchronous Programming in Python</p> <p>This document compares three common approaches to concurrency and parallelism in Python: Multithreading, Multiprocessing, and Asynchronous Programming (async/await).</p> Concept Multithreading Multiprocessing Asynchronous (async/await) Definition Runs multiple threads (small tasks) inside one process. Runs multiple processes, each with its own Python interpreter and memory. Runs tasks concurrently using a single thread and event loop without blocking. Execution model Threads share the same memory space. Each process has its own separate memory space. Single-threaded by default; uses an event loop to switch between tasks. Parallel execution Limited by the GIL \u2014 only one thread runs Python code at a time. True parallelism \u2014 each process runs on a separate CPU core. Concurrent execution for I/O-bound tasks, but not parallel on CPU. Best for I/O-bound tasks (like file or network operations). CPU-bound tasks (like calculations or data processing). I/O-bound tasks (like network requests, file operations, database queries). Global Interpreter Lock (GIL) Only one thread runs Python code at a time. Not affected by GIL \u2014 all processes run independently. Not affected by GIL since only one thread is used; tasks yield control with <code>await</code>. Memory Shared between threads. Separate memory for each process. Shared memory of a single thread; uses coroutines instead of separate threads. Data sharing Easy (since memory is shared). Needs special tools like Queues or Pipes to share data. Easy \u2014 tasks can share data within the same thread. Performance Great for I/O tasks. Great for CPU-heavy tasks. Great for high-latency I/O tasks; not for CPU-heavy tasks. Overhead Low. Higher (creates separate processes). Very low \u2014 lightweight coroutines managed by event loop. <p>\u2705 When to Use</p> <ul> <li>Use Multithreading \u2192 for I/O-bound work (e.g., downloading files, reading APIs).  </li> <li>Use Multiprocessing \u2192 for CPU-bound work (e.g., heavy computation, image processing).</li> </ul>"},{"location":"ref/","title":"Reference","text":"<ol> <li> <p>Deploy Reddit Clone Application on Kubernetes Using Jenkins CI/CD Pipeline</p> </li> <li> <p>Open Telemetry Doc</p> </li> <li> <p>I\u2019ve Been Using These 8 Core Linux Commands Wrong for Years</p> </li> <li> <p>How to fix StatefulSet with Persistent Volume not working after Cloud Migration</p> </li> <li> <p>Kubernetes RBAC</p> </li> <li> <p>Top 6 Most Popular API Architecture Styles</p> </li> <li> <p>Tasks.md - Self-Hosted Markdown Based Task Manager</p> </li> <li> <p>Ansible Jinja Template</p> </li> <li> <p>Terraform lock file</p> </li> <li> <p>How to Setup 2FA on an SSH Server</p> </li> <li> <p>Argocd deployment challenges</p> </li> <li> <p>hpa-vs-vpa-vs-keda</p> </li> <li> <p>Why Are k8s Secrets Base64 Encoded</p> </li> <li> <p>Python script for log file monitoring and alerting</p> </li> <li> <p>Docker logs</p> </li> <li> <p>Neon DB Pipeline</p> </li> <li> <p>Log analytics workspace</p> </li> <li> <p>How to Improve Performance Your Database</p> </li> </ol>"},{"location":"ref/#pages","title":"Pages","text":"<p>Devopscube</p> <p>ByteBytego</p> <p>Jeff Brown</p> <p>Coach DevOps</p> <p>Jhooq</p> <p>Bibin Wilson LinkedIn</p> <p>All About Database ScalableThread</p> <p>DataBase CaseStudy</p>"},{"location":"scenario_based/","title":"Scenario Based Question","text":""},{"location":"scenario_based/#terraform","title":"Terraform","text":""},{"location":"scenario_based/#you-have-50-terraform-resources-created-using-a-jenkins-pipeline-and-the-pipeline-takes-more-than-5-hours-to-complete-how-would-you-reduce-the-build-time","title":"You have 50 Terraform resources created using a Jenkins pipeline, and the pipeline takes more than 5 hours to complete. How would you reduce the build time?","text":"<p>Enable Parallelism in Terraform</p> <p>By default, Terraform applies resources sequentially. You can increase the number of parallel operations using the -parallelism flag.</p> <pre><code>terraform apply -parallelism=20\n</code></pre> <p>Default is 10. Increasing it allows Terraform to create independent resources concurrently.</p> <p>However, it must be tuned carefully \u2014 too high may overload the backend or API rate limits.</p>"},{"location":"scenario_based/#what-is-a-module","title":"What is a Module?","text":"<p>A module in Terraform is a container for multiple resources that are used together. It allows reusability, organization, and consistency across environments (dev, test, prod).</p> <p>Type                  | Description Root Module         |   The directory where you run terraform init, plan, or apply. It contains main Terraform configuration files like main.tf, variables.tf, and outputs.tf. Child Module          | A reusable module defined in a separate directory (or from a remote source) and called by the root module using the module block.</p> <p>Folder structure</p> <p><pre><code>terraform-project/\n\u2502\n\u251c\u2500\u2500 main.tf                # Root module\n\u251c\u2500\u2500 variables.tf\n\u251c\u2500\u2500 outputs.tf\n\u2502\n\u2514\u2500\u2500 modules/\n|   \u2514\u2500\u2500 ec2-instance/      # Child module\n|       \u251c\u2500\u2500 main.tf\n|       \u251c\u2500\u2500 variables.tf\n|       \u2514\u2500\u2500 outputs.tf\n\u2514\u2500\u2500 modules/\n    \u2514\u2500\u2500 vpc/      # Child module\n        \u251c\u2500\u2500 main.tf\n        \u251c\u2500\u2500 variables.tf\n        \u2514\u2500\u2500 outputs.tf\n</code></pre> Step 1 \u2014 Create a Child Module</p> <p>\ud83d\udcc1 modules/ec2-instance/main.tf</p> <pre><code>resource \"aws_instance\" \"example\" {\n  ami           = var.ami\n  instance_type = var.instance_type\n  tags = {\n    Name = var.instance_name\n  }\n}\n</code></pre> <p>\ud83d\udcc1 modules/ec2-instance/variables.tf</p> <pre><code>variable \"ami\" {\n  description = \"AMI ID for the EC2 instance\"\n  type        = string\n}\n\nvariable \"instance_type\" {\n  description = \"Type of EC2 instance\"\n  type        = string\n  default     = \"t2.micro\"\n}\n\nvariable \"instance_name\" {\n  description = \"Name tag for the EC2 instance\"\n  type        = string\n}\n</code></pre> <p>\ud83d\udcc1 modules/ec2-instance/outputs.tf</p> <pre><code>output \"instance_id\" {\n  description = \"The ID of the created EC2 instance\"\n  value       = aws_instance.example.id\n}\n</code></pre> <p>Step 2 \u2014 Use the Child Module in the Root Module</p> <p>\ud83d\udcc1 main.tf (Root Module)</p> <pre><code>provider \"aws\" {\n  region = var.region\n}\n\n# Using the child module\nmodule \"web_server\" {\n  source         = \"./modules/ec2-instance\"\n  ami            = var.ami\n  instance_type  = var.instance_type\n  instance_name  = \"web-server\"\n}\n\noutput \"web_server_id\" {\n  value = module.web_server.instance_id\n}\n</code></pre> <p>\ud83d\udcc1 variables.tf</p> <pre><code>variable \"region\" {\n  description = \"AWS region\"\n  type        = string\n  default     = \"us-east-1\"\n}\n\nvariable \"ami\" {\n  description = \"AMI ID for EC2 instance\"\n  type        = string\n  default     = \"ami-0c55b159cbfafe1f0\"  # Example Amazon Linux 2 AMI\n}\n\nvariable \"instance_type\" {\n  description = \"Type of EC2 instance\"\n  type        = string\n  default     = \"t3.micro\"\n}\n</code></pre> <p>\ud83d\udcc1 outputs.tf</p> <pre><code>output \"instance_id\" {\n  description = \"ID of the EC2 instance created via module\"\n  value       = module.web_server.instance_id\n}\n</code></pre> <p>Step 3: Run the Terraform Commands</p> <pre><code>terraform init      # Initialize and download providers &amp; modules\nterraform plan      # Show what will be created\nterraform apply\n</code></pre> <p>Use Remote Module (Example from GitHub)</p> <p>You can also source a module remotely:</p> <pre><code>module \"network\" {\n  source  = \"git::https://github.com/example-org/terraform-aws-vpc.git?ref=v1.0.0\"\n  cidr_block = \"10.0.0.0/16\"\n  environment = \"dev\"\n}\n</code></pre>"},{"location":"scenario_based/#your-terraform-state-file-terraformtfstate-got-corrupted-what-will-you-do","title":"Your Terraform state file (terraform.tfstate) got corrupted. What will you do?","text":"<ol> <li>Don\u2019t run terraform apply immediately \u2014 it can worsen the situation.</li> <li>Check if you have state file backups:     Local backend \u2192 .terraform/backup/     S3 backend \u2192 Versioning-enabled bucket.</li> <li>Restore the last known good version:</li> </ol> <pre><code>aws s3 cp s3://bucket/path/terraform.tfstate &lt;restore-location&gt;\n</code></pre> <ol> <li>If partial corruption \u2192 try manual fix by editing JSON carefully.</li> <li>If full recovery not possible \u2192 use terraform import to rebuild the state from real infrastructure.</li> </ol>"},{"location":"scenario_based/#someone-manually-changed-a-resource-in-the-cloud-outside-terraform-how-do-you-detect-and-fix-it","title":"Someone manually changed a resource in the cloud outside Terraform. How do you detect and fix it?","text":"<pre><code>## run\nterraform plan\n</code></pre> <p>Terraform will detect the drift and show differences.</p> <p>Revert the manual change by re-applying:</p> <pre><code>terraform apply\n</code></pre> <p>Or, if the manual change is correct, update the Terraform configuration and re-run plan.</p>"},{"location":"scenario_based/#two-team-members-applied-terraform-changes-to-the-same-module-at-the-same-time-one-of-the-applies-failed-how-can-you-prevent-this","title":"Two team members applied Terraform changes to the same module at the same time. One of the applies failed. How can you prevent this?","text":"<p>Use state locking in your backend. Example: AWS S3 + DynamoDB backend setup</p> <pre><code>backend \"s3\" {\n  bucket         = \"tf-state-bucket\"\n  key            = \"prod/terraform.tfstate\"\n  region         = \"us-east-1\"\n  dynamodb_table = \"terraform-locks\"\n}\n</code></pre> <p>The DynamoDB lock prevents simultaneous apply operations.</p> <p>Educate team to use:</p> <pre><code>terraform plan -out=tfplan\nterraform apply tfplan\n</code></pre> <p>to ensure reproducible state.</p> <p>The terraform apply tfplan command in Terraform is used to execute a previously generated execution plan. This command is crucial for applying infrastructure changes in a controlled and predictable manner, especially in automated pipelines or when a plan needs to be reviewed and approved before deployment.</p> <p>Here's how it works:</p> <ol> <li>Generate a plan: First, you create an execution plan using terraform plan -out tfplan, where tfplan is the name of the file where the plan will be saved. This command analyzes your Terraform configuration and the current state of your infrastructure to determine the actions (create, update, or destroy) required to reach the desired state.</li> <li>Review the plan: The saved tfplan file can be reviewed using terraform show tfplan to understand the exact changes Terraform intends to make. This step is critical for ensuring that the proposed changes align with your expectations and do not introduce unintended consequences.</li> <li>Apply the plan: Once the plan is reviewed and approved, you can execute it using terraform apply tfplan. Terraform will then perform the actions defined in the tfplan file, making the necessary changes to your infrastructure and updating the Terraform state file to reflect the new state of your resources.</li> </ol>"},{"location":"scenario_based/#during-terraform-apply-some-resources-were-created-successfully-while-others-failed-what-would-you-do-next","title":"During terraform apply, some resources were created successfully, while others failed. What would you do next?","text":"<p>First, don\u2019t destroy everything.</p> <p>Run: <pre><code>terraform apply\n</code></pre></p> <p>again \u2014 Terraform will detect already created resources and continue where it left off.</p> <p>If still failing:</p> <p>Use <code>terraform taint &lt;resource&gt;</code> to mark specific failed resources for recreation.</p> <p>Or use <code>terraform state rm</code> to remove manually created resources if needed.</p> <p>Always review the terraform plan output before reapplying.</p>"},{"location":"scenario_based/#you-already-have-an-aws-ec2-instance-created-manually-how-can-you-bring-it-under-terraform-management","title":"You already have an AWS EC2 instance created manually. How can you bring it under Terraform management?","text":"<p>Write the Terraform configuration that represents that instance:</p> <pre><code>## add resource in main.tf\nresource \"aws_instance\" \"web\" {\n  ami           = \"ami-0abc12345\"\n  instance_type = \"t2.micro\"\n}\n\n## Run the import command:\nterraform import aws_instance.web i-0abcd1234ef56789\n\n## Now Terraform tracks it in state \u2014 verify with:\nterraform state show aws_instance.web\n</code></pre>"},{"location":"scenario_based/#large-state-file-performance-issues","title":"Large State File &amp; Performance Issues","text":"<p>Your Terraform state file has grown large and plan is getting slow. How do you optimize?</p> <ol> <li>Split your infrastructure into multiple smaller states (e.g., per environment or per component).</li> <li>Use data sources instead of having everything in a single root module.</li> <li>Enable state file compression and remote backends.</li> <li>Use <code>terraform plan -target</code> for selective planning if needed temporarily (but not as long-term solution).</li> </ol>"},{"location":"scenario_based/#how-do-you-handle-secrets-in-terraform-without-exposing-them-in-git","title":"How do you handle secrets in Terraform without exposing them in Git?","text":"<p>Store them in Azure Key Vault, AWS Secrets Manager, or Vault, and use data sources to fetch:</p> <pre><code>data \"aws_secretsmanager_secret_version\" \"db_password\" {\n  secret_id = \"my-db-password\"\n}\n\nvariable \"db_password\" {\n  default = data.aws_secretsmanager_secret_version.db_password.secret_string\n}\n</code></pre> <p>Never hardcode secrets or commit .tfvars files with credentials. Use environment variables:</p> <pre><code>export TF_VAR_db_password=\"supersecret\"\n</code></pre>"},{"location":"scenario_based/#create-a-terraform-workspace-with-dev-and-prod-and-configure-backend-file-for-dev-and-prod","title":"Create a terraform workspace with dev and prod and configure backend file for dev and prod","text":"<p>Project structure</p> <pre><code>terraform-project/\n\u2502\n\u251c\u2500\u2500 main.tf\n\u251c\u2500\u2500 variables.tf\n\u251c\u2500\u2500 outputs.tf\n\u251c\u2500\u2500 backend.tf\n\u2502\n\u251c\u2500\u2500 modules/\n\u2502   \u2514\u2500\u2500 ec2_instance/\n\u2502       \u251c\u2500\u2500 main.tf\n\u2502       \u251c\u2500\u2500 variables.tf\n\u2502       \u2514\u2500\u2500 outputs.tf\n\u2502\n\u2514\u2500\u2500 environments/\n    \u251c\u2500\u2500 dev.tfvars\n    \u2514\u2500\u2500 prod.tfvars\n</code></pre> <p>\ud83e\udde9 Step 1: Define a Module (Example: EC2 Instance)</p> <p>modules/ec2_instance/main.tf</p> <pre><code>resource \"aws_instance\" \"example\" {\n  ami           = var.ami\n  instance_type = var.instance_type\n  tags = {\n    Name = \"${var.env}-instance\"\n  }\n}\n</code></pre> <p>modules/ec2_instance/variables.tf</p> <pre><code>variable \"ami\" {\n  type        = string\n  description = \"AMI ID for the instance\"\n}\n\nvariable \"instance_type\" {\n  type        = string\n  description = \"EC2 instance type\"\n}\n\nvariable \"env\" {\n  type        = string\n  description = \"Environment name\"\n}\n</code></pre> <p>modules/ec2_instance/outputs.tf</p> <pre><code>output \"instance_id\" {\n  value = aws_instance.example.id\n}\n</code></pre> <p>\u2699\ufe0f Step 2: Root Configuration</p> <p>backend.tf</p> <pre><code>terraform {\n  backend \"s3\" {\n    bucket = \"your-terraform-state-bucket\"\n    key    = \"workspace-example/terraform.tfstate\"\n    region = \"us-east-1\"\n  }\n}\n</code></pre> <p>main.tf</p> <pre><code>provider \"aws\" {\n  region = \"us-east-1\"\n}\n\nmodule \"ec2\" {\n  source         = \"./modules/ec2_instance\"\n  ami            = var.ami\n  instance_type  = var.instance_type\n  env            = terraform.workspace\n}\n</code></pre> <p>variables.tf</p> <pre><code>variable \"ami\" {\n  type        = string\n  description = \"AMI ID\"\n}\n\nvariable \"instance_type\" {\n  type        = string\n  description = \"EC2 instance type\"\n}\n</code></pre> <p>outputs.tf</p> <pre><code>output \"instance_id\" {\n  value = module.ec2.instance_id\n}\n</code></pre> <p>\ud83c\udf0d Step 3: Environment Variable Files</p> <p>environments/dev.tfvars</p> <pre><code>ami            = \"ami-0c55b159cbfafe1f0\"\ninstance_type  = \"t2.micro\"\n</code></pre> <p>environments/prod.tfvars</p> <pre><code>ami            = \"ami-0d527b8c289b4af7f\"\ninstance_type  = \"t3.medium\"\n</code></pre> <p>\ud83d\ude80 Step 4: Create and Use Workspaces</p> <pre><code># Initialize Terraform\nterraform init\n\n# Create workspaces\nterraform workspace new dev\nterraform workspace new prod\n\n# Switch to dev\nterraform workspace select dev\nterraform apply -var-file=environments/dev.tfvars\n\n# Switch to prod\nterraform workspace select prod\nterraform apply -var-file=environments/prod.tfvars\n</code></pre> <p>\u2705 Result:</p> <p>You now have a single Terraform configuration that:</p> <p>Uses modules for reusable infrastructure logic.</p> <p>Uses workspaces (dev, prod) to isolate state.</p> <p>Uses environment variable files to customize settings</p> <p>\u2699\ufe0f Step 5: Configure Terraform Backend for Azure Blob</p> <p>backend.tf</p> <pre><code>terraform {\n  backend \"azurerm\" {\n    resource_group_name  = \"tfstate-rg\"\n    storage_account_name = \"tfstateacct12345\"\n    container_name       = \"tfstate\"\n    key                  = \"terraform.${terraform.workspace}.tfstate\"\n  }\n}\n</code></pre> <p>\ud83d\udd0d What happens here:</p> <p>terraform.workspace dynamically names the state file.</p> <p>When you use the dev workspace \u2192 state file = terraform.dev.tfstate</p> <p>When you use the prod workspace \u2192 state file = terraform.prod.tfstate</p> <p>\ud83e\udde9 Step 6: Initialize the Backend</p> <p>After setting up the backend:</p> <pre><code>terraform init \\\n  -backend-config=\"resource_group_name=tfstate-rg\" \\\n  -backend-config=\"storage_account_name=tfstateacct12345\" \\\n  -backend-config=\"container_name=tfstate\" \\\n  -backend-config=\"key=terraform.tfstate\"\n</code></pre> <p>Then create workspaces:</p> <pre><code>terraform workspace new dev\nterraform workspace new prod\n</code></pre> <p>\ud83c\udf0d Step 7: Apply per Environment</p> <p>Use workspace + var files:</p> <pre><code># Switch to dev\nterraform workspace select dev\nterraform apply -var-file=environments/dev.tfvars\n\n# Switch to prod\nterraform workspace select prod\nterraform apply -var-file=environments/prod.tfvars\n</code></pre>"},{"location":"scenario_based/#kubernetes","title":"Kubernetes","text":""},{"location":"scenario_based/#whats-difference-between-loadbalancer-and-ingress-in-kubernetes","title":"Whats difference between loadbalancer and ingress in kubernetes?","text":"<p>LoadBalancer: It exposes your application externally (outside the cluster) by provisioning a cloud load balancer (like AWS ELB, Azure Load Balancer, GCP Load Balancer).</p> <p>When you create a Service of type LoadBalancer, Kubernetes asks your cloud provider to create an external load balancer.</p> <p>The load balancer forwards traffic to the Service, which then routes it to the right Pods.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-app-lb\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n    - port: 80\n      targetPort: 8080\n</code></pre> <p>Result: \u2192 Cloud provider creates a load balancer (e.g., with an external IP) \u2192 Traffic to that IP goes to your app pods.</p> <p>\u2705 Pros</p> <p>Simple to set up.</p> <p>Directly exposes your app to the internet.</p> <p>\u26a0\ufe0f Cons</p> <p>Each service of type LoadBalancer creates a separate cloud load balancer \u2014 expensive and not scalable if you have many services.</p> <p>Limited control over routing (just ports).</p> <p>Ingress: It\u2019s an HTTP/HTTPS reverse proxy that manages external access to multiple services \u2014 typically at Layer 7 (application layer).</p> <p>You deploy an Ingress Controller (like NGINX, HAProxy, Traefik, or the cloud provider\u2019s ingress).</p> <p>You define Ingress rules that tell it how to route incoming requests based on:</p> <p>Hostnames (e.g., api.example.com)</p> <p>Paths (e.g., /api, /web)</p> <p>The Ingress Controller usually runs behind a single LoadBalancer.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\nspec:\n  rules:\n  - host: myapp.example.com\n    http:\n      paths:\n      - path: /api\n        pathType: Prefix\n        backend:\n          service:\n            name: api-service\n            port:\n              number: 80\n      - path: /web\n        pathType: Prefix\n        backend:\n          service:\n            name: web-service\n            port:\n              number: 80\n</code></pre> <p>Result: \u2192 One LoadBalancer (via the ingress controller) handles requests for many services. \u2192 Routes based on domain name and path.</p> <p>\u2705 Pros</p> <p>Single entry point for all services.</p> <p>Advanced routing (paths, hostnames, SSL termination, etc.).</p> <p>Cost-effective (only one LoadBalancer needed).</p> <p>\u26a0\ufe0f Cons</p> <p>Requires setting up an Ingress Controller.</p> <p>More complex configuration.</p>"},{"location":"scenario_based/#explain-taints-and-tolerence","title":"Explain taints and tolerence","text":"<p>In Kubernetes (k8s), taints and tolerations are a way to control which pods can be scheduled onto which nodes \u2014 they\u2019re essentially the inverse of node selectors and affinity rules.</p> <p>Think of it like this:</p> <p>Node labels / affinity = \"Please put me on this kind of node\" (pod\u2019s request)</p> <p>Taints / tolerations = \"Stay away unless you have permission\" (node\u2019s warning)</p> <ol> <li>What is a taint?</li> </ol> <p>A taint is a property you put on a node that says:</p> <p>\"I won\u2019t accept pods unless they tolerate this taint.\"</p> <p>It\u2019s defined by three parts:</p> <pre><code>key=value:effect\n</code></pre> <p>Where:</p> <pre><code>key \u2192 Identifier (e.g., dedicated)\n\nvalue \u2192 Description of the taint (e.g., gpu-workload)\n\neffect \u2192 One of:\n\nNoSchedule \u2192 Don\u2019t schedule pods unless they tolerate it.\n\nPreferNoSchedule \u2192 Avoid scheduling pods unless no better option exists.\n\nNoExecute \u2192 Evict existing pods that don\u2019t tolerate it and stop new ones from being scheduled.\n</code></pre> <p>Example command to add a taint:</p> <pre><code>kubectl taint nodes node1 dedicated=gpu-workload:NoSchedule\n</code></pre> <ol> <li>What is a toleration?</li> </ol> <p>A toleration is a property you add to a pod that says:</p> <p>\"I\u2019m okay with being scheduled on nodes that have this taint.\"</p> <p>It\u2019s defined in the pod\u2019s YAML spec:</p> <pre><code>tolerations:\n- key: \"dedicated\"\n  operator: \"Equal\"\n  value: \"gpu-workload\"\n  effect: \"NoSchedule\"\n</code></pre> <p>This tells Kubernetes: \u201cIf a node has the taint dedicated=gpu-workload:NoSchedule, I can still go there.\u201d</p> <ol> <li>How they work together</li> </ol> <p>Without a toleration \u2192 Pod cannot be scheduled on a node with a matching taint.</p> <p>With a matching toleration \u2192 Pod can be scheduled on the tainted node, but not forced \u2014 Kubernetes still considers other scheduling rules.</p> <p>Example scenario:</p> <p>Node is tainted: dedicated=gpu-workload:NoSchedule</p> <p>Normal pods (no toleration) \u2192 won\u2019t land there.</p> <p>GPU job pods (with toleration) \u2192 can land there.</p> <ol> <li>Why use taints &amp; tolerations?</li> </ol> <p>Dedicated nodes for specific workloads (e.g., GPU, high-memory, compliance-sensitive).</p> <p>Isolating workloads (e.g., separate dev/test from prod).</p> <p>Evicting pods during maintenance (NoExecute).</p>"},{"location":"scenario_based/#network-flow","title":"Network Flow","text":"<p>You\u2019ve deployed your web app (say, a website running on Nginx or Node.js) into Kubernetes. Your goal: people on the internet should be able to open https://myapp.com and see your site.</p> <p>\ud83d\ude80 Step-by-step flow</p> <p>1\ufe0f\u20e3 User makes a request</p> <ol> <li>A user types https://myapp.com in their browser.  </li> <li>The browser asks the DNS to find where myapp.com lives.</li> <li>DNS points it to a public IP \u2014 that\u2019s your Kubernetes LoadBalancer or Ingress Controller.</li> </ol> <p>2\ufe0f\u20e3 Request reaches Kubernetes</p> <ol> <li>The request enters your Kubernetes cluster through one of these:</li> <li>A LoadBalancer (provided by your cloud provider like AWS, Azure, or GCP)</li> <li>Or an Ingress Controller (like Nginx or Traefik)</li> </ol> <p>3\ufe0f\u20e3 Ingress / LoadBalancer sends it to a Service</p> <ol> <li>Inside Kubernetes, you have a Service that knows which app (Pods) should get this request.</li> <li>The Service acts like a traffic director.</li> <li>It decides which Pod (copy of your app) will handle the request.</li> </ol> <p>4\ufe0f\u20e3 Service forwards the request to a Pod</p> <ol> <li>The Pod is where your web app actually runs \u2014 it\u2019s like a small computer running your app container.</li> <li>Each Pod has its own internal IP address.</li> <li>The Service forwards the request to one of the Pods using that IP.</li> <li>Kubernetes automatically load-balances between Pods (if you have multiple replicas).</li> </ol> <p>5\ufe0f\u20e3 Pod handles the request</p> <ol> <li>Inside the Pod, your web server or app container (like Nginx or Flask or Node.js) receives the HTTP request on a port \u2014 for example, port 8080.</li> <li>It processes the request (maybe fetches some data, renders HTML) and sends back a response.</li> </ol> <p>6\ufe0f\u20e3 Response goes back the same way</p> <p>The response travels backward:</p> <p>Pod \u2192 Service \u2192 Ingress/LoadBalancer \u2192 Internet \u2192 User's Browser</p>"},{"location":"scenario_based/#how-to-connect-frontend-and-backend-in-kubernetes-using-clusterip-simple-explanation","title":"How to connect frontend and backend in Kubernetes using ClusterIP (Simple Explanation)","text":"<p>You have two apps in Kubernetes:</p> <p>Frontend</p> <p>Backend</p> <p>To make the frontend talk to the backend, you must expose the backend inside the cluster using a ClusterIP service.</p> <ol> <li>Create a ClusterIP service for the backend</li> </ol> <p>Example:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend-service\nspec:\n  selector:\n    app: backend\n  ports:\n    - port: 80\n      targetPort: 8080\n  type: ClusterIP\n</code></pre> <p>This service gets a name: backend-service</p> <ol> <li>Use the service name inside the frontend</li> </ol> <p>In the frontend code, set the backend URL to:</p> <pre><code>GET http://backend-service/api\n</code></pre> <p>That\u2019s it \u2014 no IP needed.</p>"},{"location":"scenario_based/#azure","title":"Azure","text":""},{"location":"scenario_based/#difference-between-application-gateway-and-load-balancer","title":"Difference between application gateway and load balancer","text":"<p>\ud83d\udd39 High-Level Difference</p> Feature Azure Load Balancer Azure Application Gateway OSI Layer Layer 4 (Transport Layer: TCP/UDP) Layer 7 (Application Layer: HTTP/HTTPS) Routing Type Based on IP address and port Based on HTTP(S) content (URL, headers, cookies, etc.) Primary Use Case Distributes network traffic across backend servers Provides web traffic routing, SSL termination, WAF (security), and URL-based routing Protocol Support TCP, UDP HTTP, HTTPS, WebSocket SSL/TLS Termination \u274c Not supported \u2705 Supported Web Application Firewall (WAF) \u274c Not available \u2705 Integrated WAF option Health Probes Checks TCP/port connectivity Checks application-level health (HTTP response codes, paths) Session Affinity \u2705 Supported (by source IP) \u2705 Supported (by cookies) URL Path-Based Routing \u274c Not possible \u2705 Possible Redirection (HTTP to HTTPS, etc.) \u274c Not supported \u2705 Supported Typical Scenario Internal or external non-HTTP traffic load balancing (e.g., databases, custom TCP apps) Web application routing and protection for websites, APIs, or web apps <p>\ud83d\udd38 Example Use Cases</p> <p>\ud83d\udd39 Azure Load Balancer</p> <p>Balancing traffic between VMs running non-HTTP workloads \u2014 e.g.:</p> <ul> <li> <p>SQL servers</p> </li> <li> <p>FTP servers</p> </li> <li> <p>Gaming servers</p> </li> <li> <p>Custom TCP/UDP-based apps</p> </li> </ul> <p>Backend VM access in an internal network (internal load balancer)</p> <p>Simple, high-performance layer-4 traffic distribution</p> <p>\ud83d\udd39 Azure Application Gateway</p> <ul> <li> <p>Fronting web applications (HTTP/HTTPS)</p> </li> <li> <p>Terminating SSL to reduce load on backend servers</p> </li> <li> <p>URL path-based routing (e.g., /api \u2192 API backend, /images \u2192 static server)</p> </li> <li> <p>Protecting against web attacks using Web Application Firewall (WAF)</p> </li> <li> <p>Redirecting HTTP \u2192 HTTPS</p> </li> <li> <p>Hosting multiple websites using one gateway (multi-site routing)</p> </li> </ul> <p>\ud83d\udd39 Can they work together?</p> <p>You can use both in combination:</p> <ul> <li> <p>Application Gateway (Layer 7) \u2192 routes and secures web traffic</p> </li> <li> <p>Azure Load Balancer (Layer 4) \u2192 distributes that traffic to backend VMs or containers</p> </li> </ul> <p>This layered approach improves both performance and security.</p> <p>\ud83e\udde0 Quick Summary</p> Question Answer What layer does it work on? Load Balancer = Layer 4, Application Gateway = Layer 7 What does it understand? Load Balancer = IP/Port, Application Gateway = HTTP(S) requests Can it inspect or modify requests? Only Application Gateway Can it protect web apps (WAF)? Only Application Gateway"},{"location":"scenario_based/#when-to-use-which-database","title":"When to use which database","text":"Database Type Azure Service AWS Equivalent Description / Purpose Best Use Cases Key Features Relational (SQL Server) Azure SQL Database Amazon RDS for SQL Server Fully managed relational DB-as-a-service built on SQL Server. - OLTP apps- Web &amp; enterprise systems- Structured schema &amp; ACID compliance - Auto scaling- High availability- Built-in security &amp; backups Open Source Relational (MySQL) Azure Database for MySQL Amazon RDS for MySQL / Aurora MySQL Managed open-source MySQL DB service. - Web &amp; CMS apps- LAMP stack- Cross-platform compatibility - Fully managed- Auto backup &amp; patching- High availability Open Source Relational (PostgreSQL) Azure Database for PostgreSQL Amazon RDS for PostgreSQL / Aurora PostgreSQL Managed PostgreSQL service. - Geospatial apps- Data analytics- Django / Flask apps - Open-source compatible- Built-in HA- Autoscaling Open Source Relational (MariaDB) Azure Database for MariaDB Amazon RDS for MariaDB Managed MariaDB service. - Existing MariaDB workloads- Web applications - Managed MariaDB- Easy migration- Security integrated NoSQL (Multi-model) Azure Cosmos DB Amazon DynamoDB Globally distributed, multi-model NoSQL database (document, key-value, graph, column). - IoT &amp; real-time apps- Global scale, low latency- JSON document storage - Multi-region replication- Guaranteed low latency- Horizontal scaling Big Data / Analytics Warehouse Azure Synapse Analytics Amazon Redshift Cloud-based enterprise data warehouse for analytics and BI. - Data warehousing- Business intelligence- ETL workloads - MPP architecture- Integrates with Power BI- Serverless options In-Memory Cache Azure Cache for Redis Amazon ElastiCache for Redis / Memcached Managed, in-memory cache for fast data access. - Caching- Session storage- Real-time analytics - Sub-millisecond latency- Scalable performance- Pub/Sub messaging Time-Series / Log Analytics Azure Data Explorer (ADX) Amazon Timestream Optimized for time-series and log data ingestion and querying. - IoT telemetry- Monitoring &amp; logging- Analytics dashboards - KQL query language- Fast ingestion- Real-time queries Graph Database Azure Cosmos DB (Gremlin API) Amazon Neptune Graph database for highly connected data. - Social networks- Fraud detection- Knowledge graphs - Gremlin query support- Multi-model flexibility- Global scalability Data Lake / Object Storage Azure Data Lake Storage (ADLS) Amazon S3 / AWS Lake Formation Data lake for storing unstructured and big data. - Data science- ML pipelines- Big data processing - Unlimited storage- Hadoop compatible- Cost effective Search Engine / Indexing Azure Cognitive Search Amazon OpenSearch Service (Elasticsearch) Full-text search &amp; AI-powered content indexing. - App &amp; site search- Product catalogs- Knowledge mining - AI enrichment- Integration with Blob, SQL, Cosmos- Fast indexing Ledger / Immutable Records Azure Confidential Ledger Amazon QLDB (Quantum Ledger DB) Immutable, tamper-proof ledger for auditable transactions. - Financial systems- Compliance logs- Auditing - Cryptographic verification- Managed ledger- High security"},{"location":"scenario_based/#azure-networking-services-types-use-cases-aws-equivalents","title":"Azure Networking Services \u2014 Types, Use Cases &amp; AWS Equivalents","text":"Network Type / Service Azure Service Name Description / Purpose When to Use Common Use Case Example AWS Equivalent Virtual Network (VNet) Azure Virtual Network (VNet) Core private network in Azure that allows Azure resources (VMs, databases, etc.) to securely communicate. When you need a private, isolated network in Azure. Hosting web apps, databases, and VMs within the same secure subnet. Amazon VPC (Virtual Private Cloud) Subnet Azure Subnet (inside VNet) Logical segmentation of a VNet into smaller address spaces. To organize and isolate workloads (frontend, backend, DB). Separate subnet for web servers, app servers, and DB. AWS Subnet (inside VPC) Network Security Group (NSG) Azure NSG Acts as a virtual firewall to control inbound/outbound traffic at subnet or NIC level. When you need granular control over traffic. Allow only HTTP/HTTPS to web subnet, block others. AWS Security Group / NACL Application Gateway (Layer 7) Azure Application Gateway Layer 7 load balancer with Web Application Firewall (WAF). For web traffic routing, SSL termination, and WAF protection. Distribute web traffic between multiple app servers. AWS Application Load Balancer (ALB) Load Balancer (Layer 4) Azure Load Balancer Distributes network traffic (TCP/UDP) across multiple VMs. For internal/external traffic balancing at network layer. Balancing requests between backend VM instances. AWS Network Load Balancer (NLB) VPN Gateway Azure VPN Gateway Secure site-to-site or point-to-site encrypted connection between on-premises and Azure. When connecting Azure network to on-prem data center securely. Hybrid setup with on-prem servers and Azure VMs. AWS VPN Gateway ExpressRoute Azure ExpressRoute Dedicated private fiber connection between on-prem data center and Azure. For enterprise-grade, low-latency, high-security private connection. Banking or healthcare workloads requiring guaranteed performance. AWS Direct Connect Firewall / Security Appliance Azure Firewall Managed network firewall for centralized policy control and logging. When you need enterprise-grade protection across VNets. Centralized firewall protecting all workloads. AWS Network Firewall Private Endpoint / Link Azure Private Link Enables private access to Azure services via private IPs. To access Azure services (Storage, SQL) privately without public internet. Accessing Azure SQL Database securely from VNet. AWS PrivateLink Traffic Manager (DNS-based routing) Azure Traffic Manager Global DNS load balancer for routing traffic based on geography, latency, or priority. For distributing traffic across multiple Azure regions. Global app with users in multiple continents. AWS Route 53 (Latency / Geo Routing) Application Gateway + Front Door Azure Front Door Global load balancer and CDN for web applications. For global, high-availability web applications with caching. Multi-region web app with CDN acceleration. AWS CloudFront + Global Accelerator Bastion Host Azure Bastion Provides secure RDP/SSH access to VMs via browser (no public IP). For secure remote access to Azure VMs. Admins accessing VMs without exposing them publicly. AWS Systems Manager Session Manager / AWS Bastion Host DNS Zone Azure DNS Host and manage DNS records for your domains within Azure. When managing custom domains within Azure. Hosting <code>myapp.com</code> DNS zone inside Azure. Amazon Route 53 (DNS Management) Peering / Hub-Spoke Network VNet Peering / Hub-Spoke Architecture Connect multiple VNets for cross-communication with low latency. When you have multiple VNets (apps, DBs, regions). Multi-environment (Dev/Test/Prod) network design. AWS VPC Peering / Transit Gateway Content Delivery Network (CDN) Azure CDN Delivers content from edge servers close to users. For fast delivery of static web content globally. Serving website images, videos, scripts. AWS CloudFront NAT Gateway Azure NAT Gateway Provides outbound internet access for private resources without exposing inbound access. For VMs that need outbound access securely. VM in private subnet accessing internet APIs. AWS NAT Gateway Virtual WAN (Global Network) Azure Virtual WAN Simplifies connectivity between multiple branches, VNets, and on-prem sites. For global enterprise networks with many sites. Connecting multiple branch offices to Azure. AWS Transit Gateway / AWS Cloud WAN Hybrid Storage Gateway Azure File Sync, Azure Files, Azure Blob Storage, Azure NetApp Files, Azure Data Box, Azure Backup Provides hybrid on-prem \u2194 cloud storage scenarios: cache and sync on-prem file shares to cloud, archive/tape replacement, bulk data transfer. Azure combines these services to cover AWS Storage Gateway modes (file, volume, tape). - File caching / sync: Azure File Sync + Azure Files (SMB).- Block/volume storage: Azure NetApp Files / Managed Disks.- Archive/tape replacement: Azure Blob (cool/archive) + Azure Backup.- Offline bulk transfer: Azure Data Box. AWS Storage Gateway (File / Volume / Tape Gateway)"},{"location":"scenario_based/#azure-cost-management-optimization-services-with-aws-equivalents","title":"Azure Cost Management &amp; Optimization Services \u2014 With AWS Equivalents","text":"Category / Resource Azure Service / Feature Purpose / Description When to Use Common Use Case Example AWS Equivalent Cost Monitoring &amp; Analysis Azure Cost Management + Billing Built-in tool to track, analyze, and optimize Azure spending. Provides cost breakdown by resource, service, and subscription. To monitor and analyze cloud costs across subscriptions, resource groups, or management groups. View monthly cost reports, forecast future spending, and analyze cost by department. AWS Cost Explorer Budgets &amp; Alerts Azure Budgets Set spending limits with automated alerts when thresholds are reached. To control overspending and trigger notifications or automation. Set $10,000/month budget for dev environment, send alert at 80% spend. AWS Budgets Pricing &amp; Estimation Tool Azure Pricing Calculator Estimate cost before deployment; simulate different configurations and services. During planning or proposal phases to estimate future Azure costs. Estimate total monthly cost for new VM setup. AWS Pricing Calculator Advisor Recommendations Azure Advisor (Cost Recommendations) AI-driven insights to optimize cost by right-sizing resources, removing idle VMs, etc. When you want actionable cost-saving recommendations. Reduce VM size or shut down unused VMs to save cost. AWS Trusted Advisor (Cost Optimization) Reserved Instances / Savings Plans Azure Reservations Prepay for 1 or 3 years to save up to 70% on VM, SQL, or App Service costs. For predictable workloads running continuously. Reserve VM instances for steady-state production workloads. AWS Reserved Instances / Savings Plans Spot Instances (Low-cost Compute) Azure Spot VMs Use unused Azure compute capacity at deep discounts. Can be evicted anytime. For non-critical, batch, or test workloads tolerant to interruptions. Run test jobs or render farms cheaply. AWS EC2 Spot Instances Auto-Scaling Azure Autoscale (in VMSS / App Service) Automatically scale compute up/down based on demand to control cost. For variable or seasonal workloads. Scale out web servers during peak traffic, scale in after hours. AWS Auto Scaling / EC2 Auto Scaling Resource Tagging Azure Tags Add metadata (like cost center, owner, environment) for cost tracking and governance. To allocate and report costs by department, project, or environment. Tag all resources with \u201cDept:Finance\u201d for chargeback reports. AWS Resource Tags / Cost Allocation Tags Management Groups &amp; Policies Azure Management Groups + Azure Policy Organize multiple subscriptions and enforce cost-related policies globally. For enterprise-wide cost governance. Apply cost limits or restrictions across all subscriptions. AWS Organizations + Service Control Policies (SCPs) Enterprise Agreements / Billing Accounts Enterprise Enrollment / MCA (Microsoft Customer Agreement) Centralized billing and negotiated enterprise pricing for large organizations. When managing cost across multiple subscriptions and departments. Consolidate billing for multiple teams under one contract. AWS Organizations Consolidated Billing / Enterprise Agreement Cost Export &amp; API Access Azure Cost Management Exports / APIs Export detailed usage and cost data for BI tools or automation. When integrating Azure cost data with Power BI or third-party systems. Export daily cost usage data to Power BI for analysis. AWS Cost &amp; Usage Report (CUR) Free Tier &amp; Credits Azure Free Account / Dev/Test Pricing / Sponsorships Limited free usage and credits for testing or learning. For new users or developers testing Azure services. Deploy free-tier web apps or databases for POC. AWS Free Tier Hybrid Benefit / License Savings Azure Hybrid Benefit Reuse on-prem Windows Server or SQL licenses to reduce Azure costs. When migrating licensed workloads from on-prem to Azure. Apply hybrid benefit to reduce cost of SQL Server VMs. AWS BYOL (Bring Your Own License) Cost Control for Dev/Test Azure Dev/Test Labs / Dev/Test Pricing Manage cost-effective environments for development and testing. For development teams that need to spin up/down environments frequently. Create auto-expiring test environments for developers. AWS CloudFormation StackSets with Budget Controls / AWS Sandbox Accounts Storage Tiering Azure Blob Storage Tiers (Hot, Cool, Archive) Automatically move data between tiers based on usage to save cost. For long-term data retention or infrequently accessed data. Move logs to Cool tier, backups to Archive tier. AWS S3 Storage Classes (Standard, Infrequent Access, Glacier) Monitoring &amp; Insights Azure Monitor + Log Analytics Analyze resource usage and performance metrics to detect cost spikes. To monitor resource utilization and optimize for efficiency. Detect overprovisioned compute or unused storage. AWS CloudWatch + CloudWatch Logs FinOps &amp; Governance Integration Azure Cost Management APIs + Power BI Integration Integrate cost data into dashboards for FinOps visibility. For finance and IT teams managing multi-department budgets. Create Power BI dashboards from exported Azure cost data. AWS Cost Anomaly Detection + QuickSight Dashboards Automation / Optimization Tools Azure Automation / Logic Apps / Functions Automate shutdown/startup or resizing of VMs to save cost. When automating cost control tasks. Auto-stop VMs at 7 PM and restart at 8 AM daily. AWS Lambda + EventBridge + Instance Scheduler"},{"location":"scenario_based/#ways-external-users-can-access-a-web-application-hosted-in-azure","title":"Ways External Users Can Access a Web Application Hosted in Azure","text":"<p>Direct Public Internet Access</p> Access Method Description When to Use Example / Use Case Security Options Public Endpoint (Default) Azure Web Apps (App Service), VMs, or APIs can be accessed directly via a public URL (e.g., <code>https://myapp.azurewebsites.net</code>). For public-facing apps (portals, marketing sites). A company website accessible globally. Use HTTPS, Azure Front Door WAF, and custom domains. Custom Domain + SSL Map your custom domain (e.g., <code>www.contoso.com</code>) to Azure Web App or Front Door endpoint. When you need branded domain for end users. Public e-commerce website. Enable HTTPS with Azure-managed certificate. <p>Secured Public Access (via Edge Services)</p> Access Method Description When to Use Example / Use Case Security Options Azure Front Door Global entry point that provides SSL offload, CDN caching, WAF, and DDoS protection. For global, high-performance web applications. Multi-region web app serving customers worldwide. Built-in WAF, HTTPS enforcement, geo-filtering. Azure Application Gateway (WAF) Layer 7 load balancer with Web Application Firewall; routes traffic to backend pools (VMs, App Services, etc.). For apps needing centralized Layer 7 routing and security. Banking portal behind WAF. WAF policies, SSL termination, rate limiting. Azure CDN Delivers static content globally through caching. For static-heavy sites (images, videos, JS). Media or marketing websites. HTTPS, token authentication, CDN rules. Azure API Management (APIM) Acts as a secure gateway for APIs exposed to external developers/partners. For exposing APIs publicly with rate limits, authentication, and analytics. Public developer API platform. OAuth2, JWT validation, subscription keys, IP filtering. <p>Controlled Private Access (with Secure Tunnel or Private Link)</p> Access Method Description When to Use Example / Use Case Security Options Azure Application Gateway + Private Endpoint Use private endpoint for backend app service, but expose only via secure gateway/WAF. When you want internet access but hide internal app endpoints. Internal web app accessed securely through gateway. WAF, private link, NSGs. Azure Private Link / Private Endpoint Allows access to your web app via private IP \u2014 no public internet exposure. For B2B or internal partner access over VPN or ExpressRoute. Partner accessing internal customer portal. Private DNS, VPN/ExpressRoute, RBAC. VPN Gateway / Point-to-Site VPN External users connect to Azure network via secure VPN tunnel. For employees or trusted partners accessing internal apps. Employee accessing intranet portal from home. IPSec, certificate-based authentication. Azure Bastion + RDP/SSH over Browser Provides secure browser-based access to VMs without public IPs. For admins or developers needing internal VM access. DevOps team managing app servers securely. Azure AD + MFA + NSG + Bastion. <p>Authentication-Based Access</p> Access Method Description When to Use Example / Use Case Security Options Azure AD Authentication (App Service Auth) Secure your web app using Azure AD or external IdPs (Google, Facebook, etc.). For user-facing apps needing login. Employee or customer portal. Azure AD, OAuth2, OpenID Connect, MFA. Azure AD B2C Customer Identity and Access Management (CIAM) for external users. When managing external customers or partners with self-service signup. E-commerce or SaaS app login for end users. Custom policies, SSO, MFA, social logins. Conditional Access (Azure AD) Enforce conditions (device, location, MFA) before access. For sensitive internal web apps. Finance or HR portal. Azure AD Conditional Access policies. <p>Hybrid or Enterprise Access</p> Access Method Description When to Use Example / Use Case Security Options Azure ExpressRoute (Private Fiber) Dedicated private network connection between on-prem data center and Azure. For mission-critical enterprise or government workloads. Financial institution accessing Azure apps privately. Private circuits, network-level encryption. Azure Application Proxy (from Entra ID) Publishes on-prem web apps securely for external users without exposing internal network. When migrating hybrid apps or exposing legacy intranet apps. Legacy HR web app accessible over internet securely. Azure AD, Conditional Access, MFA."},{"location":"scenario_based/#allow-only-one-external-company-to-access-your-azure-web-application","title":"Allow Only One External Company to Access Your Azure Web Application","text":"<p>\ud83c\udfaf Requirement</p> <ul> <li>Your web application is hosted in Azure (e.g., Azure App Service, AKS, or VM).  </li> <li>A specific external company (CA company) must be able to access it.  </li> <li>\u274c No other user (public internet or other companies) should have access.</li> </ul> <p>\ud83e\udd47 Option1: Azure AD B2B (Business-to-Business) Guest Access \u2014 Recommended</p> <p>\ud83e\udde9 Concept Use Azure Active Directory (Microsoft Entra ID) to authenticate external users. Invite users from the CA company as B2B guest users \u2014 only they can sign in to the app.  </p> <p>\u2699\ufe0f How It Works - App is protected by Azure AD Authentication. - Only invited B2B guest users can log in. - Everyone else is blocked at the identity layer (no public access).</p> <p>\ud83e\ude9c Steps 1. Enable Authentication in your Azure App Service    - Go to \u2192 App Service \u2192 Authentication \u2192 Enable \u201cApp Service Authentication\u201d.    - Choose Microsoft Entra ID (Azure AD) as identity provider.</p> <ol> <li>Invite CA Company Users as Guests</li> <li>Azure Portal \u2192 Microsoft Entra ID \u2192 Users \u2192 New Guest User \u2192 enter their email (e.g., <code>john@cacomapny.com</code>).</li> <li> <p>They\u2019ll get an invitation email.</p> </li> <li> <p>Create a Security Group (e.g., <code>CACompanyUsers</code>)  </p> </li> <li> <p>Add those guest users into the group.</p> </li> <li> <p>Restrict App Access</p> </li> <li>Go to App registration \u2192 Enterprise Applications \u2192 Users and groups \u2192 assign only that group access.</li> <li> <p>Optionally use Conditional Access Policy to enforce MFA or IP restrictions.</p> </li> <li> <p>Test Access</p> </li> <li>Only users in that group (from CA company) can log in.</li> <li>All others get \u201cAccess Denied\u201d.</li> </ol> <p>\u2705 Advantages - Most secure and scalable. - Identity-based, not IP-based. - Supports MFA, auditing, SSO. - Easy to revoke or add new users.</p> <p>\ud83e\udd48 Option 2: Restrict Access by IP Address (Network Level)</p> <p>\ud83e\udde9 Concept Allow traffic only from the CA company\u2019s known public IP address(es). All other traffic is denied.</p> <p>\u2699\ufe0f How It Works App Service has Access Restrictions that allow only specific IPs or CIDR ranges.</p> <p>\ud83e\ude9c Steps 1. Ask CA company for their static public IP (or range). 2. In Azure Portal \u2192 App Service \u2192 Networking \u2192 Access Restrictions. 3. Add a rule:    - Action: Allow    - IP Range: <code>203.0.113.10/32</code> (example) 4. Add another rule:    - Action: Deny All 5. Save and apply.</p> <p>\u2705 Advantages - Simple to implement. - No identity setup required.</p> <p>Option 3: Private Endpoint + VPN or ExpressRoute (Private Connectivity)</p> <p>\ud83e\udde9 Concept Make the app private \u2014 accessible only inside a VNet. Then create a VPN connection between your VNet and the CA company\u2019s on-prem network.</p> <p>How It Works - Your app is not publicly accessible (private endpoint only). - The CA company connects via a Site-to-Site VPN (or ExpressRoute). - The app uses a private IP inside Azure.</p> <p>\ud83e\ude9c Steps 1. Enable Private Endpoint for your App Service (in \u201cNetworking\u201d \u2192 \u201cPrivate Endpoint Connections\u201d). 2. Deploy an Azure VPN Gateway in your VNet. 3. Create a Site-to-Site VPN with the CA company\u2019s on-prem router/firewall. 4. Configure routing so that CA company users reach the private IP of your app. 5. Disable the public endpoint of your app.</p> <p>\u2705 Advantages - No internet exposure at all. - Secure and compliant for sensitive data. - Works even if CA company doesn\u2019t use Azure AD.</p> <p>\ud83c\udfc5 Option 4: Azure AD Application Proxy (If the App is Internal)</p> <p>\ud83e\udde9 Concept Use Azure AD Application Proxy to publish your internal or Azure-hosted app securely for external users.</p> <p>\u2699\ufe0f How It Works - External users must authenticate via Azure AD. - Proxy connector allows only authorized users through. - No direct public exposure of your app.</p> <p>\ud83e\ude9c Steps 1. Enable Azure AD Application Proxy in your tenant. 2. Register your web app as an Enterprise Application. 3. Assign CA company guest users to the app. 4. Distribute the app proxy URL only to authorized users.</p> <p>\u2705 Advantages - Adds extra security layer. - Works for both on-prem and cloud apps. - Uses Azure AD for identity and policy enforcement.</p> Scenario Recommended Option Security Level Ease of Setup company has Microsoft accounts / Azure AD Option 1 \u2013 Azure AD B2B \ud83d\udd12\ud83d\udd12\ud83d\udd12 \u2b50\u2b50 company has fixed public IP Option 2 \u2013 IP Restriction \ud83d\udd12 \u2b50\u2b50\u2b50 company wants private network access Option 3 \u2013 Private Endpoint + VPN \ud83d\udd12\ud83d\udd12\ud83d\udd12 \u2b50 company access internal/legacy app Option 4 \u2013 Application Proxy \ud83d\udd12\ud83d\udd12 \u2b50\u2b50"},{"location":"scenario_based/#python","title":"Python","text":""},{"location":"scenario_based/#reverse-the-words-in-a-given-string","title":"reverse the words in a given string","text":"<pre><code>input =\"hello ramakrishna how are you\"\n\nfor i in input.split()[::-1]:\n    print(i,end=' ')\n</code></pre>"},{"location":"scenario_based/#sum-of-digits-of-number","title":"sum of digits of number","text":"<pre><code>num='123456'\nsum = 0\nfor i in num:\n    sum=sum+int(i)\nprint(sum)\n</code></pre>"},{"location":"scenario_based/#group-words-that-are-anagrams","title":"Group Words That Are Anagrams","text":"<pre><code>Input=[\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\",\"ant\"]\n\nfrom collections import defaultdict\n\ndef anagram(Input):\n    groups=defaultdict(list)\n    for word in Input:\n        key = ''.join(sorted(word))\n        groups[key].append(word)\n    return groups\n\nprint(anagram(Input))\n</code></pre>"},{"location":"scenario_based/#find-duplicates-in-a-list","title":"Find Duplicates in a List","text":"<pre><code>Input = [1, 3, 4, 2, 2, 3,3,3]\n\nnum_list=[]\nduplicate=[]\nfor i in Input:\n    if i not in num_list:\n        num_list.append(i)\n    else:\n        duplicate.append(i)\nprint(list(set(duplicate)))\n</code></pre>"},{"location":"scenario_based/#merge-two-sorted-lists","title":"Merge Two Sorted Lists","text":"<pre><code>input1= [1, 3, 5] \n\ninput2=[2, 4, 6]\n\nmerge =input1+input2\n\nprint(sorted(merge))\n</code></pre>"},{"location":"scenario_based/#find-common-elements-between-two-lists","title":"Find Common Elements Between Two Lists","text":"<pre><code>Input= [1, 2, 3, 4]\nInput2=[3, 4, 5, 6]\n\ncommon_list=[]\nfor i in Input:\n    if i in Input2:\n        common_list.append(i)\nprint(common_list)\n</code></pre>"},{"location":"scenario_based/#move-all-zeros-to-end-of-list","title":"Move All Zeros to End of List","text":"<pre><code>Input= [0, 1, 0, 3, 12]\n\nlist1=[]\nlist2=[]\nfor i in Input:\n    if i != 0:\n        list1.append(i)\n    else:\n        list2.append(i)\nprint(list1+list2)\n\n# or\n\ndef merge(Input):\n    nonzeros = [x for x in Input if x !=0]\n    zeros= [0] * (len(Input) - len(nonzeros))\n    print(nonzeros+zeros)\n\nmerge(Input)\n</code></pre>"},{"location":"scenario_based/#find-longest-substring-without-repeating-characters","title":"Find Longest Substring Without Repeating Characters","text":"<pre><code>def longest_unique_substring(s):\n    start = 0\n    longest = 0\n    current = \"\"\n    seen = {}\n\n    for end in range(len(s)):\n        char = s[end]          # s[0]\n        if char in seen and seen[char] &gt;= start:\n            start = seen[char]+1\n\n        seen[char] = end\n        current_length= end - start +1\n\n        if current_length &gt; longest:\n            longest = current_length\n            current= s[start:end + 1]\n\n    return longest, current\n\ns = \"abcabcbbabcdefabc\"\nlength, substring = longest_unique_substring(s)\nprint(\"Length:\", length)\nprint(\"Substring:\", substring)\n</code></pre>"},{"location":"scenario_based/#two-sum-problem","title":"Two Sum Problem","text":"<pre><code>nums = [2,7,11,15]\ns=len(nums)\n\ndef find_num(nums):\n\n    for i in range(len(nums)):\n        for j in range(i+1,len(nums)):\n            if nums[i]+nums[j]==target:\n                return [i,j]\n    return \"no sum match\"\n\ntarget=22\nprint(find_num(nums))\n</code></pre>"},{"location":"scenario_based/#find-majority-element-in-a-list","title":"Find Majority Element in a List","text":"<pre><code>Input= [3, 3, 4, 2, 3, 3, 3]\n\nmax=0\n\nfor i in Input:\n    if Input.count(i) &gt; max:\n        max = Input.count(i)\n        print(i)\n</code></pre>"},{"location":"scenario_based/#read-a-log-file-and-count-how-many-times-each-error-message-appears","title":"Read a log file and count how many times each error message appears.","text":"<p>You have a log file (e.g., app.log) containing lines like this:</p> <pre><code>INFO User logged in\nERROR Disk full\nWARNING Memory low\nINFO File saved\nERROR Disk full\nERROR Network timeout\n</code></pre> <p>You need to read the log file and count how many times each error message appears.</p> <pre><code>from collections import Counter\n\ndef count_error_messages(log_file_path):\n    error_messages = []\n\n    with open(log_file_path, 'r') as file:\n        for line in file:\n            line = line.strip()\n            if line.startswith(\"ERROR\"):\n                # Extract the message after \"ERROR\"\n                message = line.split(\"ERROR\", 1)[1].strip()\n                error_messages.append(message)\n\n    # Count occurrences\n    error_counts = Counter(error_messages)\n\n    # Display results\n    for message, count in error_counts.items():\n        print(f\"{message}: {count}\")\n\n# Example usage\ncount_error_messages(\"app.log\")\n</code></pre> <p>Output:</p> <pre><code>Disk full: 2\nNetwork timeout: 1\n</code></pre>"},{"location":"service_mesh/","title":"Istio service-mesh","text":""},{"location":"service_mesh/#what-is-service-mesh","title":"What is service-mesh?","text":"<p>A service mesh is a software layer that manages communication between services in an application. It's made up of containerized microservices and network proxies called \"sidecars\".</p>"},{"location":"service_mesh/#what-is-east-west-and-north-south-networking-traffic","title":"What is East West and North South Networking Traffic","text":"<p>North-South Traffic Definition: North-South traffic refers to the data flow between clients or external networks (outside the data center) and servers or resources inside the data center (or cloud).</p> <p>Direction: Traffic moving in and out of the data center or cloud environment.</p> <p>Typical Use Cases:</p> <p>Client-server communication: When a user accesses a website, sends a request to an API, or retrieves data from a cloud-based service.</p> <p>East-West Traffic Definition: East-West traffic refers to the data flow between different systems or components within the same data center or cloud environment. It\u2019s internal traffic.</p> <p>Direction: Traffic moving laterally between servers, microservices, virtual machines, or containers within the data center.</p> <p>Typical Use Cases:</p> <p>Microservices communication: In modern cloud-native applications, microservices frequently communicate with each other to process requests. Internal system processes: When one part of an application interacts with another part inside the data center (e.g., web server talking to a database server).</p> <p></p>"},{"location":"service_mesh/#why-to-use-istio-what-are-benefit-of-using-istio","title":"Why to use Istio, what are benefit of using Istio?","text":"<p>Istio is a powerful open-source service mesh platform used to connect, secure, control, and observe services in a microservices architecture. It provides a wide range of features that help manage and secure service-to-service communication across modern distributed applications.</p> <p>Advantages of using Istio:</p> <ul> <li> <p>mTLS: Istio provides mutual TLS connection between applications to communicate.</p> </li> <li> <p>Traffic Management: Fine-grained control over service traffic with intelligent routing, load balancing, and failure recovery.</p> </li> <li> <p>Security: Built-in security features, like mutual TLS, encryption, and authentication, for securing communication between services.</p> </li> <li> <p>Observability: Provides deep visibility with tracing, monitoring, and logging of service-to-service traffic.</p> </li> <li> <p>Policy Enforcement: Enforce policies like rate-limiting, quotas, and access control for services.</p> </li> <li> <p>Resilience: Built-in fault tolerance features like retries, timeouts, and circuit breaking.</p> </li> <li> <p>Service Mesh: Simplifies management of microservices by decoupling networking logic from application code, enhancing scalability.</p> </li> </ul>"},{"location":"service_mesh/#workflow","title":"Workflow","text":"<p>Imagine you have three services, A, B, and C, deployed in your Kubernetes cluster. Each service has its own Envoy sidecar proxy.</p> <p>Service A sends a request to Service B.</p> <p>The request is first intercepted by Service A\u2019s Envoy proxy, which forwards it to Service B\u2019s Envoy proxy.</p> <p>Service B\u2019s Envoy proxy applies any traffic rules (e.g., retries, timeouts, load balancing) and sends the request to Service B.</p> <p>Along the way, the request is authenticated using mTLS, encrypted, and checked against any security or authorization policies.</p> <p>Metrics, logs, and traces are collected by both Envoy proxies, providing full visibility into the request.</p> <p>If Service B is unhealthy or experiencing issues, Istio can automatically route traffic to a different instance of Service B or even roll back to a previous version using traffic routing rules.</p>"},{"location":"terraform_associate/","title":"Terraform Associate","text":""},{"location":"terraform_associate/#provider","title":"Provider","text":"<p>Terraform providers are plugins or modules in Terraform that let it interact with external systems or services.</p> <p>How They Work:</p> <p>Connect to Systems: Providers are like translators that help Terraform communicate with cloud platforms (like AWS, Azure, or Google Cloud), on-premises tools, or third-party services.  </p> <p>Define Resources: They define the types of resources you can manage, like virtual machines, databases, or storage buckets.  </p> <p>Manage Resources: Providers handle creating, reading, updating, and deleting those resources in the system.  </p> <p>Examples of Providers:</p> <ul> <li>Cloud Providers: AWS, Azure, Google Cloud  </li> <li>Service Providers: Kubernetes, GitHub, Datadog</li> <li>Infrastructure Providers: VMware, OpenStack</li> </ul> <p>Key Points:</p> <p>Each provider needs configuration, usually credentials or access keys, to connect to the system. Terraform uses providers to translate your .tf files (infrastructure code) into API calls to the respective services.</p> <p>Eg: To install a provider</p> <pre><code>terraform {\n    required_providers {\n        azurerm = {\n            source = \"hashicorp/azurerm\"\n            version = \"4.14.0\"\n        }\n    }\n}\n</code></pre> <p>command : <code>terraform init</code></p>"},{"location":"terraform_associate/#lock-file","title":"Lock file","text":"<p>Dependency Lock File</p> <p>Purpose: Ensures consistent versions of Terraform providers are used across environments.</p> <p>How It Works:</p> <p>When you run terraform init, Terraform creates or updates the terraform.lock.hcl file. This file contains the exact versions of the providers Terraform is configured to use.</p> <p>Benefits:</p> <p>Prevents breaking changes by ensuring the same provider versions are used, even if newer versions are released. Ensures reproducibility across teams and environments.</p> <p>Managing the Lock File:</p> <p>To update provider versions, run:</p> <pre><code>terraform init -upgrade\n</code></pre>"},{"location":"terraform_associate/#state-file","title":"State file","text":"<p>The state file contains details about the resources created by Terraform, including their current configuration and metadata like resource IDs and attributes.</p> <p>When you run terraform plan - Terraform compares the desired configuration (defined in your .tf files) with the actual state of the infrastructure (in the state file). Any differences are shown in the plan output</p> <p>State Management Commands</p> <pre><code>terraform state list: Lists all resources in the state file.\nterraform state show &lt;resource_address&gt;: Displays detailed information about a specific resource.\nterraform state pull: Downloads the remote state file locally.\nterraform state push: Uploads a local state file to the remote backend.\n</code></pre>"},{"location":"terraform_associate/#terraform-import","title":"Terraform Import","text":"<p>Terraform import is command that allows you to incorporate existing infrastructure resources into your Terraform configuration and state management.</p> <p>Steps:</p> <ol> <li> <p>create main.tf containing resource name, subscription id</p> </li> <li> <p>Execute import command with resource name and subscription id</p> <pre><code>terraform init\nterraform import azurerm_key_vault_secret.example /subscriptions/5896dffd-29db-dd8g-b786-dgdg8dgdg8/resourceGroups/pss-common/providers/Microsoft.KeyVault/vaults/terraform-kv-001/secrets/terraform-secret-name\n</code></pre> </li> <li> <p>Display imported resources</p> <pre><code>terraform state list #to display configured resource\nterraform state show azurerm_key_vault_secret.example # to display resource values\n</code></pre> </li> <li> <p>To verify resources using terraform plan, updated main.tf with details display in terminal</p> <pre><code>terraform plan\n</code></pre> </li> </ol> <p>To show state file in json format</p> <pre><code>terraform show -json &gt; terraform_state.json\n</code></pre>"},{"location":"terraform_associate/#verbose","title":"Verbose","text":"<p>Levels of TF_LOG</p> <p>TF_LOG=TRACE: This is the most verbose logging level. It logs every detail about the process, including internal operations and data being sent between the Terraform CLI and providers.</p> <p>TF_LOG=DEBUG: This level provides detailed information but omits some internal operations that aren\u2019t normally needed for most users. It's useful for debugging the configuration and interactions with providers.</p> <p>TF_LOG=INFO: This is the default level and provides general information about what Terraform is doing, like showing the resources being created, updated, or destroyed.</p> <p>TF_LOG=WARN: Shows warnings, such as deprecated functionality, but doesn't provide much information otherwise.</p> <p>TF_LOG=ERROR: Only shows error messages when something goes wrong.</p> <pre><code># set terraform log\nexport TF_LOG=\"TRACE\"\nexport TF_LOG_PATH=\"filepath.log\"\n\n# unset var\nunset TF_LOG\nunset TF_LOG_PATH\n</code></pre>"},{"location":"terraform_associate/#terraform-console","title":"Terraform Console","text":"<p>The terraform console command provides an interactive shell to evaluate Terraform expressions and interact with the Terraform configuration and state. It is a powerful tool for debugging, testing expressions, and exploring Terraform resources and data.</p> <p>Key Features of terraform console: You can test Terraform expressions, such as arithmetic, string manipulation, or functions, to understand their output.</p> <p>Inspect Variables and Outputs:Access variables or outputs defined in the configuration.</p> <pre><code>&gt; var.my_variable\n\"some value\"\n&gt; output.my_output\n\"output value\"\n</code></pre>"},{"location":"terraform_associate/#modules","title":"Modules","text":"<p>In Terraform, a module is a container for multiple resources that are used together. It allows you to group resources into reusable, self-contained units of configuration. Modules help organize Terraform code, improve reusability, and reduce duplication. They can be simple, like a single resource, or more complex, containing a set of resources for creating an entire infrastructure component.</p> <p>Modules in Terraform allow for logical separation of infrastructure code and enable reusability. They are fundamental to writing clean, maintainable, and modular infrastructure code.</p> <p>Types of Terraform Modules</p> <p>Root Module: This is the starting point for Terraform execution and contains all the Terraform configuration files in the current working directory. It is the main module where you run terraform init, terraform plan, and terraform apply.</p> <p>Child Modules: Modules that are used within the root module or other modules. You can create and call these modules to encapsulate parts of your configuration logic.</p> <p>External Modules: Terraform modules that are stored outside of your project, typically shared in a module registry, like the Terraform Module Registry. You can use external modules to avoid \"reinventing the wheel\" and make use of pre-existing infrastructure code written by others.</p> <pre><code>/project\n  \u251c\u2500\u2500 main.tf           (root module)\n  \u251c\u2500\u2500 modules/\n  \u2502    \u2514\u2500\u2500 s3_bucket/\n  \u2502        \u251c\u2500\u2500 main.tf  (child module to create an S3 bucket)\n  \u2502        \u2514\u2500\u2500 variables.tf\n  \u251c\u2500\u2500 outputs.tf\n  \u2514\u2500\u2500 variables.tf\n</code></pre>"},{"location":"terraform_associate/#lock-terraform-state-file","title":"lock terraform state file","text":"<p>Locking the Terraform state file is a mechanism to prevent simultaneous operations (like multiple terraform apply or terraform plan commands) from corrupting the state file. When multiple users or processes try to modify the state file concurrently, there\u2019s a risk of conflicts or overwrites. State file locking ensures that only one process can modify the state file at a time.</p> <p>How State Locking Works in Terraform Terraform uses a locking mechanism provided by the backend where the state file is stored.</p> <ol> <li>Acquires a lock before modifying the state file.</li> <li>Prevents other operations until the lock is released.</li> <li>Releases the lock after the operation completes.</li> </ol> <p>Locking in Azure Cloud : In Azure, the Azure Blob Storage backend supports state file locking by using Azure Storage blob leases. A lease is a mechanism provided by Azure Blob Storage to ensure exclusive access to a blob for a certain period.</p> <p>Setting Up Terraform State Locking with Azure Blob Storage</p> <p>Prerequisites:</p> <ol> <li>An Azure Storage account.</li> <li>A container in Azure Blob Storage to store the state file.</li> <li>Configure the Backend: Add the following configuration in your Terraform code to use Azure Blob Storage as the backend:</li> </ol> <pre><code>terraform {\n  backend \"azurerm\" {\n    resource_group_name  = \"myResourceGroup\" # The Azure resource group containing the storage account.\n    storage_account_name = \"mystorageaccount\" # The name of the storage account.\n    container_name       = \"terraformstate\" # The container name in Blob Storage to store the state file.\n    key                  = \"terraform.tfstate\" # The file name for the Terraform state file.\n  }\n}\n</code></pre> <p>Enable State Locking: By default, when you use Azure Blob Storage as a backend, Terraform automatically uses blob leases to lock the state file during operations. No additional configuration is needed.</p> <p>Managing Locks: If Terraform detects a lock during an operation, it will:</p> <p>Wait until the lock is released.</p> <p>If needed, you can manually break the lock using Azure CLI:</p> <pre><code>az storage blob lease break \\\n  --blob-name terraform.tfstate \\\n  --container-name terraformstate \\\n  --account-name mystorageaccount\n</code></pre>"},{"location":"terraform_associate/#resource-drift","title":"Resource drift","text":"<p>Resource drift in Terraform refers to the situation where the actual state of resources in your infrastructure differs from the expected state defined in your Terraform configuration files. Drift can occur when changes are made to infrastructure outside of Terraform's control, such as manual modifications in the cloud provider's console or API.</p> <p>Causes of Resource Drift</p> <ol> <li>Manual Changes: Alterations made directly in the cloud provider's management console or via other tools.</li> <li>Automated Processes: Updates performed by scripts or other automated workflows outside of Terraform.</li> <li>External Dependencies: Changes in resources that Terraform depends on but does not manage, such as auto-scaling events or updates in external systems.</li> <li>Configuration Changes: Updates in Terraform configuration files that do not match the current state of the resources.</li> </ol> <p>If manual changes have been made to your infrastructure and you want to bring those changes into Terraform so that it recognizes them without overwriting them, you can achieve this through a process called Terraform import. Here's how you can do it:</p> <p>Steps to Add Manual Changes to Terraform</p> <ol> <li> <p>Identify the Resources to Import  </p> <p>Determine which resources have been manually changed and need to be imported into Terraform. Collect the identifiers (e.g., resource ID, ARN, or name) for these resources.</p> </li> <li> <p>Update Your Terraform Configuration  </p> <p>Add the resource block to your Terraform configuration for the resource you want to import. Ensure that the resource block matches the current state of the resource as closely as possible (e.g., resource type, attributes).</p> <p>Example:</p> <pre><code>resource \"azurerm_virtual_machine\" \"example\" {\nname                  = \"my-vm\"\nresource_group_name   = \"myResourceGroup\"\nlocation              = \"East US\"\nvm_size               = \"Standard_DS1_v2\"\nnetwork_interface_ids = [azurerm_network_interface.example.id]\n}\n</code></pre> </li> <li> <p>Run the Terraform Import Command</p> <p>Use the terraform import command to associate the manually created resource with the resource block in your configuration.</p> <p>Example: Replace placeholders ({subscriptionId}, {resourceGroupName}, {vmName}) with the actual values for your resource.</p> <pre><code>terraform import azurerm_virtual_machine.example /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Compute/virtualMachines/{vmName}\n</code></pre> </li> <li> <p>Refresh the State</p> <p>After importing, run the terraform plan command to verify that the resource's current state matches the Terraform configuration.</p> <pre><code>terraform plan\n</code></pre> <p>If there are discrepancies, Terraform will show a plan to align the state with the configuration.</p> </li> </ol>"},{"location":"terraform_associate/#terraform-taint","title":"terraform taint","text":"<p>Terraform taint command marks a specific resource in the Terraform state as \"tainted,\" meaning it needs to be destroyed and recreated during the next terraform apply.</p> <p>Use Case: A resource is in a bad state (e.g., misconfigured, corrupted, or broken). You want to force a resource to be recreated without modifying its configuration.</p> <ol> <li> <p>Mark the Resource as Tainted:</p> <pre><code>terraform taint &lt;resource_address&gt;\n\n#example\nterraform taint aws_instance.my_instance #&lt;resource_address&gt; refers to the resource's address in the Terraform configuration.\n</code></pre> </li> <li> <p>Apply the Changes: Run terraform apply to destroy the tainted resource and recreate it.</p> <pre><code>terraform apply\n</code></pre> </li> </ol>"},{"location":"terraform_associate/#terraform-apply-replace","title":"terraform apply -replace","text":"<p><code>terraform apply -replace=&lt;address&gt;</code> command is a more modern approach (introduced in Terraform 0.15) to achieve what terraform taint does but in a single step. It directly forces the replacement of a specific resource during the terraform apply operation.</p> <p>Use Case: You want to recreate a specific resource immediately without manually tainting it first.</p> <p>Run the Apply Command with Replace:</p> <pre><code>terraform apply -replace=&lt;resource_address&gt;\n\n#Example\nterraform apply -replace=aws_instance.my_instance\n</code></pre>"},{"location":"terraform_associate/#meta-arguments","title":"Meta Arguments","text":""},{"location":"terraform_associate/#provider-meta-argument","title":"Provider meta-argument","text":"<p>The provider meta-argument is used to specify which provider configuration to use for a resource or data source. It allows you to override the default provider configuration within a specific resource.</p> <p>Usage: You can use the provider meta-argument to define the provider configuration for a resource or data source, especially in cases where you might have multiple provider configurations. This is particularly useful when managing resources across multiple accounts or regions.</p> <pre><code>provider \"aws\" {\n  region = \"us-west-2\"\n}\n\nprovider \"aws\" {\n  alias  = \"east\"\n  region = \"us-east-1\"\n}\n\nresource \"aws_instance\" \"west_instance\" {\n  provider = aws\n  ami           = \"ami-12345678\"\n  instance_type = \"t2.micro\"\n}\n\nresource \"aws_instance\" \"east_instance\" {\n  provider = aws.east\n  ami           = \"ami-23456789\"\n  instance_type = \"t2.micro\"\n}\n</code></pre> <p>In this example:</p> <ul> <li>The first aws_instance uses the us-west-2 region.</li> <li>The second aws_instance uses the us-east-1 region with an alias east</li> </ul>"},{"location":"terraform_associate/#provisioner","title":"Provisioner","text":"<p>local-exec provisioner : The local-exec provisioner runs a command or script on the machine where Terraform is executed (i.e., the local workstation or CI/CD server running Terraform).</p> <p>Use Cases:</p> <ol> <li>Triggering external processes, like CI/CD pipelines or API calls.  </li> <li>Executing local scripts for configuration, validation, or logging.  </li> <li>Sending notifications or updates to external systems.  </li> </ol> <p>Configuration file local-exec</p> <pre><code># example1\n\nresource \"aws_instance\" \"example\" {\n# Resource definition\nami           = \"ami-12345678\"\ninstance_type = \"t2.micro\"\n\nprovisioner \"local-exec\" {\n    command = \"echo 'Instance created!'\"\n}\n}\n\n\n# example2\nresource \"aws_instance\" \"example\" {\nami           = \"ami-12345678\"\ninstance_type = \"t2.micro\"\n\nprovisioner \"local-exec\" {\n    command = \"aws ec2 describe-instances &gt; instances.json\"\n}\n}\n</code></pre> <p>The local-exec provisioner runs an AWS CLI command locally to save instance details to a JSON file.</p> <p>remote-exec provisioner : The remote-exec provisioner runs commands or scripts on a remote resource (e.g., a VM or instance) after it is created. This requires SSH or WinRM access to the resource.</p> <p>Use Cases:</p> <ol> <li>Installing or configuring software on a server.</li> <li>Running startup scripts or applying configuration management tools (e.g., Ansible, Puppet).</li> <li>Performing post-deployment tasks, such as downloading application code.</li> </ol> <p>Configuration file remote-exec</p> <pre><code>resource \"aws_instance\" \"example\" {\n  # Resource definition\n  ami           = \"ami-12345678\"\n  instance_type = \"t2.micro\"\n\n  connection {\n    type        = \"ssh\"\n    host        = self.public_ip\n    user        = \"ec2-user\"\n    private_key = file(\"~/.ssh/id_rsa\")\n  }\n\n  provisioner \"remote-exec\" {\n    inline = [\n      \"sudo apt-get update -y\",\n      \"sudo apt-get install nginx -y\"\n    ]\n  }\n}\n</code></pre>"},{"location":"terraform_associate/#lifecycle","title":"lifecycle","text":"<p>The lifecycle meta-argument defines how Terraform should handle the lifecycle of resources during create, update, and destroy operations. It provides additional control over the resource management process, including handling resource replacement, deletion, and preventing accidental modifications.</p> <p>Common Lifecycle Arguments:</p> <ul> <li>create_before_destroy: Ensures that a new resource is created before an old one is destroyed.</li> <li>prevent_destroy: Prevents the resource from being destroyed, even if it is removed from the configuration.</li> <li>ignore_changes: Tells Terraform to ignore changes to specific resource attributes during updates.</li> </ul> <pre><code>resource \"aws_instance\" \"example\" {\n  ami           = \"ami-12345678\"\n  instance_type = \"t2.micro\"\n\n  lifecycle {\n    create_before_destroy = true\n    prevent_destroy       = true\n    ignore_changes        = [\n      ami,\n    ]\n  }\n}\n</code></pre> <p>In this example:</p> <ul> <li>create_before_destroy = true: Ensures the new instance is created before the old one is destroyed (useful for avoiding downtime).</li> <li>prevent_destroy = true: Prevents the instance from being destroyed accidentally, even if it is removed from the configuration.</li> <li>ignore_changes: Instructs Terraform to ignore changes to the ami attribute, so if the AMI changes, it won't trigger a replacement of the instance</li> </ul>"},{"location":"terraform_associate/#depends_on","title":"depends_on","text":"<p>depends_on meta-argument explicitly defines dependencies between resources.</p> <pre><code>resource \"aws_instance\" \"example\" {\nami           = \"ami-12345678\"\ninstance_type = \"t2.micro\"\n}\n\nresource \"aws_security_group\" \"example\" {\nname        = \"example-sg\"\ndescription = \"Example security group\"\n\ndepends_on = [aws_instance.example]\n}\n</code></pre>"},{"location":"terraform_associate/#count","title":"count","text":"<p>The count meta-argument allows the creation of multiple instances of a resource.</p> <pre><code>resource \"aws_instance\" \"example\" {\ncount         = 3\nami           = \"ami-12345678\"\ninstance_type = \"t2.micro\"\n}\n</code></pre>"},{"location":"terraform_associate/#for_each","title":"for_each","text":"<p>The for_each meta-argument allows iteration over a set of values (like a map or list) to create multiple resources.</p> <pre><code>resource \"aws_instance\" \"example\" {\nfor_each      = var.instance_configs\nami           = each.value[\"ami\"]\ninstance_type = each.value[\"instance_type\"]\n}\n</code></pre>"},{"location":"terraform_associate/#create_before_destroy","title":"create_before_destroy","text":"<p>The create_before_destroy argument in the lifecycle block ensures that the new resource is created before the old one is destroyed. This is useful when a resource's replacement might disrupt service or functionality.</p> <pre><code>resource \"aws_instance\" \"example\" {\nami           = \"ami-12345678\"\ninstance_type = \"t2.micro\"\n\nlifecycle {\n    create_before_destroy = true\n}\n}\n</code></pre>"},{"location":"terraform_associate/#prevent_destroy","title":"prevent_destroy","text":"<p>Prevents the destruction of the resource even if the configuration changes. Useful for protecting critical resources (e.g., databases, production servers). This prevents the resource from being destroyed, even if you run terraform destroy.</p> <pre><code>resource \"aws_instance\" \"example\" {\nami           = \"ami-12345678\"\ninstance_type = \"t2.micro\"\n\nlifecycle {\n    prevent_destroy = true\n}\n}\n</code></pre>"},{"location":"terraform_associate/#output-files","title":"Output files","text":"<p>Output.tf is a file (or a section in any Terraform configuration file) where you define outputs. Outputs allow you to extract and display information about your infrastructure after Terraform has applied changes. These outputs can also be used as inputs to other modules or external systems.</p> <p>Integration: Share data between Terraform modules. Pass infrastructure details to external tools, scripts, or CI/CD pipelines.</p> <p>Basic Syntax:</p> <pre><code>output \"&lt;name&gt;\" {\n  value = &lt;expression&gt;\n}\n</code></pre> <p>Example: Using output.tf</p> <pre><code>output \"instance_public_ip\" {\n  value       = aws_instance.example.public_ip\n  description = \"The public IP address of the EC2 instance\"\n}\n</code></pre>"},{"location":"terraform_associate/#secure-secret-available","title":"Secure secret available","text":"<p>Terraform often requires sensitive information such as API keys, passwords, and other secrets to provision infrastructure. Properly securing these secrets is crucial to prevent accidental exposure or breaches.</p> <ol> <li> <p>Use Environment Variables</p> <p>Terraform allows you to pass variables as environment variables, keeping them out of your configuration files.</p> <p>Define the variable in variables.tf:</p> <pre><code>variable \"db_password\" {\ntype        = string\ndescription = \"The database password\"\n}\n</code></pre> <p>Set the environment variable:</p> <pre><code>export TF_VAR_db_password=\"your_secret_password\"\nReference the variable in your configuration:\n</code></pre> <pre><code>resource \"aws_db_instance\" \"example\" {\npassword = var.db_password\n}\n</code></pre> </li> <li> <p>Use Terraform's Sensitive Attribute</p> <p>``Terraform\u2019s sensitive attribute can be used to mask sensitive data in the Terraform plan and output.</p> <p>Example:</p> <pre><code>variable \"db_password\" {\ntype      = string\nsensitive = true\n}\n\noutput \"db_password\" {\nvalue     = var.db_password\nsensitive = true\n}\n</code></pre> <p>This prevents the sensitive variable from being displayed in the Terraform CLI output.``</p> </li> <li> <p>Use .tfvars Files with Care</p> <p>You can define variables, including secrets, in a .tfvars file.</p> <p>Example: secrets.tfvars:</p> <pre><code>db_password = \"super_secret_password\"\n</code></pre> <p>Run Terraform with the .tfvars file:</p> <pre><code>terraform apply -var-file=\"secrets.tfvars\"\n</code></pre> </li> </ol>"},{"location":"terraform_associate/#understand-the-use-of-collections-and-structural-types","title":"Understand the use of collections and structural types","text":"<p>In Terraform, collections and structural types are mechanisms to handle complex and flexible data structures. They are used to represent and manage data such as lists, maps, and objects, which help organize configuration in a scalable and reusable way.</p>"},{"location":"terraform_associate/#collections-in-terraform","title":"Collections in Terraform","text":"<p>Collections are data types that group multiple values together. Terraform provides two main types of collections: lists and maps.</p> <ol> <li> <p>Lists</p> <p>Definition: A list is an ordered collection of values, indexed by sequential integers starting at 0.</p> <p>Example: list(string) (a list of strings)</p> <p>When to Use: Use lists when order matters, or when iterating through items sequentially.</p> <p>Example:</p> <pre><code>variable \"instance_types\" {\ntype    = list(string)\ndefault = [\"t2.micro\", \"t2.small\", \"t2.medium\"]\n}\n\nresource \"aws_instance\" \"example\" {\ncount         = length(var.instance_types)\ninstance_type = var.instance_types[count.index]\n}\n</code></pre> <p>This creates multiple instances using instance types from the list.</p> </li> <li> <p>Maps</p> <p>Definition: A map is a collection of key-value pairs, where each value is identified by a unique key.</p> <p>Example: map(string) (a map with string keys and string values)</p> <p>When to Use: Use maps when you need to look up values by keys or store key-value pairs for better readability.</p> <pre><code>variable \"region_amis\" {\ntype = map(string)\ndefault = {\n    us-east-1 = \"ami-12345678\"\n    us-west-2 = \"ami-87654321\"\n}\n}\n\nresource \"aws_instance\" \"example\" {\nami           = var.region_amis[var.region]\ninstance_type = \"t2.micro\"\n}\n</code></pre> <p>This selects the correct AMI based on the region.</p> </li> <li> <p>Combining Lists and Maps: Terraform supports nested collections, like a list of maps or a map of lists.</p> <pre><code>variable \"servers\" {\ntype = list(map(string))\ndefault = [\n    { name = \"web1\", type = \"t2.micro\" },\n    { name = \"web2\", type = \"t2.small\" },\n]\n}\n\nresource \"aws_instance\" \"example\" {\ncount         = length(var.servers)\ninstance_type = var.servers[count.index][\"type\"]\ntags = {\n    Name = var.servers[count.index][\"name\"]\n}\n}\n</code></pre> <p>This creates instances with types and names derived from the list of maps.</p> </li> </ol>"},{"location":"terraform_associate/#structural-types-in-terraform","title":"Structural Types in Terraform","text":"<p>Structural types describe complex data structures using objects, tuples, or nested types. They allow for fine-grained control over the shape and constraints of the data.</p> <ol> <li> <p>Objects</p> <p>Definition: Objects are collections of attributes with specified names and types. Each attribute is like a named field in a JSON object.</p> <p>When to Use: Use objects for structured data with named fields and predictable types.</p> <pre><code>variable \"server_config\" {\ntype = object({\n    name         = string\n    instance_type = string\n    tags          = map(string)\n})\ndefault = {\n    name         = \"web-server\"\n    instance_type = \"t2.micro\"\n    tags          = { Environment = \"production\" }\n}\n}\n\nresource \"aws_instance\" \"example\" {\nami           = \"ami-12345678\"\ninstance_type = var.server_config.instance_type\ntags          = var.server_config.tags\n}\n</code></pre> </li> <li> <p>Tuples</p> <p>Definition: Tuples are ordered collections of values with a fixed number of elements, where each element can have a different type.</p> <p>When to Use: Use tuples when you have a fixed structure but need to mix data types.</p> <pre><code>variable \"database_info\" {\ntype = tuple([string, number, bool])\ndefault = [\"db-primary\", 3306, true]\n}\n\noutput \"database_name\" {\nvalue = var.database_info[0]\n}\n\noutput \"database_port\" {\nvalue = var.database_info[1]\n}\n\noutput \"is_database_active\" {\nvalue = var.database_info[2]\n}\n</code></pre> <p>Here, the tuple holds a database name (string), port (number), and active status (boolean)</p> </li> </ol>"},{"location":"terraform_associate/#built-in-functions","title":"built-in functions","text":"<p>Terraform provides a variety of built-in functions that you can use to manipulate and operate on data within your configuration files. These functions allow you to work with strings, numbers, collections, dates, and more, enabling more dynamic and flexible Terraform code.</p>"}]}